<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-typography-survey-disabled vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>Algorithmic bias - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-typography-survey-disabled vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],
"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"11fbdd7c-269b-4662-a83e-6f52969d27cf","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Algorithmic_bias","wgTitle":"Algorithmic bias","wgCurRevisionId":1182647182,"wgRevisionId":1182647182,"wgArticleId":55817338,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1: long volume value","CS1 maint: location missing publisher","Articles with short description","Short description is different from Wikidata","Good articles","Use mdy dates from October 2023","Wikipedia articles needing clarification from September 2021","Machine learning","Information ethics","Computing and society","Philosophy of artificial intelligence","Discrimination","Bias"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en",
"wgPageContentModel":"wikitext","wgRelevantPageName":"Algorithmic_bias","wgRelevantArticleId":55817338,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":10,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":100000,"wgULSCurrentAutonym":"English","wgCentralAuthMobileDomain":false,"wgEditSubmitButtonLabelPublish":true,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q45253460","wgCheckUserClientHintsHeadersJsApi":["architecture","bitness","brands","fullVersionList","mobile","model",
"platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGETopicsMatchModeEnabled":false,"wgGEStructuredTaskRejectionReasonTextInputEnabled":false,"wgGELevelingUpEnabledForUser":false};RLSTATE={"skins.vector.user.styles":"ready","ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","skins.vector.user":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","codex-search-styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","mediawiki.page.media","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher",
"ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","ext.checkUser.clientHints","ext.growthExperiments.SuggestedEditSession"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=codex-search-styles%7Cext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.42.0-wmf.2">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/1/1c/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg">
<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="1200">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg/800px-Artificial_intelligence_prompt_completion_by_dalle_mini.jpg">
<meta property="og:image:width" content="800">
<meta property="og:image:height" content="800">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg/640px-Artificial_intelligence_prompt_completion_by_dalle_mini.jpg">
<meta property="og:image:width" content="640">
<meta property="og:image:height" content="640">
<meta name="viewport" content="width=1000">
<meta property="og:title" content="Algorithmic bias - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" media="only screen and (max-width: 720px)" href="//en.m.wikipedia.org/wiki/Algorithmic_bias">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Algorithmic_bias&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Algorithmic_bias">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="//login.wikimedia.org">
</head>
<body class="skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Algorithmic_bias rootpage-Algorithmic_bias skin-vector-2022 action-view"><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site" role="navigation">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation"><span>Donate</span></a></li>
		</ul>
		
	</div>
</div>

	
	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li>
		</ul>
		
	</div>
</div>

	
<div class="vector-main-menu-action vector-main-menu-action-lang-alert">
	<div class="vector-main-menu-action-item">
		<div class="vector-main-menu-action-heading vector-menu-heading">Languages</div>
		<div class="vector-main-menu-action-content vector-menu-content">
			<div class="mw-message-box cdx-message cdx-message--block mw-message-box-notice cdx-message--notice vector-language-sidebar-alert"><span class="cdx-message__icon"></span><div class="cdx-message__content">Language links are at the top of the page across from the title.</div></div>
		</div>
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/wikipedia.png" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en.svg" style="width: 7.5em; height: 1.125em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en.svg" width="117" height="13" style="width: 7.3125em; height: 0.8125em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" id="" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links" aria-label="Personal tools" role="navigation" >
	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet mw-portlet-vector-user-menu-overflow"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Algorithmic+bias" title="You are encouraged to create an account and log in; however, it is not mandatory"><span>Create account</span></a></li><li id="pt-login-2" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Algorithmic+bias" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Algorithmic+bias" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Algorithmic+bias" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-user-menu-anon-editor" class="vector-menu mw-portlet mw-portlet-user-menu-anon-editor"  >
	<div class="vector-menu-heading">
		Pages for logged out editors <a href="/wiki/Help:Introduction" aria-label="Learn more about editing"><span>learn more</span></a>
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice"><!-- CentralNotice --></div>
		</div>
		
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site" role="navigation">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<nav id="mw-panel-toc" role="navigation" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark vector-sticky-pinned-container">
				<div id="vector-toc-pinned-container" class="vector-pinned-container">
				<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Definitions"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Definitions">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">1</span>Definitions</div>
		</a>
		
		<ul id="toc-Definitions-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Methods"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Methods">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">2</span>Methods</div>
		</a>
		
		<ul id="toc-Methods-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-History"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#History">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">3</span>History</div>
		</a>
		
			<button aria-controls="toc-History-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle History subsection</span>
			</button>
		
		<ul id="toc-History-sublist" class="vector-toc-list">
			<li id="toc-Early_critiques"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Early_critiques">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.1</span>Early critiques</div>
			</a>
			
			<ul id="toc-Early_critiques-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Contemporary_critiques_and_responses"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Contemporary_critiques_and_responses">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">3.2</span>Contemporary critiques and responses</div>
			</a>
			
			<ul id="toc-Contemporary_critiques_and_responses-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Types"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Types">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">4</span>Types</div>
		</a>
		
			<button aria-controls="toc-Types-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Types subsection</span>
			</button>
		
		<ul id="toc-Types-sublist" class="vector-toc-list">
			<li id="toc-Pre-existing"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Pre-existing">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.1</span>Pre-existing</div>
			</a>
			
			<ul id="toc-Pre-existing-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Technical"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Technical">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.2</span>Technical</div>
			</a>
			
			<ul id="toc-Technical-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Emergent"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Emergent">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.3</span>Emergent</div>
			</a>
			
			<ul id="toc-Emergent-sublist" class="vector-toc-list">
				<li id="toc-Correlations"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Correlations">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.3.1</span>Correlations</div>
			</a>
			
			<ul id="toc-Correlations-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Unanticipated_uses"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Unanticipated_uses">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.3.2</span>Unanticipated uses</div>
			</a>
			
			<ul id="toc-Unanticipated_uses-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Feedback_loops"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Feedback_loops">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">4.3.3</span>Feedback loops</div>
			</a>
			
			<ul id="toc-Feedback_loops-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Impact"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Impact">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">5</span>Impact</div>
		</a>
		
			<button aria-controls="toc-Impact-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Impact subsection</span>
			</button>
		
		<ul id="toc-Impact-sublist" class="vector-toc-list">
			<li id="toc-Commercial_influences"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Commercial_influences">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.1</span>Commercial influences</div>
			</a>
			
			<ul id="toc-Commercial_influences-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Voting_behavior"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Voting_behavior">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.2</span>Voting behavior</div>
			</a>
			
			<ul id="toc-Voting_behavior-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Gender_discrimination"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Gender_discrimination">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.3</span>Gender discrimination</div>
			</a>
			
			<ul id="toc-Gender_discrimination-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Racial_and_ethnic_discrimination"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Racial_and_ethnic_discrimination">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.4</span>Racial and ethnic discrimination</div>
			</a>
			
			<ul id="toc-Racial_and_ethnic_discrimination-sublist" class="vector-toc-list">
				<li id="toc-Law_enforcement_and_legal_proceedings"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Law_enforcement_and_legal_proceedings">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.4.1</span>Law enforcement and legal proceedings</div>
			</a>
			
			<ul id="toc-Law_enforcement_and_legal_proceedings-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Online_hate_speech"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Online_hate_speech">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.4.2</span>Online hate speech</div>
			</a>
			
			<ul id="toc-Online_hate_speech-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Surveillance"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Surveillance">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.4.3</span>Surveillance</div>
			</a>
			
			<ul id="toc-Surveillance-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
		<li id="toc-Discrimination_against_the_LGBTQ_community"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Discrimination_against_the_LGBTQ_community">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.5</span>Discrimination against the LGBTQ community</div>
			</a>
			
			<ul id="toc-Discrimination_against_the_LGBTQ_community-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Disability_discrimination"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Disability_discrimination">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.6</span>Disability discrimination</div>
			</a>
			
			<ul id="toc-Disability_discrimination-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Google_Search"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Google_Search">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">5.7</span>Google Search</div>
			</a>
			
			<ul id="toc-Google_Search-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Obstacles_to_research"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Obstacles_to_research">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">6</span>Obstacles to research</div>
		</a>
		
			<button aria-controls="toc-Obstacles_to_research-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Obstacles to research subsection</span>
			</button>
		
		<ul id="toc-Obstacles_to_research-sublist" class="vector-toc-list">
			<li id="toc-Defining_fairness"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Defining_fairness">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.1</span>Defining fairness</div>
			</a>
			
			<ul id="toc-Defining_fairness-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Complexity"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Complexity">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.2</span>Complexity</div>
			</a>
			
			<ul id="toc-Complexity-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Lack_of_transparency"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Lack_of_transparency">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.3</span>Lack of transparency</div>
			</a>
			
			<ul id="toc-Lack_of_transparency-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Lack_of_data_about_sensitive_categories"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Lack_of_data_about_sensitive_categories">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">6.4</span>Lack of data about sensitive categories</div>
			</a>
			
			<ul id="toc-Lack_of_data_about_sensitive_categories-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Solutions"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Solutions">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">7</span>Solutions</div>
		</a>
		
			<button aria-controls="toc-Solutions-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Solutions subsection</span>
			</button>
		
		<ul id="toc-Solutions-sublist" class="vector-toc-list">
			<li id="toc-Technical_2"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Technical_2">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">7.1</span>Technical</div>
			</a>
			
			<ul id="toc-Technical_2-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Transparency_and_monitoring"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Transparency_and_monitoring">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">7.2</span>Transparency and monitoring</div>
			</a>
			
			<ul id="toc-Transparency_and_monitoring-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Right_to_remedy"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Right_to_remedy">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">7.3</span>Right to remedy</div>
			</a>
			
			<ul id="toc-Right_to_remedy-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Diversity_and_inclusion"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Diversity_and_inclusion">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">7.4</span>Diversity and inclusion</div>
			</a>
			
			<ul id="toc-Diversity_and_inclusion-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Interdisciplinarity_and_Collaboration"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Interdisciplinarity_and_Collaboration">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">7.5</span>Interdisciplinarity and Collaboration</div>
			</a>
			
			<ul id="toc-Interdisciplinarity_and_Collaboration-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-Regulation"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Regulation">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">8</span>Regulation</div>
		</a>
		
			<button aria-controls="toc-Regulation-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon vector-icon--x-small mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Regulation subsection</span>
			</button>
		
		<ul id="toc-Regulation-sublist" class="vector-toc-list">
			<li id="toc-Europe"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Europe">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">8.1</span>Europe</div>
			</a>
			
			<ul id="toc-Europe-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-United_States"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#United_States">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">8.2</span>United States</div>
			</a>
			
			<ul id="toc-United_States-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-India"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#India">
				<div class="vector-toc-text">
				<span class="vector-toc-numb">8.3</span>India</div>
			</a>
			
			<ul id="toc-India-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">9</span>See also</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">10</span>References</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Further_reading"
		class="vector-toc-list-item vector-toc-level-1">
		<a class="vector-toc-link" href="#Further_reading">
			<div class="vector-toc-text">
			<span class="vector-toc-numb">11</span>Further reading</div>
		</a>
		
		<ul id="toc-Further_reading-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

				</div>
	</nav>
		
		<div class="mw-content-container">
			<main id="content" class="mw-body" role="main">
				<header class="mw-body-header vector-page-titlebar">
					<nav role="navigation" aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">Algorithmic bias</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 8 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-8" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">8 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%AA%D8%AD%D9%8A%D8%B2_%D8%AE%D9%88%D8%A7%D8%B1%D8%B2%D9%85%D9%8A" title="تحيز خوارزمي – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Sesgo_algor%C3%ADtmico" title="Sesgo algorítmico – Spanish" lang="es" hreflang="es" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/Biais_algorithmique" title="Biais algorithmique – French" lang="fr" hreflang="fr" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-gl mw-list-item"><a href="https://gl.wikipedia.org/wiki/Nesgo_algor%C3%ADtmico" title="Nesgo algorítmico – Galician" lang="gl" hreflang="gl" class="interlanguage-link-target"><span>Galego</span></a></li><li class="interlanguage-link interwiki-ja mw-list-item"><a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9" title="アルゴリズムバイアス – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target"><span>日本語</span></a></li><li class="interlanguage-link interwiki-ps mw-list-item"><a href="https://ps.wikipedia.org/wiki/%D8%A7%D9%84%DA%AF%D9%88%D8%B1%DB%8C%D8%AA%D9%85%D9%8A_%D8%A7%D9%86%D8%AD%D8%B1%D8%A7%D9%81" title="الگوریتمي انحراف – Pashto" lang="ps" hreflang="ps" class="interlanguage-link-target"><span>پښتو</span></a></li><li class="interlanguage-link interwiki-pt mw-list-item"><a href="https://pt.wikipedia.org/wiki/Discrimina%C3%A7%C3%A3o_algor%C3%ADtmica" title="Discriminação algorítmica – Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target"><span>Português</span></a></li><li class="interlanguage-link interwiki-sq mw-list-item"><a href="https://sq.wikipedia.org/wiki/Paragjykimi_algoritmik" title="Paragjykimi algoritmik – Albanian" lang="sq" hreflang="sq" class="interlanguage-link-target"><span>Shqip</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q45253460#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Algorithmic_bias" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:Algorithmic_bias" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="p-variants" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="p-variants-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-variants" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="p-variants-label" for="p-variants-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/Algorithmic_bias"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=Algorithmic_bias&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/Algorithmic_bias"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=Algorithmic_bias&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Algorithmic_bias" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Algorithmic_bias" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Algorithmic_bias&amp;oldid=1182647182" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Algorithmic_bias&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Algorithmic_bias&amp;id=1182647182&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAlgorithmic_bias"><span>Get shortened URL</span></a></li><li id="t-wikibase" class="mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q45253460" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Algorithmic_bias&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Algorithmic_bias&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end">
					<nav class="vector-page-tools-landmark vector-sticky-pinned-container" aria-label="Page tools">
						<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
			
						</div>
	</nav>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		<div id="mw-indicator-good-star" class="mw-indicator"><div class="mw-parser-output"><span typeof="mw:File"><a href="/wiki/Wikipedia:Good_articles" title="This is a good article. Click here for more information."><img alt="This is a good article. Click here for more information." src="//upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/19px-Symbol_support_vote.svg.png" decoding="async" width="19" height="20" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/29px-Symbol_support_vote.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/39px-Symbol_support_vote.svg.png 2x" data-file-width="180" data-file-height="185" /></a></span></div></div>
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content mw-content-ltr" lang="en" dir="ltr"><div class="mw-parser-output"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Technological phenomenon with social implications</div>
<p class="mw-empty-elt">

</p>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:02-Sandvig-Seeing-the-Sort-2014-WEB.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/f/f7/02-Sandvig-Seeing-the-Sort-2014-WEB.png/310px-02-Sandvig-Seeing-the-Sort-2014-WEB.png" decoding="async" width="310" height="529" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/f7/02-Sandvig-Seeing-the-Sort-2014-WEB.png/465px-02-Sandvig-Seeing-the-Sort-2014-WEB.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/f7/02-Sandvig-Seeing-the-Sort-2014-WEB.png/620px-02-Sandvig-Seeing-the-Sort-2014-WEB.png 2x" data-file-width="640" data-file-height="1093" /></a><figcaption>A flow chart showing the decisions made by a <a href="/wiki/Recommendation_engine" class="mw-redirect" title="Recommendation engine">recommendation engine</a>, circa 2001<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup></figcaption></figure>
<style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1045330069">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><table class="sidebar sidebar-collapse nomobile nowraplinks hlist"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Outline_of_artificial_intelligence" title="Outline of artificial intelligence">Artificial intelligence</a></th></tr><tr><td class="sidebar-image"><figure class="mw-halign-center" typeof="mw:File"><a href="/wiki/File:Artificial_intelligence_prompt_completion_by_dalle_mini.jpg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg/150px-Artificial_intelligence_prompt_completion_by_dalle_mini.jpg" decoding="async" width="150" height="150" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg/225px-Artificial_intelligence_prompt_completion_by_dalle_mini.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg/300px-Artificial_intelligence_prompt_completion_by_dalle_mini.jpg 2x" data-file-width="1024" data-file-height="1024" /></a><figcaption></figcaption></figure></td></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center"><a href="/wiki/Artificial_intelligence#Goals" title="Artificial intelligence">Major goals</a></div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence</a></li>
<li><a href="/wiki/Automated_planning_and_scheduling" title="Automated planning and scheduling">Planning</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/General_game_playing" title="General game playing">General game playing</a></li>
<li><a href="/wiki/Knowledge_representation_and_reasoning" title="Knowledge representation and reasoning">Knowledge reasoning</a></li>
<li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
<li><a href="/wiki/Natural_language_processing" title="Natural language processing">Natural language processing</a></li>
<li><a href="/wiki/Robotics" title="Robotics">Robotics</a></li>
<li><a href="/wiki/AI_safety" title="AI safety">AI safety</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center">Approaches</div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Symbolic_artificial_intelligence" title="Symbolic artificial intelligence">Symbolic</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian networks</a></li>
<li><a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">Evolutionary algorithms</a></li>
<li><a href="/wiki/Situated_approach_(artificial_intelligence)" title="Situated approach (artificial intelligence)">Situated approach</a></li>
<li><a href="/wiki/Hybrid_intelligent_system" title="Hybrid intelligent system">Hybrid intelligent systems</a></li>
<li><a href="/wiki/Artificial_intelligence_systems_integration" title="Artificial intelligence systems integration">Systems integration</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center"><a href="/wiki/Philosophy_of_artificial_intelligence" title="Philosophy of artificial intelligence">Philosophy</a></div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Chinese_room" title="Chinese room">Chinese room</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly AI</a></li>
<li><a href="/wiki/AI_control_problem" class="mw-redirect" title="AI control problem">Control problem</a>/<a href="/wiki/AI_takeover" title="AI takeover">Takeover</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a></li>
<li><a href="/wiki/Turing_test" title="Turing test">Turing test</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center"><a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a></div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">Timeline</a></li>
<li><a href="/wiki/Progress_in_artificial_intelligence" title="Progress in artificial intelligence">Progress</a></li>
<li><a href="/wiki/AI_winter" title="AI winter">AI winter</a></li>
<li><a href="/wiki/AI_boom" title="AI boom">AI boom</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center">Technology</div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications</a></li>
<li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li>
<li><a href="/wiki/List_of_programming_languages_for_artificial_intelligence" title="List of programming languages for artificial intelligence">Programming languages</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center">Glossary</div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1063604349">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence" title="Template:Artificial intelligence"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence" title="Template talk:Artificial intelligence"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Artificial_intelligence" title="Special:EditPage/Template:Artificial intelligence"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p><b>Algorithmic bias</b> describes systematic and repeatable <a href="/wiki/Error" title="Error">errors</a> in a <a href="/wiki/Computer_System" class="mw-redirect" title="Computer System">computer system</a> that create "<a href="#Defining_fairness">unfair</a>" outcomes, such as "privileging" one category over another in ways different from the intended function of the algorithm.
</p><p>For mapping this to ideas in statistical learning, it is ```extremely important``` to note that the "bias" here is typically the ```variance``` in the bias-variance trade off in machine learning. So it is actually the opposite of real machine learning bias. It is variance, the model is over-fitting the data or data generation process. In situations where there was no training data and or no consistent learning, then the "algorithmic bias" is simply a measure of deviation according to some score.
</p><p>Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in <a href="/wiki/Search_engine_bias" class="mw-redirect" title="Search engine bias">search engine results</a> and <a href="/wiki/Social_media_bias" class="mw-redirect" title="Social media bias">social media platforms</a>. This bias can have impacts ranging from inadvertent privacy violations to reinforcing <a href="/wiki/Bias" title="Bias">social biases</a> of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect "systematic and unfair" discrimination. This bias has only recently been addressed in legal frameworks, such as the European Union's <a href="/wiki/General_Data_Protection_Regulation" title="General Data Protection Regulation">General Data Protection Regulation</a> (2018) and the proposed <a href="/wiki/Artificial_Intelligence_Act" title="Artificial Intelligence Act">Artificial Intelligence Act</a> (2021).
</p><p>As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of <a href="/wiki/Automation_bias" title="Automation bias">automation bias</a>), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.
</p><p>Algorithmic bias has been cited in cases ranging from election outcomes to the spread of <a href="/wiki/Online_hate_speech" title="Online hate speech">online hate speech</a>. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single "algorithm" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.
</p>
<meta property="mw:PageProp/toc" />
<h2><span class="mw-headline" id="Definitions">Definitions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=1" title="Edit section: Definitions"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:A_computer_program_for_evaluating_forestry_opportunities_under_three_investment_criteria_(1969)_(20385500690).jpg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/9/9f/A_computer_program_for_evaluating_forestry_opportunities_under_three_investment_criteria_%281969%29_%2820385500690%29.jpg/220px-A_computer_program_for_evaluating_forestry_opportunities_under_three_investment_criteria_%281969%29_%2820385500690%29.jpg" decoding="async" width="220" height="328" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/9/9f/A_computer_program_for_evaluating_forestry_opportunities_under_three_investment_criteria_%281969%29_%2820385500690%29.jpg/330px-A_computer_program_for_evaluating_forestry_opportunities_under_three_investment_criteria_%281969%29_%2820385500690%29.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/9f/A_computer_program_for_evaluating_forestry_opportunities_under_three_investment_criteria_%281969%29_%2820385500690%29.jpg/440px-A_computer_program_for_evaluating_forestry_opportunities_under_three_investment_criteria_%281969%29_%2820385500690%29.jpg 2x" data-file-width="1976" data-file-height="2948" /></a><figcaption>A 1969 diagram for how a simple computer program makes decisions, illustrating a very simple algorithm</figcaption></figure>
<p>Algorithms are <a href="/wiki/Algorithm_characterizations" title="Algorithm characterizations">difficult to define</a>,<sup id="cite_ref-Striphas_2-0" class="reference"><a href="#cite_note-Striphas-2">&#91;2&#93;</a></sup> but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze <a href="/wiki/Data" title="Data">data</a> to generate output.<sup id="cite_ref-Cormen_3-0" class="reference"><a href="#cite_note-Cormen-3">&#91;3&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 13">&#58;&#8202;13&#8202;</span></sup> For a rigorous technical introduction, see <a href="/wiki/Algorithm" title="Algorithm">Algorithms</a>. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a>.<sup id="cite_ref-Kitchin_4-0" class="reference"><a href="#cite_note-Kitchin-4">&#91;4&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 14–15">&#58;&#8202;14–15&#8202;</span></sup> By analyzing and processing data, algorithms are the backbone of search engines,<sup id="cite_ref-GoogleAlgorithms_5-0" class="reference"><a href="#cite_note-GoogleAlgorithms-5">&#91;5&#93;</a></sup> social media websites,<sup id="cite_ref-Luckerson_6-0" class="reference"><a href="#cite_note-Luckerson-6">&#91;6&#93;</a></sup> recommendation engines,<sup id="cite_ref-Vanderbilt_7-0" class="reference"><a href="#cite_note-Vanderbilt-7">&#91;7&#93;</a></sup> online retail,<sup id="cite_ref-AngwinMattu_8-0" class="reference"><a href="#cite_note-AngwinMattu-8">&#91;8&#93;</a></sup> online advertising,<sup id="cite_ref-Livingstone_9-0" class="reference"><a href="#cite_note-Livingstone-9">&#91;9&#93;</a></sup> and more.<sup id="cite_ref-Hickman_10-0" class="reference"><a href="#cite_note-Hickman-10">&#91;10&#93;</a></sup>
</p><p>Contemporary <a href="/wiki/Social_science" title="Social science">social scientists</a> are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality.<sup id="cite_ref-Seaver_11-0" class="reference"><a href="#cite_note-Seaver-11">&#91;11&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 2">&#58;&#8202;2&#8202;</span></sup><sup id="cite_ref-Graham_12-0" class="reference"><a href="#cite_note-Graham-12">&#91;12&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 563">&#58;&#8202;563&#8202;</span></sup><sup id="cite_ref-Tewell_13-0" class="reference"><a href="#cite_note-Tewell-13">&#91;13&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 294">&#58;&#8202;294&#8202;</span></sup><sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup> The term <i>algorithmic bias</i> describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a <a href="/wiki/Credit_score" title="Credit score">credit score</a> algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as <i>biased</i>.<sup id="cite_ref-FriedmanNissenbaum_15-0" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 332">&#58;&#8202;332&#8202;</span></sup> This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).
</p>
<h2><span class="mw-headline" id="Methods">Methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=2" title="Edit section: Methods"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Bias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a <a href="/wiki/Database" title="Database">database</a> according to human-designed <a href="/wiki/Cataloging" class="mw-redirect" title="Cataloging">cataloging</a> criteria.<sup id="cite_ref-Gillespie_et_al_16-0" class="reference"><a href="#cite_note-Gillespie_et_al-16">&#91;16&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 3">&#58;&#8202;3&#8202;</span></sup> Next, programmers assign priorities, or <a href="/wiki/Hierarchy" title="Hierarchy">hierarchies</a>, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded.<sup id="cite_ref-Gillespie_et_al_16-1" class="reference"><a href="#cite_note-Gillespie_et_al-16">&#91;16&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 4">&#58;&#8202;4&#8202;</span></sup> Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers.<sup id="cite_ref-Gillespie_et_al_16-2" class="reference"><a href="#cite_note-Gillespie_et_al-16">&#91;16&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 8">&#58;&#8202;8&#8202;</span></sup> Other algorithms may reinforce stereotypes and preferences as they process and display "relevant" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.<sup id="cite_ref-Gillespie_et_al_16-3" class="reference"><a href="#cite_note-Gillespie_et_al-16">&#91;16&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 6">&#58;&#8202;6&#8202;</span></sup>
</p><p>Beyond assembling and processing data, bias can emerge as a result of design.<sup id="cite_ref-TowCenter_17-0" class="reference"><a href="#cite_note-TowCenter-17">&#91;17&#93;</a></sup> For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores).<sup id="cite_ref-Lipartito_18-0" class="reference"><a href="#cite_note-Lipartito-18">&#91;18&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 36">&#58;&#8202;36&#8202;</span></sup> Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. This criteria could present unanticipated outcomes for search results, such as with flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths.<sup id="cite_ref-TowCenter_17-1" class="reference"><a href="#cite_note-TowCenter-17">&#91;17&#93;</a></sup> Algorithms may also display an <i>uncertainty bias</i>, offering more confident assessments when larger <a href="/wiki/Data_set" title="Data set">data sets</a> are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.<sup id="cite_ref-GoodmanFlaxman2016_19-0" class="reference"><a href="#cite_note-GoodmanFlaxman2016-19">&#91;19&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 4">&#58;&#8202;4&#8202;</span></sup>
</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=3" title="Edit section: History"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Early_critiques">Early critiques</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=4" title="Edit section: Early critiques"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Used_Punchcard_(5151286161).jpg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Used_Punchcard_%285151286161%29.jpg/220px-Used_Punchcard_%285151286161%29.jpg" decoding="async" width="220" height="96" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Used_Punchcard_%285151286161%29.jpg/330px-Used_Punchcard_%285151286161%29.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Used_Punchcard_%285151286161%29.jpg/440px-Used_Punchcard_%285151286161%29.jpg 2x" data-file-width="2209" data-file-height="967" /></a><figcaption>This card was used to load software into an old mainframe computer. Each byte (the letter 'A', for example) is entered by punching holes. Though contemporary computers are more complex, they reflect this human decision-making process in collecting and processing data.<sup id="cite_ref-Weizenbaum1976_20-0" class="reference"><a href="#cite_note-Weizenbaum1976-20">&#91;20&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 70">&#58;&#8202;70&#8202;</span></sup><sup id="cite_ref-Goffrey_21-0" class="reference"><a href="#cite_note-Goffrey-21">&#91;21&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 16">&#58;&#8202;16&#8202;</span></sup></figcaption></figure>
<p>The earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book <i><a href="/wiki/Computer_Power_and_Human_Reason" title="Computer Power and Human Reason">Computer Power and Human Reason</a></i>, <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> pioneer <a href="/wiki/Joseph_Weizenbaum" title="Joseph Weizenbaum">Joseph Weizenbaum</a> suggested that bias could arise both from the data used in a program, but also from the way a program is coded.<sup id="cite_ref-Weizenbaum1976_20-1" class="reference"><a href="#cite_note-Weizenbaum1976-20">&#91;20&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 149">&#58;&#8202;149&#8202;</span></sup>
</p><p>Weizenbaum wrote that <a href="/wiki/Computer_program" title="Computer program">programs</a> are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs "embody law",<sup id="cite_ref-Weizenbaum1976_20-2" class="reference"><a href="#cite_note-Weizenbaum1976-20">&#91;20&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 40">&#58;&#8202;40&#8202;</span></sup> that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including their biases and expectations.<sup id="cite_ref-Weizenbaum1976_20-3" class="reference"><a href="#cite_note-Weizenbaum1976-20">&#91;20&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 109">&#58;&#8202;109&#8202;</span></sup> While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects "human decisionmaking processes" as data is being selected.<sup id="cite_ref-Weizenbaum1976_20-4" class="reference"><a href="#cite_note-Weizenbaum1976-20">&#91;20&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 70, 105">&#58;&#8202;70,&#8202;105&#8202;</span></sup>
</p><p>Finally, he noted that machines might also transfer good information with <a href="/wiki/Unintended_consequence" class="mw-redirect" title="Unintended consequence">unintended consequences</a> if users are unclear about how to interpret the results.<sup id="cite_ref-Weizenbaum1976_20-5" class="reference"><a href="#cite_note-Weizenbaum1976-20">&#91;20&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 65">&#58;&#8202;65&#8202;</span></sup> Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.<sup id="cite_ref-Weizenbaum1976_20-6" class="reference"><a href="#cite_note-Weizenbaum1976-20">&#91;20&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 226">&#58;&#8202;226&#8202;</span></sup>
</p><p>An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to <a href="/wiki/St_George%27s,_University_of_London" title="St George&#39;s, University of London">St. George's Hospital Medical School</a> per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with "foreign-sounding names" based on historical trends in admissions.<sup id="cite_ref-LowryMacpherson_22-0" class="reference"><a href="#cite_note-LowryMacpherson-22">&#91;22&#93;</a></sup> While many schools at the time employed similar biases in their selection process, St. George was most notable for automating said bias through the use of an algorithm, thus gaining the attention of people on a much wider scale.
</p><p>In recent years, when more algorithms started to use machine learning methods on real world data, algorithmic bias can be found more often due to the bias existing in the data.
</p>
<h3><span class="mw-headline" id="Contemporary_critiques_and_responses">Contemporary critiques and responses</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=5" title="Edit section: Contemporary critiques and responses"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Though well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze.<sup id="cite_ref-Miller_23-0" class="reference"><a href="#cite_note-Miller-23">&#91;23&#93;</a></sup> The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten.<sup id="cite_ref-Introna1_24-0" class="reference"><a href="#cite_note-Introna1-24">&#91;24&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 115">&#58;&#8202;115&#8202;</span></sup> In theory, these biases may create new patterns of behavior, or "scripts", in relationship to specific technologies as the code <a href="/wiki/Cybernetics" title="Cybernetics">interacts</a> with other elements of society.<sup id="cite_ref-Bogost_25-0" class="reference"><a href="#cite_note-Bogost-25">&#91;25&#93;</a></sup> Biases may also impact how society shapes itself around the <a href="/wiki/Data_point" class="mw-redirect" title="Data point">data points</a> that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.<sup id="cite_ref-IntronaWood_26-0" class="reference"><a href="#cite_note-IntronaWood-26">&#91;26&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 180">&#58;&#8202;180&#8202;</span></sup>
</p><p>The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist,<sup id="cite_ref-Introna2_27-0" class="reference"><a href="#cite_note-Introna2-27">&#91;27&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 15">&#58;&#8202;15&#8202;</span></sup> a process described by author <a href="/wiki/Clay_Shirky" title="Clay Shirky">Clay Shirky</a> as "algorithmic authority".<sup id="cite_ref-ShirkyAuthority_28-0" class="reference"><a href="#cite_note-ShirkyAuthority-28">&#91;28&#93;</a></sup> Shirky uses the term to describe "the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources", such as search results.<sup id="cite_ref-ShirkyAuthority_28-1" class="reference"><a href="#cite_note-ShirkyAuthority-28">&#91;28&#93;</a></sup> This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as "trending" or "popular" may be created based on significantly wider criteria than just their popularity.<sup id="cite_ref-Gillespie_et_al_16-4" class="reference"><a href="#cite_note-Gillespie_et_al-16">&#91;16&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 14">&#58;&#8202;14&#8202;</span></sup>
</p><p>Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans.<sup id="cite_ref-Introna2_27-1" class="reference"><a href="#cite_note-Introna2-27">&#91;27&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 16">&#58;&#8202;16&#8202;</span></sup><sup id="cite_ref-Ziewitz1_29-0" class="reference"><a href="#cite_note-Ziewitz1-29">&#91;29&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 6">&#58;&#8202;6&#8202;</span></sup> This can have the effect of reducing alternative options, compromises, or flexibility.<sup id="cite_ref-Introna2_27-2" class="reference"><a href="#cite_note-Introna2-27">&#91;27&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 16">&#58;&#8202;16&#8202;</span></sup> Sociologist <a href="/wiki/Scott_Lash" title="Scott Lash">Scott Lash</a> has critiqued algorithms as a new form of "generative power", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.<sup id="cite_ref-Lash_30-0" class="reference"><a href="#cite_note-Lash-30">&#91;30&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 71">&#58;&#8202;71&#8202;</span></sup>
</p><p>Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as <a href="/wiki/Google" title="Google">Google</a> and <a href="/wiki/Microsoft" title="Microsoft">Microsoft</a>, which have co-created a working group named Fairness, Accountability,
and Transparency in Machine Learning.<sup id="cite_ref-Garcia_31-0" class="reference"><a href="#cite_note-Garcia-31">&#91;31&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 115">&#58;&#8202;115&#8202;</span></sup> Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences.<sup id="cite_ref-Garcia_31-1" class="reference"><a href="#cite_note-Garcia-31">&#91;31&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 117">&#58;&#8202;117&#8202;</span></sup> In recent years, the study of the Fairness, Accountability,
and Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT.<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup> Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied.<sup id="cite_ref-Ochigame_33-0" class="reference"><a href="#cite_note-Ochigame-33">&#91;33&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Types">Types</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=6" title="Edit section: Types"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Pre-existing">Pre-existing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=7" title="Edit section: Pre-existing"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Pre-existing bias in an algorithm is a consequence of underlying social and institutional <a href="/wiki/Ideology" title="Ideology">ideologies</a>. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious.<sup id="cite_ref-FriedmanNissenbaum_15-1" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 334">&#58;&#8202;334&#8202;</span></sup><sup id="cite_ref-Tewell_13-1" class="reference"><a href="#cite_note-Tewell-13">&#91;13&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 294">&#58;&#8202;294&#8202;</span></sup> Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines.<sup id="cite_ref-Goffrey_21-1" class="reference"><a href="#cite_note-Goffrey-21">&#91;21&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 17">&#58;&#8202;17&#8202;</span></sup> Encoding pre-existing bias into software can preserve social and institutional bias, and, without correction, could be replicated in all future uses of that algorithm.<sup id="cite_ref-Introna1_24-1" class="reference"><a href="#cite_note-Introna1-24">&#91;24&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 116">&#58;&#8202;116&#8202;</span></sup><sup id="cite_ref-Ziewitz1_29-1" class="reference"><a href="#cite_note-Ziewitz1-29">&#91;29&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 8">&#58;&#8202;8&#8202;</span></sup>
</p><p>An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new British citizens after the 1981 <a href="/wiki/British_Nationality_Act" title="British Nationality Act">British Nationality Act</a>.<sup id="cite_ref-FriedmanNissenbaum_15-2" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 341">&#58;&#8202;341&#8202;</span></sup> The program accurately reflected the tenets of the law, which stated that "a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not."<sup id="cite_ref-FriedmanNissenbaum_15-3" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 341">&#58;&#8202;341&#8202;</span></sup><sup id="cite_ref-SergotEtAl_34-0" class="reference"><a href="#cite_note-SergotEtAl-34">&#91;34&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 375">&#58;&#8202;375&#8202;</span></sup> In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.<sup id="cite_ref-FriedmanNissenbaum_15-4" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 342">&#58;&#8202;342&#8202;</span></sup>
</p><p>Another source of bias, which has been called “label choice bias",<sup id="cite_ref-35" class="reference"><a href="#cite_note-35">&#91;35&#93;</a></sup> arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely-used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup> Solutions to the "label choice bias" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Technical">Technical</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=8" title="Edit section: Technical"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Three_Surveillance_cameras.jpg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Three_Surveillance_cameras.jpg/220px-Three_Surveillance_cameras.jpg" decoding="async" width="220" height="179" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Three_Surveillance_cameras.jpg/330px-Three_Surveillance_cameras.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Three_Surveillance_cameras.jpg/440px-Three_Surveillance_cameras.jpg 2x" data-file-width="1815" data-file-height="1475" /></a><figcaption>Facial recognition software used in conjunction with surveillance cameras was found to display bias in recognizing Asian and black faces over white faces.<sup id="cite_ref-IntronaWood_26-1" class="reference"><a href="#cite_note-IntronaWood-26">&#91;26&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 191">&#58;&#8202;191&#8202;</span></sup></figcaption></figure>
<p>Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system.<sup id="cite_ref-FriedmanNissenbaum_15-5" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 332">&#58;&#8202;332&#8202;</span></sup> Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display.<sup id="cite_ref-FriedmanNissenbaum_15-6" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 336">&#58;&#8202;336&#8202;</span></sup> Another case is software that relies on <a href="/wiki/Randomness" title="Randomness">randomness</a> for fair distributions of results. If the <a href="/wiki/Random_number_generation" title="Random number generation">random number generation</a> mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.<sup id="cite_ref-FriedmanNissenbaum_15-7" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 332">&#58;&#8202;332&#8202;</span></sup>
</p><p>A <i>decontextualized algorithm</i> uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines.<sup id="cite_ref-FriedmanNissenbaum_15-8" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 332">&#58;&#8202;332&#8202;</span></sup> The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when <a href="/wiki/Facial_recognition_system" title="Facial recognition system">facial recognition</a> software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's <a href="/wiki/Visual_field" title="Visual field">field of vision</a>. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.<sup id="cite_ref-Graham_12-1" class="reference"><a href="#cite_note-Graham-12">&#91;12&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 574">&#58;&#8202;574&#8202;</span></sup>
</p><p>Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury.<sup id="cite_ref-FriedmanNissenbaum_15-9" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 332">&#58;&#8202;332&#8202;</span></sup> Another unintended result of this form of bias was found in the plagiarism-detection software <a href="/wiki/Turnitin" title="Turnitin">Turnitin</a>, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.<sup id="cite_ref-Introna2_27-3" class="reference"><a href="#cite_note-Introna2-27">&#91;27&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 21–22">&#58;&#8202;21–22&#8202;</span></sup>
</p>
<h3><span class="mw-headline" id="Emergent">Emergent</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=9" title="Edit section: Emergent"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="/wiki/Emergent_properties" class="mw-redirect" title="Emergent properties">Emergent</a> bias is the result of the use and reliance on algorithms across new or unanticipated contexts.<sup id="cite_ref-FriedmanNissenbaum_15-10" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 334">&#58;&#8202;334&#8202;</span></sup> Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms.<sup id="cite_ref-FriedmanNissenbaum_15-11" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 334, 336">&#58;&#8202;334,&#8202;336&#8202;</span></sup> This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion.<sup id="cite_ref-IntronaWood_26-2" class="reference"><a href="#cite_note-IntronaWood-26">&#91;26&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 179">&#58;&#8202;179&#8202;</span></sup><sup id="cite_ref-Tewell_13-2" class="reference"><a href="#cite_note-Tewell-13">&#91;13&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 294">&#58;&#8202;294&#8202;</span></sup> Similarly, problems may emerge when <a href="/wiki/Training_data" class="mw-redirect" title="Training data">training data</a> (the samples "fed" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.<sup id="cite_ref-Gillespie_38-0" class="reference"><a href="#cite_note-Gillespie-38">&#91;38&#93;</a></sup>
</p><p>In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP).<sup id="cite_ref-FriedmanNissenbaum_15-12" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 338">&#58;&#8202;338&#8202;</span></sup> The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.<sup id="cite_ref-FriedmanNissenbaum_15-13" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 338">&#58;&#8202;338&#8202;</span></sup><sup id="cite_ref-Roth_39-0" class="reference"><a href="#cite_note-Roth-39">&#91;39&#93;</a></sup>
</p><p>Additional emergent biases include:
</p>
<h4><span class="mw-headline" id="Correlations">Correlations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=10" title="Edit section: Correlations"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Unpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data.<sup id="cite_ref-GoodmanFlaxman2016_19-1" class="reference"><a href="#cite_note-GoodmanFlaxman2016-19">&#91;19&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 6">&#58;&#8202;6&#8202;</span></sup> In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.<sup id="cite_ref-Kuang_40-0" class="reference"><a href="#cite_note-Kuang-40">&#91;40&#93;</a></sup><sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (September 2021)">clarification needed</span></a></i>&#93;</sup>
</p>
<h4><span class="mw-headline" id="Unanticipated_uses">Unanticipated uses</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=11" title="Edit section: Unanticipated uses"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Emergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand.<sup id="cite_ref-FriedmanNissenbaum_15-14" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 334">&#58;&#8202;334&#8202;</span></sup> These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.<sup id="cite_ref-IntronaWood_26-3" class="reference"><a href="#cite_note-IntronaWood-26">&#91;26&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 179">&#58;&#8202;179&#8202;</span></sup>
</p><p>Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a <a href="/wiki/Proof_of_concept" title="Proof of concept">proof-of-concept</a> by computer scientists and immigration lawyers to evaluate suitability for <a href="/wiki/British_nationality_law" title="British nationality law">British citizenship</a>. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of British immigration law.<sup id="cite_ref-FriedmanNissenbaum_15-15" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 342">&#58;&#8202;342&#8202;</span></sup>
</p>
<h4><span class="mw-headline" id="Feedback_loops">Feedback loops</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=12" title="Edit section: Feedback loops"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Emergent bias may also create a <a href="/wiki/Feedback_loop" class="mw-redirect" title="Feedback loop">feedback loop</a>, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm.<sup id="cite_ref-JouvenalPredPol_41-0" class="reference"><a href="#cite_note-JouvenalPredPol-41">&#91;41&#93;</a></sup><sup id="cite_ref-Chamma_42-0" class="reference"><a href="#cite_note-Chamma-42">&#91;42&#93;</a></sup> For example, simulations of the <a href="/wiki/Predictive_policing" title="Predictive policing">predictive policing</a> software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public.<sup id="cite_ref-LumIsaac_43-0" class="reference"><a href="#cite_note-LumIsaac-43">&#91;43&#93;</a></sup> The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods.<sup id="cite_ref-JouvenalPredPol_41-1" class="reference"><a href="#cite_note-JouvenalPredPol-41">&#91;41&#93;</a></sup><sup id="cite_ref-SmithPredPol_44-0" class="reference"><a href="#cite_note-SmithPredPol-44">&#91;44&#93;</a></sup><sup id="cite_ref-LumIsaacFAQ_45-0" class="reference"><a href="#cite_note-LumIsaacFAQ-45">&#91;45&#93;</a></sup> The <a href="/wiki/Human_Rights_Data_Analysis_Group" title="Human Rights Data Analysis Group">Human Rights Data Analysis Group</a>, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.<sup id="cite_ref-Chamma_42-1" class="reference"><a href="#cite_note-Chamma-42">&#91;42&#93;</a></sup> Another well known example of such an algorithm exhibiting such behavior is <a href="/wiki/COMPAS_(software)" title="COMPAS (software)">COMPAS</a>, a software that determines an individual's likelihood of becoming a criminal offender. The software is often criticized for labeling Black individuals as criminals much more likely than others, and then feeds the data back into itself in the event individuals become registered criminals, further enforcing the bias created by the dataset the algorithm is acting on.
</p><p>Recommender systems such as those used to recommend online videos or news articles can create feedback loops.<sup id="cite_ref-46" class="reference"><a href="#cite_note-46">&#91;46&#93;</a></sup> When users click on content that is suggested by algorithms, it influences the next set of suggestions.<sup id="cite_ref-47" class="reference"><a href="#cite_note-47">&#91;47&#93;</a></sup> Over time this may lead to users entering a <a href="/wiki/Filter_bubble" title="Filter bubble">filter bubble</a> and being unaware of important or useful content.<sup id="cite_ref-48" class="reference"><a href="#cite_note-48">&#91;48&#93;</a></sup><sup id="cite_ref-49" class="reference"><a href="#cite_note-49">&#91;49&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Impact">Impact</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=13" title="Edit section: Impact"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Commercial_influences">Commercial influences</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=14" title="Edit section: Commercial influences"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, <a href="/wiki/American_Airlines" title="American Airlines">American Airlines</a> created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the <a href="/wiki/United_States_Congress" title="United States Congress">United States Congress</a>, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.<sup id="cite_ref-Sandvig1_50-0" class="reference"><a href="#cite_note-Sandvig1-50">&#91;50&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 2">&#58;&#8202;2&#8202;</span></sup><sup id="cite_ref-FriedmanNissenbaum_15-16" class="reference"><a href="#cite_note-FriedmanNissenbaum-15">&#91;15&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 331">&#58;&#8202;331&#8202;</span></sup>
</p><p>In a 1998 paper describing <a href="/wiki/Google" title="Google">Google</a>, the founders of the company had adopted a policy of transparency in search results regarding paid placement, arguing that "advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers."<sup id="cite_ref-BrinPage98_51-0" class="reference"><a href="#cite_note-BrinPage98-51">&#91;51&#93;</a></sup> This bias would be an "invisible" manipulation of the user.<sup id="cite_ref-Sandvig1_50-1" class="reference"><a href="#cite_note-Sandvig1-50">&#91;50&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 3">&#58;&#8202;3&#8202;</span></sup>
</p>
<h3><span class="mw-headline" id="Voting_behavior">Voting behavior</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=15" title="Edit section: Voting behavior"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have "no means of competing" if an algorithm, with or without intent, boosted page listings for a rival candidate.<sup id="cite_ref-Epstein_52-0" class="reference"><a href="#cite_note-Epstein-52">&#91;52&#93;</a></sup> Facebook users who saw messages related to voting were more likely to vote. A 2010 <a href="/wiki/Randomized_trial" class="mw-redirect" title="Randomized trial">randomized trial</a> of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted.<sup id="cite_ref-Bond-etal_53-0" class="reference"><a href="#cite_note-Bond-etal-53">&#91;53&#93;</a></sup> Legal scholar Jonathan Zittrain has warned that this could create a "digital gerrymandering" effect in elections, "the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users", if intentionally manipulated.<sup id="cite_ref-Zittrain_54-0" class="reference"><a href="#cite_note-Zittrain-54">&#91;54&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 335">&#58;&#8202;335&#8202;</span></sup>
</p>
<h3><span class="mw-headline" id="Gender_discrimination">Gender discrimination</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=16" title="Edit section: Gender discrimination"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In 2016, the professional networking site <a href="/wiki/LinkedIn" title="LinkedIn">LinkedIn</a> was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, "Andrea" would bring up a prompt asking if users meant "Andrew", but queries for "Andrew" did not ask if users meant to find "Andrea". The company said this was the result of an analysis of users' interactions with the site.<sup id="cite_ref-Day_55-0" class="reference"><a href="#cite_note-Day-55">&#91;55&#93;</a></sup>
</p><p>In 2012, the department store franchise <a href="/wiki/Target_(company)" class="mw-redirect" title="Target (company)">Target</a> was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners.<sup id="cite_ref-CrawfordSchultz_56-0" class="reference"><a href="#cite_note-CrawfordSchultz-56">&#91;56&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 94">&#58;&#8202;94&#8202;</span></sup><sup id="cite_ref-Duhigg_57-0" class="reference"><a href="#cite_note-Duhigg-57">&#91;57&#93;</a></sup> Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.<sup id="cite_ref-CrawfordSchultz_56-1" class="reference"><a href="#cite_note-CrawfordSchultz-56">&#91;56&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 98">&#58;&#8202;98&#8202;</span></sup>
</p><p>Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, "lesbian". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, "Top 25 Sexiest Women Athletes" articles displayed as first-page results in searches for "women athletes".<sup id="cite_ref-Noble_58-0" class="reference"><a href="#cite_note-Noble-58">&#91;58&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 31">&#58;&#8202;31&#8202;</span></sup> In 2017, Google adjusted these results along with others that surfaced <a href="/wiki/Hate_groups" class="mw-redirect" title="Hate groups">hate groups</a>, racist views, child abuse and pornography, and other upsetting and offensive content.<sup id="cite_ref-Guynn2_59-0" class="reference"><a href="#cite_note-Guynn2-59">&#91;59&#93;</a></sup> Other examples include the display of higher-paying jobs to male applicants on job search websites.<sup id="cite_ref-SimoniteMIT_60-0" class="reference"><a href="#cite_note-SimoniteMIT-60">&#91;60&#93;</a></sup> Researchers have also identified that machine translation exhibits a strong tendency towards male defaults.<sup id="cite_ref-61" class="reference"><a href="#cite_note-61">&#91;61&#93;</a></sup> In particular, this is observed in fields linked to unbalanced gender distribution, including <a href="/wiki/Science,_technology,_engineering,_and_mathematics" title="Science, technology, engineering, and mathematics">STEM</a> occupations.<sup id="cite_ref-62" class="reference"><a href="#cite_note-62">&#91;62&#93;</a></sup> In fact, current machine translation systems fail to reproduce the real world distribution of female workers.<sup id="cite_ref-63" class="reference"><a href="#cite_note-63">&#91;63&#93;</a></sup>
</p><p>In 2015, <a href="/wiki/Amazon.com" class="mw-redirect" title="Amazon.com">Amazon.com</a> turned off an AI system it developed to screen job applications when they realized it was biased against women.<sup id="cite_ref-64" class="reference"><a href="#cite_note-64">&#91;64&#93;</a></sup> The recruitment tool excluded applicants who attended all-women's colleges and resumes that included the word "women's".<sup id="cite_ref-65" class="reference"><a href="#cite_note-65">&#91;65&#93;</a></sup> A similar problem emerged with music streaming services—In 2019, it was discovered that the recommender system algorithm used by Spotify was biased against women artists.<sup id="cite_ref-66" class="reference"><a href="#cite_note-66">&#91;66&#93;</a></sup> Spotify's song recommendations suggested more male artists over women artists.
</p>
<h3><span class="mw-headline" id="Racial_and_ethnic_discrimination">Racial and ethnic discrimination</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=17" title="Edit section: Racial and ethnic discrimination"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Algorithms have been criticized as a method for obscuring racial prejudices in decision-making.<sup id="cite_ref-Buolamwini-Gebru_67-0" class="reference"><a href="#cite_note-Buolamwini-Gebru-67">&#91;67&#93;</a></sup><sup id="cite_ref-Noble2_68-0" class="reference"><a href="#cite_note-Noble2-68">&#91;68&#93;</a></sup><sup id="cite_ref-Nakamura1_69-0" class="reference"><a href="#cite_note-Nakamura1-69">&#91;69&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 158">&#58;&#8202;158&#8202;</span></sup> Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases.<sup id="cite_ref-70" class="reference"><a href="#cite_note-70">&#91;70&#93;</a></sup> For example, black people are likely to receive longer sentences than white people who committed the same crime.<sup id="cite_ref-71" class="reference"><a href="#cite_note-71">&#91;71&#93;</a></sup><sup id="cite_ref-72" class="reference"><a href="#cite_note-72">&#91;72&#93;</a></sup> This could potentially mean that a system amplifies the original biases in the data.
</p><p>In 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as <a href="/wiki/Ethnic_stereotype" title="Ethnic stereotype">gorillas</a>.<sup id="cite_ref-Guynn_73-0" class="reference"><a href="#cite_note-Guynn-73">&#91;73&#93;</a></sup> In 2010, <a href="/wiki/Nikon" title="Nikon">Nikon</a> cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking.<sup id="cite_ref-Rose_74-0" class="reference"><a href="#cite_note-Rose-74">&#91;74&#93;</a></sup> Such examples are the product of bias in <a href="/wiki/Biometric_data" class="mw-redirect" title="Biometric data">biometric data</a> sets.<sup id="cite_ref-Guynn_73-1" class="reference"><a href="#cite_note-Guynn-73">&#91;73&#93;</a></sup> Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.<sup id="cite_ref-Nakamura1_69-1" class="reference"><a href="#cite_note-Nakamura1-69">&#91;69&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 154">&#58;&#8202;154&#8202;</span></sup> Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.<sup id="cite_ref-75" class="reference"><a href="#cite_note-75">&#91;75&#93;</a></sup>
</p><p>Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name.<sup id="cite_ref-Sweeney_76-0" class="reference"><a href="#cite_note-Sweeney-76">&#91;76&#93;</a></sup> A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithm's model of lung function.<sup id="cite_ref-77" class="reference"><a href="#cite_note-77">&#91;77&#93;</a></sup><sup id="cite_ref-78" class="reference"><a href="#cite_note-78">&#91;78&#93;</a></sup>
</p><p>In 2019, a research study revealed that a healthcare algorithm sold by <a href="/wiki/Optum" title="Optum">Optum</a> favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.<sup id="cite_ref-79" class="reference"><a href="#cite_note-79">&#91;79&#93;</a></sup>
</p><p>A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on "creditworthiness" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.<sup id="cite_ref-80" class="reference"><a href="#cite_note-80">&#91;80&#93;</a></sup><sup class="noprint Inline-Template noprint Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources" title="Wikipedia:No original research"><span title="This claim needs references to reliable secondary sources. (December 2019)">non-primary source needed</span></a></i>&#93;</sup>
</p>
<h4><span class="mw-headline" id="Law_enforcement_and_legal_proceedings">Law enforcement and legal proceedings</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=18" title="Edit section: Law enforcement and legal proceedings"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Algorithms already have numerous applications in legal systems. An example of this is <a href="/wiki/COMPAS_(software)" title="COMPAS (software)">COMPAS</a>, a commercial program widely used by <a href="/wiki/U.S._court" class="mw-redirect" title="U.S. court">U.S. courts</a> to assess the likelihood of a <a href="/wiki/Defendant" title="Defendant">defendant</a> becoming a <a href="/wiki/Recidivist" class="mw-redirect" title="Recidivist">recidivist</a>. <a href="/wiki/ProPublica" title="ProPublica">ProPublica</a> claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants, and that black defendants are twice as likely to be erroneously assigned the label "high-risk" as white defendants.<sup id="cite_ref-81" class="reference"><a href="#cite_note-81">&#91;81&#93;</a></sup><sup id="cite_ref-82" class="reference"><a href="#cite_note-82">&#91;82&#93;</a></sup>
</p><p>One example is the use of <a href="/wiki/Risk_assessment" title="Risk assessment">risk assessments</a> in <a href="/wiki/Criminal_sentencing_in_the_United_States" title="Criminal sentencing in the United States">criminal sentencing in the United States</a> and <a href="/wiki/Parole_board" title="Parole board">parole hearings</a>, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime.<sup id="cite_ref-ProPublica_83-0" class="reference"><a href="#cite_note-ProPublica-83">&#91;83&#93;</a></sup> For the time period starting in 1920 and ending in 1970, the nationality of a criminal's father was a consideration in those risk assessment scores.<sup id="cite_ref-Harcourt_84-0" class="reference"><a href="#cite_note-Harcourt-84">&#91;84&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 4">&#58;&#8202;4&#8202;</span></sup> Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by <a href="/wiki/ProPublica" title="ProPublica">ProPublica</a> found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.<sup id="cite_ref-ProPublica_83-1" class="reference"><a href="#cite_note-ProPublica-83">&#91;83&#93;</a></sup>
</p><p>One study that set out to examine "Risk, Race, &amp; Recidivism: Predictive Bias and Disparate Impact" alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation.<sup id="cite_ref-85" class="reference"><a href="#cite_note-85">&#91;85&#93;</a></sup> 
</p><p>In the pretrial detention context, a law review article argues that algorithmic risk assessments violate <a href="/wiki/Fourteenth_Amendment_to_the_United_States_Constitution" title="Fourteenth Amendment to the United States Constitution">14th Amendment</a> <a href="/wiki/Equal_Protection" class="mw-redirect" title="Equal Protection">Equal Protection</a> rights on the basis of race, since the algorithms are argued to be facially discriminatory, to result in disparate treatment, and to not be narrowly tailored.<sup id="cite_ref-86" class="reference"><a href="#cite_note-86">&#91;86&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Online_hate_speech">Online hate speech</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=19" title="Edit section: Online hate speech"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In 2017 a <a href="/wiki/Facebook" title="Facebook">Facebook</a> algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents.<sup id="cite_ref-AngwinGrassegger_87-0" class="reference"><a href="#cite_note-AngwinGrassegger-87">&#91;87&#93;</a></sup> The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing "Muslims" would be blocked, while posts denouncing "Radical Muslims" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the "children" subset of blacks, rather than "all blacks", whereas "all white men" would trigger a block, because whites and males are not considered subsets.<sup id="cite_ref-AngwinGrassegger_87-1" class="reference"><a href="#cite_note-AngwinGrassegger-87">&#91;87&#93;</a></sup> Facebook was also found to allow ad purchasers to target "Jew haters" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.<sup id="cite_ref-AngwinVarnerTobin_88-0" class="reference"><a href="#cite_note-AngwinVarnerTobin-88">&#91;88&#93;</a></sup>
</p><p>While algorithms are used to track and block hate speech, some were found to be 1.5 times more likely to flag information posted by Black users and 2.2 times likely to flag information as hate speech if written in <a href="/wiki/African_American_English" class="mw-redirect" title="African American English">African American English</a>.<sup id="cite_ref-89" class="reference"><a href="#cite_note-89">&#91;89&#93;</a></sup> Without context for slurs and epithets, even when used by communities which have re-appropriated them, were flagged.<sup id="cite_ref-90" class="reference"><a href="#cite_note-90">&#91;90&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Surveillance">Surveillance</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=20" title="Edit section: Surveillance"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times.<sup id="cite_ref-Graham_12-2" class="reference"><a href="#cite_note-Graham-12">&#91;12&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 572">&#58;&#8202;572&#8202;</span></sup> The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender.<sup id="cite_ref-Furl2002_91-0" class="reference"><a href="#cite_note-Furl2002-91">&#91;91&#93;</a></sup> However, even audits of these image-recognition systems are ethically fraught, and some scholars have suggested the technology's context will always have a disproportionate impact on communities whose actions are over-surveilled.<sup id="cite_ref-Raji-Gebru-Mitchell-2020_92-0" class="reference"><a href="#cite_note-Raji-Gebru-Mitchell-2020-92">&#91;92&#93;</a></sup> For example, a 2002 analysis of software used to identify individuals in <a href="/wiki/CCTV" class="mw-redirect" title="CCTV">CCTV</a> images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites.<sup id="cite_ref-IntronaWood_26-4" class="reference"><a href="#cite_note-IntronaWood-26">&#91;26&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 190">&#58;&#8202;190&#8202;</span></sup> A 2018 study found that facial recognition software most likely accurately identified light-skinned (typically European) males, with slightly lower accuracy rates for light-skinned females. Dark-skinned males and females were significanfly less likely to be accurately identified by facial recognition software. These disparities are attributed to the under-representation of darker-skinned participants in data sets used to develop this software.<sup id="cite_ref-The_New_York_Times_2018_l903_93-0" class="reference"><a href="#cite_note-The_New_York_Times_2018_l903-93">&#91;93&#93;</a></sup><sup id="cite_ref-94" class="reference"><a href="#cite_note-94">&#91;94&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Discrimination_against_the_LGBTQ_community">Discrimination against the LGBTQ community</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=21" title="Edit section: Discrimination against the LGBTQ community"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In 2011, users of the gay hookup application <a href="/wiki/Grindr" title="Grindr">Grindr</a> reported that the <a href="/wiki/Google_Play" title="Google Play">Android store</a>'s recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately <a href="/wiki/Societal_attitudes_toward_homosexuality#Association_with_child_sexual_abuse_and_pedophilia" title="Societal attitudes toward homosexuality">related homosexuality with pedophilia</a>. Writer Mike Ananny criticized this association in <i><a href="/wiki/The_Atlantic" title="The Atlantic">The Atlantic</a></i>, arguing that such associations further stigmatized <a href="/wiki/History_of_gay_men_in_the_United_States" title="History of gay men in the United States">gay men</a>.<sup id="cite_ref-Ananny_95-0" class="reference"><a href="#cite_note-Ananny-95">&#91;95&#93;</a></sup> In 2009, online retailer <a href="/wiki/Amazon_(company)" title="Amazon (company)">Amazon</a> de-listed 57,000 books after an algorithmic change expanded its "adult content" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel <i><a href="/wiki/Brokeback_Mountain" title="Brokeback Mountain">Brokeback Mountain</a></i>.<sup id="cite_ref-Kafka2_96-0" class="reference"><a href="#cite_note-Kafka2-96">&#91;96&#93;</a></sup><sup id="cite_ref-Gillespie_et_al_16-5" class="reference"><a href="#cite_note-Gillespie_et_al-16">&#91;16&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 5">&#58;&#8202;5&#8202;</span></sup><sup id="cite_ref-Kafka_97-0" class="reference"><a href="#cite_note-Kafka-97">&#91;97&#93;</a></sup>
</p><p>In 2019, it was found that on Facebook, searches for "photos of my female friends" yielded suggestions such as "in bikinis" or "at the beach". In contrast, searches for "photos of my male friends" yielded no results.<sup id="cite_ref-98" class="reference"><a href="#cite_note-98">&#91;98&#93;</a></sup>
</p><p>Facial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning.<sup id="cite_ref-99" class="reference"><a href="#cite_note-99">&#91;99&#93;</a></sup> Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.<sup id="cite_ref-100" class="reference"><a href="#cite_note-100">&#91;100&#93;</a></sup>
</p><p>There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individual's sexual orientation based on their facial images.<sup id="cite_ref-101" class="reference"><a href="#cite_note-101">&#91;101&#93;</a></sup> The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being "outed" against their will.<sup id="cite_ref-102" class="reference"><a href="#cite_note-102">&#91;102&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Disability_discrimination">Disability discrimination</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=22" title="Edit section: Disability discrimination"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>While the modalities of algorithmic fairness have been judged on the basis of different aspects of bias – like gender, race and socioeconomic status, disability often is left out of the list.<sup id="cite_ref-103" class="reference"><a href="#cite_note-103">&#91;103&#93;</a></sup><sup id="cite_ref-104" class="reference"><a href="#cite_note-104">&#91;104&#93;</a></sup> The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms, creating even more exclusion<sup id="cite_ref-105" class="reference"><a href="#cite_note-105">&#91;105&#93;</a></sup><sup id="cite_ref-106" class="reference"><a href="#cite_note-106">&#91;106&#93;</a></sup>   
</p><p>The shifting nature of disabilities and its subjective characterization, makes it more difficult to computationally address. The lack of historical depth in defining disabilities, collecting its incidence and prevalence in questionnaires, and establishing recognition add to the controversy and ambiguity in its quantification and calculations.&#160; The definition of disability has been long debated shifting from a <a href="/wiki/Medical_model_of_disability" title="Medical model of disability">medical model</a> to a <a href="/wiki/Social_model_of_disability" title="Social model of disability">social model of disability</a> most recently, which establishes that disability is a result of the mismatch between people's interactions and barriers in their environment, rather than impairments and health conditions. Disabilities can also be situational or temporary,<sup id="cite_ref-107" class="reference"><a href="#cite_note-107">&#91;107&#93;</a></sup> considered in a constant state of flux. Disabilities are incredibly diverse,<sup id="cite_ref-108" class="reference"><a href="#cite_note-108">&#91;108&#93;</a></sup> fall within a large spectrum, and can be unique to each individual. People’s identity can vary based on the specific types of disability they experience, how they use assistive technologies, and who they support.&#160; The high level of variability across people’s experiences greatly personalizes how a disability can manifest. Overlapping identities and intersectional experiences<sup id="cite_ref-109" class="reference"><a href="#cite_note-109">&#91;109&#93;</a></sup> are excluded from statistics and datasets,<sup id="cite_ref-110" class="reference"><a href="#cite_note-110">&#91;110&#93;</a></sup> hence underrepresented and nonexistent in training data.<sup id="cite_ref-111" class="reference"><a href="#cite_note-111">&#91;111&#93;</a></sup> Therefore, machine learning models are trained inequitably and artificial intelligent systems perpetuate more algorithmic bias.<sup id="cite_ref-112" class="reference"><a href="#cite_note-112">&#91;112&#93;</a></sup> For example, if people with speech impairments aren’t included in training voice control features and smart AI assistants –they are unable to use the feature or the responses received from a Google Home or Alexa are extremely poor. 
</p><p>Given the stereotypes and stigmas that still exist surrounding disabilities, the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges,&#160; As disclosing disability information can be taboo and drive further discrimination against this population, there is a lack of explicit disability data available for algorithmic systems to interact with. People with disabilities face additional harms and risks with respect to their social support, cost of health insurance, workplace discrimination and other basic necessities upon disclosing their disability status. Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures.<sup id="cite_ref-113" class="reference"><a href="#cite_note-113">&#91;113&#93;</a></sup><sup id="cite_ref-114" class="reference"><a href="#cite_note-114">&#91;114&#93;</a></sup> 
</p>
<h3><span class="mw-headline" id="Google_Search">Google Search</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=23" title="Edit section: Google Search"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>While users generate results that are "completed" automatically, Google has failed to remove sexist and racist autocompletion text. For example, <i><a href="/wiki/Algorithms_of_Oppression" title="Algorithms of Oppression">Algorithms of Oppression: How Search Engines Reinforce Racism</a></i> Safiya Noble notes an example of the search for "black girls", which was reported to result in pornographic images. Google claimed it was unable to erase those pages unless they were considered unlawful.<sup id="cite_ref-115" class="reference"><a href="#cite_note-115">&#91;115&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Obstacles_to_research">Obstacles to research</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=24" title="Edit section: Obstacles to research"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Several problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.<sup id="cite_ref-Seaver_11-1" class="reference"><a href="#cite_note-Seaver-11">&#91;11&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 5">&#58;&#8202;5&#8202;</span></sup>
</p>
<h3><span class="mw-headline" id="Defining_fairness">Defining fairness</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=25" title="Edit section: Defining fairness"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Fairness_(machine_learning)" title="Fairness (machine learning)">Fairness (machine learning)</a></div>
<p>Literature on algorithmic bias has focused on the remedy of fairness, but definitions of fairness are often incompatible with each other and the realities of machine learning optimization. For example, defining fairness as an "equality of outcomes" may simply refer to a system producing the same result for all people, while fairness defined as "equality of treatment" might explicitly consider differences between individuals.<sup id="cite_ref-Friedler_116-0" class="reference"><a href="#cite_note-Friedler-116">&#91;116&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 2">&#58;&#8202;2&#8202;</span></sup> As a result, fairness is sometimes described as being in conflict with the accuracy of a model, suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems.<sup id="cite_ref-Hu_117-0" class="reference"><a href="#cite_note-Hu-117">&#91;117&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 2">&#58;&#8202;2&#8202;</span></sup> In response to this tension, researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms, with "fairness" defined for specific applications and contexts.<sup id="cite_ref-Dwork_118-0" class="reference"><a href="#cite_note-Dwork-118">&#91;118&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Complexity">Complexity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=26" title="Edit section: Complexity"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Algorithmic processes are <a href="/wiki/Complex_system" title="Complex system">complex</a>, often exceeding the understanding of the people who use them.<sup id="cite_ref-Seaver_11-2" class="reference"><a href="#cite_note-Seaver-11">&#91;11&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 2">&#58;&#8202;2&#8202;</span></sup><sup id="cite_ref-Sandvig2_119-0" class="reference"><a href="#cite_note-Sandvig2-119">&#91;119&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 7">&#58;&#8202;7&#8202;</span></sup> Large-scale operations may not be understood even by those involved in creating them.<sup id="cite_ref-LaFrance_120-0" class="reference"><a href="#cite_note-LaFrance-120">&#91;120&#93;</a></sup> The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.<sup id="cite_ref-IntronaWood_26-5" class="reference"><a href="#cite_note-IntronaWood-26">&#91;26&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 183">&#58;&#8202;183&#8202;</span></sup> Social scientist <a href="/wiki/Bruno_Latour" title="Bruno Latour">Bruno Latour</a> has identified this process as <a href="/wiki/Blackboxing" title="Blackboxing">blackboxing</a>, a process in which "scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become."<sup id="cite_ref-121" class="reference"><a href="#cite_note-121">&#91;121&#93;</a></sup> Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.<sup id="cite_ref-KubitschkoKaun_122-0" class="reference"><a href="#cite_note-KubitschkoKaun-122">&#91;122&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 92">&#58;&#8202;92&#8202;</span></sup>
</p><p>An example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013.<sup id="cite_ref-McGee_123-0" class="reference"><a href="#cite_note-McGee-123">&#91;123&#93;</a></sup> Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms.<sup id="cite_ref-Introna1_24-2" class="reference"><a href="#cite_note-Introna1-24">&#91;24&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 118">&#58;&#8202;118&#8202;</span></sup> Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.<sup id="cite_ref-Kitchin_4-1" class="reference"><a href="#cite_note-Kitchin-4">&#91;4&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 22">&#58;&#8202;22&#8202;</span></sup>
</p><p>Additional complexity occurs through <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a> and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms.<sup id="cite_ref-Granka_124-0" class="reference"><a href="#cite_note-Granka-124">&#91;124&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 367">&#58;&#8202;367&#8202;</span></sup><sup id="cite_ref-Sandvig2_119-1" class="reference"><a href="#cite_note-Sandvig2-119">&#91;119&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 7">&#58;&#8202;7&#8202;</span></sup> One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.<sup id="cite_ref-Seaver_11-3" class="reference"><a href="#cite_note-Seaver-11">&#91;11&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 5">&#58;&#8202;5&#8202;</span></sup>
Companies also run frequent <a href="/wiki/A/B_tests" class="mw-redirect" title="A/B tests">A/B tests</a> to fine-tune algorithms based on user response. For example, the search engine <a href="/wiki/Bing_(search_engine)" class="mw-redirect" title="Bing (search engine)">Bing</a> can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.<sup id="cite_ref-Seaver_11-4" class="reference"><a href="#cite_note-Seaver-11">&#91;11&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 5">&#58;&#8202;5&#8202;</span></sup>
</p>
<h3><span class="mw-headline" id="Lack_of_transparency">Lack of transparency</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=27" title="Edit section: Lack of transparency"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Commercial algorithms are proprietary, and may be treated as <a href="/wiki/Trade_secrets" class="mw-redirect" title="Trade secrets">trade secrets</a>.<sup id="cite_ref-Seaver_11-5" class="reference"><a href="#cite_note-Seaver-11">&#91;11&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 2">&#58;&#8202;2&#8202;</span></sup><sup id="cite_ref-Sandvig2_119-2" class="reference"><a href="#cite_note-Sandvig2-119">&#91;119&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 7">&#58;&#8202;7&#8202;</span></sup><sup id="cite_ref-IntronaWood_26-6" class="reference"><a href="#cite_note-IntronaWood-26">&#91;26&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 183">&#58;&#8202;183&#8202;</span></sup> Treating algorithms as trade secrets protects companies, such as <a href="/wiki/Web_search_engine" class="mw-redirect" title="Web search engine">search engines</a>, where a transparent algorithm might reveal tactics to manipulate search rankings.<sup id="cite_ref-Granka_124-1" class="reference"><a href="#cite_note-Granka-124">&#91;124&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 366">&#58;&#8202;366&#8202;</span></sup> This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function.<sup id="cite_ref-Kitchin_4-2" class="reference"><a href="#cite_note-Kitchin-4">&#91;4&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 20">&#58;&#8202;20&#8202;</span></sup> Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output.<sup id="cite_ref-Granka_124-2" class="reference"><a href="#cite_note-Granka-124">&#91;124&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 369">&#58;&#8202;369&#8202;</span></sup> Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.<sup id="cite_ref-125" class="reference"><a href="#cite_note-125">&#91;125&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Lack_of_data_about_sensitive_categories">Lack of data about sensitive categories</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=28" title="Edit section: Lack of data about sensitive categories"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by <a href="/wiki/Anti-discrimination_law" title="Anti-discrimination law">anti-discrimination law</a>, are often not explicitly considered when collecting and processing data.<sup id="cite_ref-126" class="reference"><a href="#cite_note-126">&#91;126&#93;</a></sup> In some cases, there is little opportunity to collect this data explicitly, such as in <a href="/wiki/Device_fingerprint" title="Device fingerprint">device fingerprinting</a>, <a href="/wiki/Ubiquitous_computing" title="Ubiquitous computing">ubiquitous computing</a> and the <a href="/wiki/Internet_of_things" title="Internet of things">Internet of Things</a>. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's <a href="/wiki/General_Data_Protection_Regulation" title="General Data Protection Regulation">General Data Protection Regulation</a>, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.
</p><p>Some practitioners have tried to estimate and impute these missing sensitive categorisations in order to allow bias mitigation, for example building systems to infer ethnicity from names,<sup id="cite_ref-127" class="reference"><a href="#cite_note-127">&#91;127&#93;</a></sup> however this can introduce other forms of bias if not undertaken with care.<sup id="cite_ref-128" class="reference"><a href="#cite_note-128">&#91;128&#93;</a></sup> Machine learning researchers have drawn upon cryptographic <a href="/wiki/Privacy-enhancing_technologies" title="Privacy-enhancing technologies">privacy-enhancing technologies</a> such as <a href="/wiki/Secure_multi-party_computation" title="Secure multi-party computation">secure multi-party computation</a> to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in <a href="/wiki/Cleartext" class="mw-redirect" title="Cleartext">cleartext</a>.<sup id="cite_ref-129" class="reference"><a href="#cite_note-129">&#91;129&#93;</a></sup>
</p><p>Algorithmic bias does not only include protected categories, but can also concern characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial <a href="/wiki/Ground_truth" title="Ground truth">ground truth</a>, and removing the bias from such a system is more difficult.<sup id="cite_ref-130" class="reference"><a href="#cite_note-130">&#91;130&#93;</a></sup> Furthermore, false and accidental <a href="/wiki/Correlations" class="mw-redirect" title="Correlations">correlations</a> can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.<sup id="cite_ref-Claburn_131-0" class="reference"><a href="#cite_note-Claburn-131">&#91;131&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Solutions">Solutions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=29" title="Edit section: Solutions"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A study of 84 policy guidelines on ethical AI found that fairness and "mitigation of unwanted bias" was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts.<sup id="cite_ref-:0_132-0" class="reference"><a href="#cite_note-:0-132">&#91;132&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Technical_2">Technical</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=30" title="Edit section: Technical"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1033289096"><div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Fairness_(machine_learning)" title="Fairness (machine learning)">Fairness (machine learning)</a></div>
<p>There have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent fields focus on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its <a href="/wiki/Confusion_matrix" title="Confusion matrix">confusion matrix</a> (or table of confusion).<sup id="cite_ref-133" class="reference"><a href="#cite_note-133">&#91;133&#93;</a></sup><sup id="cite_ref-134" class="reference"><a href="#cite_note-134">&#91;134&#93;</a></sup><sup id="cite_ref-135" class="reference"><a href="#cite_note-135">&#91;135&#93;</a></sup><sup id="cite_ref-136" class="reference"><a href="#cite_note-136">&#91;136&#93;</a></sup><sup id="cite_ref-137" class="reference"><a href="#cite_note-137">&#91;137&#93;</a></sup><sup id="cite_ref-138" class="reference"><a href="#cite_note-138">&#91;138&#93;</a></sup><sup id="cite_ref-139" class="reference"><a href="#cite_note-139">&#91;139&#93;</a></sup><sup id="cite_ref-140" class="reference"><a href="#cite_note-140">&#91;140&#93;</a></sup><sup id="cite_ref-141" class="reference"><a href="#cite_note-141">&#91;141&#93;</a></sup> Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model.<sup id="cite_ref-142" class="reference"><a href="#cite_note-142">&#91;142&#93;</a></sup> Using machine learning to detect bias is called, "conducting an AI audit", where the "auditor" is an algorithm that goes through the AI model and the training data to identify biases.<sup id="cite_ref-143" class="reference"><a href="#cite_note-143">&#91;143&#93;</a></sup>
Ensuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information
from its input signals, because this is typically implicit in other signals. For example, the hobbies, sports and schools attended
by a job candidate might reveal their gender to the software, even when this is removed from the analysis. Solutions to this
problem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected
and sensitive information about the subject, as first demonstrated in <sup id="cite_ref-144" class="reference"><a href="#cite_note-144">&#91;144&#93;</a></sup> where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature. A simpler method was proposed in the context of word embeddings, and involves removing information that is correlated with the protected characteristic.<sup id="cite_ref-Sutton_145-0" class="reference"><a href="#cite_note-Sutton-145">&#91;145&#93;</a></sup>
</p><p>Currently, a new <a href="/wiki/IEEE_Standards_Association" title="IEEE Standards Association">IEEE standard</a> is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or <a href="/wiki/End_user" title="End user">end users</a>) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the <a rel="nofollow" class="external text" href="https://www.computer.org/web/standards/s2esc">Software &amp; Systems Engineering Standards Committee</a>, a committee chartered by the <a href="/wiki/IEEE_Computer_Society" class="mw-redirect" title="IEEE Computer Society">IEEE Computer Society</a>. A draft of the standard is expected to be submitted for balloting in June 2019.<sup id="cite_ref-146" class="reference"><a href="#cite_note-146">&#91;146&#93;</a></sup><sup id="cite_ref-147" class="reference"><a href="#cite_note-147">&#91;147&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Transparency_and_monitoring">Transparency and monitoring</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=31" title="Edit section: Transparency and monitoring"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1033289096"><div role="note" class="hatnote navigation-not-searchable">Further information: <a href="/wiki/Algorithmic_transparency" title="Algorithmic transparency">Algorithmic transparency</a></div>
<p>Ethics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results.<sup id="cite_ref-148" class="reference"><a href="#cite_note-148">&#91;148&#93;</a></sup> Such solutions include the consideration of the "right to understanding" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed.<sup id="cite_ref-:2_149-0" class="reference"><a href="#cite_note-:2-149">&#91;149&#93;</a></sup> Toward this end, a movement for "<a href="/wiki/Explainable_artificial_intelligence" title="Explainable artificial intelligence">Explainable AI</a>" is already underway within organizations such as <a href="/wiki/DARPA" title="DARPA">DARPA</a>, for reasons that go beyond the remedy of bias.<sup id="cite_ref-150" class="reference"><a href="#cite_note-150">&#91;150&#93;</a></sup> <a href="/wiki/PricewaterhouseCoopers" class="mw-redirect" title="PricewaterhouseCoopers">Price Waterhouse Coopers</a>, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.<sup id="cite_ref-151" class="reference"><a href="#cite_note-151">&#91;151&#93;</a></sup>
</p><p>An initial approach towards transparency included the <a href="/wiki/Open-source_software" title="Open-source software">open-sourcing of algorithms</a>.<sup id="cite_ref-152" class="reference"><a href="#cite_note-152">&#91;152&#93;</a></sup> Software code can be looked into and improvements can be proposed through <a href="/wiki/Comparison_of_source-code-hosting_facilities" title="Comparison of source-code-hosting facilities">source-code-hosting facilities</a>. However, this approach doesn't necessarily produce the intended effects. Companies and organizations can share all possible documentation and code, but this does not establish transparency if the audience doesn't understand the information given. Therefore, the role of an interested critical audience is worth exploring in relation to transparency. Algorithms cannot be held accountable without a critical audience.<sup id="cite_ref-153" class="reference"><a href="#cite_note-153">&#91;153&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Right_to_remedy">Right to remedy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=32" title="Edit section: Right to remedy"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>From a regulatory perspective, the <a href="/wiki/Toronto_Declaration" title="Toronto Declaration">Toronto Declaration</a> calls for applying a human rights framework to harms caused by algorithmic bias.<sup id="cite_ref-154" class="reference"><a href="#cite_note-154">&#91;154&#93;</a></sup> This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes.<sup id="cite_ref-155" class="reference"><a href="#cite_note-155">&#91;155&#93;</a></sup> Others propose the need for clear liability insurance mechanisms.<sup id="cite_ref-156" class="reference"><a href="#cite_note-156">&#91;156&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Diversity_and_inclusion">Diversity and inclusion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=33" title="Edit section: Diversity and inclusion"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Amid concerns that the design of AI systems is primarily the domain of white, male engineers,<sup id="cite_ref-157" class="reference"><a href="#cite_note-157">&#91;157&#93;</a></sup> a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems.<sup id="cite_ref-:2_149-1" class="reference"><a href="#cite_note-:2-149">&#91;149&#93;</a></sup><sup id="cite_ref-:0_132-1" class="reference"><a href="#cite_note-:0-132">&#91;132&#93;</a></sup> For example, just 12% of machine learning engineers are women,<sup id="cite_ref-158" class="reference"><a href="#cite_note-158">&#91;158&#93;</a></sup> with black AI leaders pointing to a "diversity crisis" in the field.<sup id="cite_ref-159" class="reference"><a href="#cite_note-159">&#91;159&#93;</a></sup> Groups like <a href="/wiki/Black_in_AI" title="Black in AI">Black in AI</a> and <a href="/w/index.php?title=Queer_in_AI&amp;action=edit&amp;redlink=1" class="new" title="Queer in AI (page does not exist)">Queer in AI</a> are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research.<sup id="cite_ref-160" class="reference"><a href="#cite_note-160">&#91;160&#93;</a></sup> Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of <a href="/wiki/Intersectional_feminism" class="mw-redirect" title="Intersectional feminism">intersectionality</a> to the design of algorithms.<sup id="cite_ref-161" class="reference"><a href="#cite_note-161">&#91;161&#93;</a></sup><sup id="cite_ref-DataFeminism_162-0" class="reference"><a href="#cite_note-DataFeminism-162">&#91;162&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 4">&#58;&#8202;4&#8202;</span></sup> Researchers at the University of Cambridge have argued that addressing racial diversity is hampered by the "whiteness" of the culture of AI.<sup id="cite_ref-163" class="reference"><a href="#cite_note-163">&#91;163&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Interdisciplinarity_and_Collaboration">Interdisciplinarity and Collaboration</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=34" title="Edit section: Interdisciplinarity and Collaboration"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Integrating <a href="/wiki/Interdisciplinarity" title="Interdisciplinarity">interdisciplinarity</a> and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias. Integrating insights, expertise, and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society. An example of this in AI research is PACT or <i>Participatory Approach to enable Capabilities in communiTies,</i> a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact.<sup id="cite_ref-164" class="reference"><a href="#cite_note-164">&#91;164&#93;</a></sup> This framework identifies guiding principals for stakeholder participation when working on AI for Social Good (AI4SG) projects. PACT attempts to reify the importance of decolonizing and power-shifting efforts in the design of human-centered AI solutions. An academic initiative in this regard is the Stanford University's Institute for Human-Centered Artificial Intelligence which aims to foster multidisciplinary collaboration. The mission of the institute is to advance artificial intelligence (AI) research, education, policy and practice to improve the human condition. <sup id="cite_ref-165" class="reference"><a href="#cite_note-165">&#91;165&#93;</a></sup>
</p><p>Collaboration with outside experts and various stakeholders facilitates ethical, inclusive, and accountable development of intelligent systems. It incorporates ethical considerations, understands the social and cultural context, promotes human-centered design, leverages technical expertise, and addresses policy and legal considerations.<sup id="cite_ref-166" class="reference"><a href="#cite_note-166">&#91;166&#93;</a></sup> Collaboration across disciplines is essential to effectively mitigate bias in AI systems and ensure that AI technologies are fair, transparent, and accountable.
</p>
<h2><span class="mw-headline" id="Regulation">Regulation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=35" title="Edit section: Regulation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Europe">Europe</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=36" title="Edit section: Europe"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/General_Data_Protection_Regulation" title="General Data Protection Regulation">General Data Protection Regulation</a> (GDPR), the <a href="/wiki/European_Union" title="European Union">European Union</a>'s revised data protection regime that was implemented in 2018, addresses "Automated individual decision-making, including <a href="/wiki/Profiling_(information_science)" title="Profiling (information science)">profiling</a>" in Article 22. These rules prohibit "solely" automated decisions which have a "significant" or "legal" effect on an individual, unless they are explicitly authorised by consent, contract, or <a href="/wiki/Member_state_of_the_European_Union" title="Member state of the European Union">member state</a> law. Where they are permitted, there must be safeguards in place, such as a right to a <a href="/wiki/Human-in-the-loop" title="Human-in-the-loop">human-in-the-loop</a>, and a non-binding <a href="/wiki/Right_to_an_explanation" class="mw-redirect" title="Right to an explanation">right to an explanation</a> of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the <a href="/wiki/Data_Protection_Directive" title="Data Protection Directive">Data Protection Directive</a>. The original automated decision rules and safeguards found in French law since the late 1970s.<sup id="cite_ref-167" class="reference"><a href="#cite_note-167">&#91;167&#93;</a></sup>
</p><p>
The GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in <a href="/wiki/Recital_(law)" title="Recital (law)">recital</a> 71,<sup id="cite_ref-:1_168-0" class="reference"><a href="#cite_note-:1-168">&#91;168&#93;</a></sup> noting that</p><blockquote><p>the controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate&#160;... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.</p></blockquote><p>Like the non-binding <a href="/wiki/Right_to_an_explanation" class="mw-redirect" title="Right to an explanation">right to an explanation</a> in recital 71, the problem is the non-binding nature of <a href="/wiki/Recital_(law)" title="Recital (law)">recitals</a>.<sup id="cite_ref-169" class="reference"><a href="#cite_note-169">&#91;169&#93;</a></sup> While it has been treated as a requirement by the <a href="/wiki/Article_29_Working_Party" class="mw-redirect" title="Article 29 Working Party">Article 29 Working Party</a> that advised on the implementation of data protection law,<sup id="cite_ref-:1_168-1" class="reference"><a href="#cite_note-:1-168">&#91;168&#93;</a></sup> its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.<sup id="cite_ref-Edwards_170-0" class="reference"><a href="#cite_note-Edwards-170">&#91;170&#93;</a></sup>
</p><h3><span class="mw-headline" id="United_States">United States</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=37" title="Edit section: United States"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used.<sup id="cite_ref-Singer_171-0" class="reference"><a href="#cite_note-Singer-171">&#91;171&#93;</a></sup> Many policies are self-enforced or controlled by the <a href="/wiki/Federal_Trade_Commission" title="Federal Trade Commission">Federal Trade Commission</a>.<sup id="cite_ref-Singer_171-1" class="reference"><a href="#cite_note-Singer-171">&#91;171&#93;</a></sup> In 2016, the Obama administration released the <a href="/w/index.php?title=National_Artificial_Intelligence_Research_and_Development_Strategic_Plan&amp;action=edit&amp;redlink=1" class="new" title="National Artificial Intelligence Research and Development Strategic Plan (page does not exist)">National Artificial Intelligence Research and Development Strategic Plan</a>,<sup id="cite_ref-ObamaAdmin_172-0" class="reference"><a href="#cite_note-ObamaAdmin-172">&#91;172&#93;</a></sup> which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to "design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases". Intended only as guidance, the report did not create any legal precedent.<sup id="cite_ref-NSTC_173-0" class="reference"><a href="#cite_note-NSTC-173">&#91;173&#93;</a></sup><sup class="reference nowrap"><span title="Page / location: 26">&#58;&#8202;26&#8202;</span></sup>
</p><p>In 2017, <a href="/wiki/New_York_City" title="New York City">New York City</a> passed the first <a href="/wiki/Algorithmic_accountability" title="Algorithmic accountability">algorithmic accountability</a> bill in the United States.<sup id="cite_ref-Kirchner_174-0" class="reference"><a href="#cite_note-Kirchner-174">&#91;174&#93;</a></sup> The bill, which went into effect on January 1, 2018, required "the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems."<sup id="cite_ref-NYC_175-0" class="reference"><a href="#cite_note-NYC-175">&#91;175&#93;</a></sup> The task force is required to present findings and recommendations for further regulatory action in 2019.<sup id="cite_ref-Powles_176-0" class="reference"><a href="#cite_note-Powles-176">&#91;176&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="India">India</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=38" title="Edit section: India"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>On July 31, 2018, a draft of the Personal Data Bill was presented.<sup id="cite_ref-177" class="reference"><a href="#cite_note-177">&#91;177&#93;</a></sup> The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for "harm resulting from any processing or any kind of processing undertaken by the fiduciary". It defines "any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal" or "any discriminatory treatment" as a source of harm that could arise from improper use of data. It also makes special provisions for people of "Intersex status".<sup id="cite_ref-178" class="reference"><a href="#cite_note-178">&#91;178&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=39" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><a href="/wiki/Fairness_(machine_learning)" title="Fairness (machine learning)">Fairness (machine learning)</a></li>
<li><a href="/wiki/Hallucination_(artificial_intelligence)" title="Hallucination (artificial intelligence)">Hallucination (artificial intelligence)</a></li>
<li><a href="/wiki/Misaligned_goals_in_artificial_intelligence" title="Misaligned goals in artificial intelligence">Misaligned goals in artificial intelligence</a></li>
<li><a href="/wiki/Predictive_policing" title="Predictive policing">Predictive policing</a></li>
<li><a href="/wiki/SenseTime" title="SenseTime">SenseTime</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=40" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1133582631">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style><cite id="CITEREFJacobi2001" class="citation web cs1">Jacobi, Jennifer (September 13, 2001). <a rel="nofollow" class="external text" href="https://worldwide.espacenet.com/publicationDetails/biblio?CC=us&amp;NR=7113917&amp;KC=&amp;FT=E&amp;locale=en_EP">"Patent #US2001021914"</a>. <i>Espacenet</i><span class="reference-accessdate">. Retrieved <span class="nowrap">July 4,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Espacenet&amp;rft.atitle=Patent+%23US2001021914&amp;rft.date=2001-09-13&amp;rft.aulast=Jacobi&amp;rft.aufirst=Jennifer&amp;rft_id=https%3A%2F%2Fworldwide.espacenet.com%2FpublicationDetails%2Fbiblio%3FCC%3Dus%26NR%3D7113917%26KC%3D%26FT%3DE%26locale%3Den_EP&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Striphas-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-Striphas_2-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFStriphas" class="citation web cs1">Striphas, Ted. <a rel="nofollow" class="external text" href="http://culturedigitally.org/2012/02/what-is-an-algorithm/">"What is an Algorithm? – Culture Digitally"</a>. <i>culturedigitally.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 20,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=culturedigitally.org&amp;rft.atitle=What+is+an+Algorithm%3F+%E2%80%93+Culture+Digitally&amp;rft.aulast=Striphas&amp;rft.aufirst=Ted&amp;rft_id=http%3A%2F%2Fculturedigitally.org%2F2012%2F02%2Fwhat-is-an-algorithm%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Cormen-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-Cormen_3-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFCormenLeisersonRivestStein2009" class="citation book cs1">Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2009). <span class="cs1-lock-limited" title="Free access subject to limited trial, subscription normally required"><a rel="nofollow" class="external text" href="https://archive.org/details/introductiontoal00corm_805"><i>Introduction to Algorithms</i></a></span> (3rd&#160;ed.). Cambridge, Mass.: MIT Press. p.&#160;<a rel="nofollow" class="external text" href="https://archive.org/details/introductiontoal00corm_805/page/n25">5</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-262-03384-8" title="Special:BookSources/978-0-262-03384-8"><bdi>978-0-262-03384-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Algorithms&amp;rft.place=Cambridge%2C+Mass.&amp;rft.pages=5&amp;rft.edition=3rd&amp;rft.pub=MIT+Press&amp;rft.date=2009&amp;rft.isbn=978-0-262-03384-8&amp;rft.aulast=Cormen&amp;rft.aufirst=Thomas+H.&amp;rft.au=Leiserson%2C+Charles+E.&amp;rft.au=Rivest%2C+Ronald+L.&amp;rft.au=Stein%2C+Clifford&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fintroductiontoal00corm_805&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Kitchin-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-Kitchin_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Kitchin_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Kitchin_4-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKitchin2016" class="citation journal cs1">Kitchin, Rob (February 25, 2016). <a rel="nofollow" class="external text" href="http://mural.maynoothuniversity.ie/11591/1/Kitchin_Thinking_2017.pdf">"Thinking critically about and researching algorithms"</a> <span class="cs1-format">(PDF)</span>. <i>Information, Communication &amp; Society</i>. <b>20</b> (1): 14–29. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F1369118X.2016.1154087">10.1080/1369118X.2016.1154087</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:13798875">13798875</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information%2C+Communication+%26+Society&amp;rft.atitle=Thinking+critically+about+and+researching+algorithms&amp;rft.volume=20&amp;rft.issue=1&amp;rft.pages=14-29&amp;rft.date=2016-02-25&amp;rft_id=info%3Adoi%2F10.1080%2F1369118X.2016.1154087&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A13798875%23id-name%3DS2CID&amp;rft.aulast=Kitchin&amp;rft.aufirst=Rob&amp;rft_id=http%3A%2F%2Fmural.maynoothuniversity.ie%2F11591%2F1%2FKitchin_Thinking_2017.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-GoogleAlgorithms-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-GoogleAlgorithms_5-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.google.com/search/howsearchworks/algorithms/">"How Google Search Works"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+Google+Search+Works&amp;rft_id=https%3A%2F%2Fwww.google.com%2Fsearch%2Fhowsearchworks%2Falgorithms%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Luckerson-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-Luckerson_6-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLuckerson" class="citation magazine cs1">Luckerson, Victor. <a rel="nofollow" class="external text" href="http://time.com/collection-post/3950525/facebook-news-feed-algorithm/">"Here's How Your Facebook News Feed Actually Works"</a>. <i><a href="/wiki/Time_(magazine)" title="Time (magazine)">Time</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Time&amp;rft.atitle=Here%27s+How+Your+Facebook+News+Feed+Actually+Works&amp;rft.aulast=Luckerson&amp;rft.aufirst=Victor&amp;rft_id=http%3A%2F%2Ftime.com%2Fcollection-post%2F3950525%2Ffacebook-news-feed-algorithm%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Vanderbilt-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-Vanderbilt_7-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFVanderbilt2013" class="citation magazine cs1">Vanderbilt, Tom (August 7, 2013). <a rel="nofollow" class="external text" href="https://www.wired.com/2013/08/qq_netflix-algorithm/">"The Science Behind the Netflix Algorithms That Decide What You'll Watch Next"</a>. <i>Wired</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=The+Science+Behind+the+Netflix+Algorithms+That+Decide+What+You%27ll+Watch+Next&amp;rft.date=2013-08-07&amp;rft.aulast=Vanderbilt&amp;rft.aufirst=Tom&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2013%2F08%2Fqq_netflix-algorithm%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-AngwinMattu-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-AngwinMattu_8-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAngwinMattu2016" class="citation web cs1"><a href="/wiki/Julia_Angwin" title="Julia Angwin">Angwin, Julia</a>; Mattu, Surya (September 20, 2016). <a rel="nofollow" class="external text" href="https://www.propublica.org/article/amazon-says-it-puts-customers-first-but-its-pricing-algorithm-doesnt">"Amazon Says It Puts Customers First. But Its Pricing Algorithm Doesn't — ProPublica"</a>. <i>ProPublica</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ProPublica&amp;rft.atitle=Amazon+Says+It+Puts+Customers+First.+But+Its+Pricing+Algorithm+Doesn%27t+%E2%80%94+ProPublica&amp;rft.date=2016-09-20&amp;rft.aulast=Angwin&amp;rft.aufirst=Julia&amp;rft.au=Mattu%2C+Surya&amp;rft_id=https%3A%2F%2Fwww.propublica.org%2Farticle%2Famazon-says-it-puts-customers-first-but-its-pricing-algorithm-doesnt&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Livingstone-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-Livingstone_9-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLivingstone2017" class="citation web cs1">Livingstone, Rob (March 13, 2017). <a rel="nofollow" class="external text" href="https://theconversation.com/the-future-of-online-advertising-is-big-data-and-algorithms-69297">"The future of online advertising is big data and algorithms"</a>. <i>The Conversation</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Conversation&amp;rft.atitle=The+future+of+online+advertising+is+big+data+and+algorithms&amp;rft.date=2017-03-13&amp;rft.aulast=Livingstone&amp;rft.aufirst=Rob&amp;rft_id=http%3A%2F%2Ftheconversation.com%2Fthe-future-of-online-advertising-is-big-data-and-algorithms-69297&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Hickman-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-Hickman_10-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFHickman2013" class="citation news cs1">Hickman, Leo (July 1, 2013). <a rel="nofollow" class="external text" href="https://www.theguardian.com/science/2013/jul/01/how-algorithms-rule-world-nsa">"How algorithms rule the world"</a>. <i>The Guardian</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=How+algorithms+rule+the+world&amp;rft.date=2013-07-01&amp;rft.aulast=Hickman&amp;rft.aufirst=Leo&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fscience%2F2013%2Fjul%2F01%2Fhow-algorithms-rule-world-nsa&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Seaver-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-Seaver_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Seaver_11-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Seaver_11-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Seaver_11-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-Seaver_11-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-Seaver_11-5"><sup><i><b>f</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSeaver" class="citation web cs1">Seaver, Nick. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20171201012555/https://static1.squarespace.com/static/55eb004ee4b0518639d59d9b/t/55ece1bfe4b030b2e8302e1e/1441587647177/seaverMiT8.pdf">"Knowing Algorithms"</a> <span class="cs1-format">(PDF)</span>. Media in Transition 8, Cambridge, MA, April 2013. Archived from <a rel="nofollow" class="external text" href="https://static1.squarespace.com/static/55eb004ee4b0518639d59d9b/t/55ece1bfe4b030b2e8302e1e/1441587647177/seaverMiT8.pdf">the original</a> <span class="cs1-format">(PDF)</span> on December 1, 2017<span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Knowing+Algorithms&amp;rft.pub=Media+in+Transition+8%2C+Cambridge%2C+MA%2C+April+2013&amp;rft.aulast=Seaver&amp;rft.aufirst=Nick&amp;rft_id=https%3A%2F%2Fstatic1.squarespace.com%2Fstatic%2F55eb004ee4b0518639d59d9b%2Ft%2F55ece1bfe4b030b2e8302e1e%2F1441587647177%2FseaverMiT8.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Graham-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-Graham_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Graham_12-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Graham_12-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGraham2016" class="citation journal cs1">Graham, Stephen D.N. (July 2016). <a rel="nofollow" class="external text" href="http://dro.dur.ac.uk/194/1/194.pdf">"Software-sorted geographies"</a> <span class="cs1-format">(PDF)</span>. <i>Progress in Human Geography</i> (Submitted manuscript). <b>29</b> (5): 562–580. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1191%2F0309132505ph568oa">10.1191/0309132505ph568oa</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:19119278">19119278</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Progress+in+Human+Geography&amp;rft.atitle=Software-sorted+geographies&amp;rft.volume=29&amp;rft.issue=5&amp;rft.pages=562-580&amp;rft.date=2016-07&amp;rft_id=info%3Adoi%2F10.1191%2F0309132505ph568oa&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A19119278%23id-name%3DS2CID&amp;rft.aulast=Graham&amp;rft.aufirst=Stephen+D.N.&amp;rft_id=http%3A%2F%2Fdro.dur.ac.uk%2F194%2F1%2F194.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Tewell-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-Tewell_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Tewell_13-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Tewell_13-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFTewell2016" class="citation journal cs1">Tewell, Eamon (April 4, 2016). <a rel="nofollow" class="external text" href="http://muse.jhu.edu/article/613843">"Toward the Resistant Reading of Information: Google, Resistant Spectatorship, and Critical Information Literacy"</a>. <i>Portal: Libraries and the Academy</i>. <b>16</b> (2): 289–310. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1353%2Fpla.2016.0017">10.1353/pla.2016.0017</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1530-7131">1530-7131</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:55749077">55749077</a><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Portal%3A+Libraries+and+the+Academy&amp;rft.atitle=Toward+the+Resistant+Reading+of+Information%3A+Google%2C+Resistant+Spectatorship%2C+and+Critical+Information+Literacy&amp;rft.volume=16&amp;rft.issue=2&amp;rft.pages=289-310&amp;rft.date=2016-04-04&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A55749077%23id-name%3DS2CID&amp;rft.issn=1530-7131&amp;rft_id=info%3Adoi%2F10.1353%2Fpla.2016.0017&amp;rft.aulast=Tewell&amp;rft.aufirst=Eamon&amp;rft_id=http%3A%2F%2Fmuse.jhu.edu%2Farticle%2F613843&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFCrawford2013" class="citation journal cs1">Crawford, Kate (April 1, 2013). <a rel="nofollow" class="external text" href="https://hbr.org/2013/04/the-hidden-biases-in-big-data">"The Hidden Biases in Big Data"</a>. <i>Harvard Business Review</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Harvard+Business+Review&amp;rft.atitle=The+Hidden+Biases+in+Big+Data&amp;rft.date=2013-04-01&amp;rft.aulast=Crawford&amp;rft.aufirst=Kate&amp;rft_id=https%3A%2F%2Fhbr.org%2F2013%2F04%2Fthe-hidden-biases-in-big-data&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-FriedmanNissenbaum-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-FriedmanNissenbaum_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-6"><sup><i><b>g</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-7"><sup><i><b>h</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-8"><sup><i><b>i</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-9"><sup><i><b>j</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-10"><sup><i><b>k</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-11"><sup><i><b>l</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-12"><sup><i><b>m</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-13"><sup><i><b>n</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-14"><sup><i><b>o</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-15"><sup><i><b>p</b></i></sup></a> <a href="#cite_ref-FriedmanNissenbaum_15-16"><sup><i><b>q</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFFriedmanNissenbaum1996" class="citation journal cs1">Friedman, Batya; Nissenbaum, Helen (July 1996). <a rel="nofollow" class="external text" href="https://nissenbaum.tech.cornell.edu/papers/biasincomputers.pdf">"Bias in Computer Systems"</a> <span class="cs1-format">(PDF)</span>. <i>ACM Transactions on Information Systems</i>. <b>14</b> (3): 330–347. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F230538.230561">10.1145/230538.230561</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:207195759">207195759</a><span class="reference-accessdate">. Retrieved <span class="nowrap">March 10,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+Transactions+on+Information+Systems&amp;rft.atitle=Bias+in+Computer+Systems&amp;rft.volume=14&amp;rft.issue=3&amp;rft.pages=330-347&amp;rft.date=1996-07&amp;rft_id=info%3Adoi%2F10.1145%2F230538.230561&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A207195759%23id-name%3DS2CID&amp;rft.aulast=Friedman&amp;rft.aufirst=Batya&amp;rft.au=Nissenbaum%2C+Helen&amp;rft_id=https%3A%2F%2Fnissenbaum.tech.cornell.edu%2Fpapers%2Fbiasincomputers.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Gillespie_et_al-16"><span class="mw-cite-backlink">^ <a href="#cite_ref-Gillespie_et_al_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Gillespie_et_al_16-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Gillespie_et_al_16-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Gillespie_et_al_16-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-Gillespie_et_al_16-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-Gillespie_et_al_16-5"><sup><i><b>f</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGillespieBoczkowskiFoot2014" class="citation book cs1">Gillespie, Tarleton; Boczkowski, Pablo; Foot, Kristin (2014). <i>Media Technologies</i>. Cambridge: MIT Press. pp.&#160;1–30. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780262525374" title="Special:BookSources/9780262525374"><bdi>9780262525374</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Media+Technologies&amp;rft.place=Cambridge&amp;rft.pages=1-30&amp;rft.pub=MIT+Press&amp;rft.date=2014&amp;rft.isbn=9780262525374&amp;rft.aulast=Gillespie&amp;rft.aufirst=Tarleton&amp;rft.au=Boczkowski%2C+Pablo&amp;rft.au=Foot%2C+Kristin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-TowCenter-17"><span class="mw-cite-backlink">^ <a href="#cite_ref-TowCenter_17-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-TowCenter_17-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDiakopoulos" class="citation web cs1">Diakopoulos, Nicholas. <a rel="nofollow" class="external text" href="https://towcenter.org/research/algorithmic-accountability-on-the-investigation-of-black-boxes-2/">"Algorithmic Accountability: On the Investigation of Black Boxes |"</a>. <i>towcenter.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=towcenter.org&amp;rft.atitle=Algorithmic+Accountability%3A+On+the+Investigation+of+Black+Boxes+%7C&amp;rft.aulast=Diakopoulos&amp;rft.aufirst=Nicholas&amp;rft_id=https%3A%2F%2Ftowcenter.org%2Fresearch%2Falgorithmic-accountability-on-the-investigation-of-black-boxes-2%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Lipartito-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-Lipartito_18-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLipartito2011" class="citation report cs1">Lipartito, Kenneth (January 6, 2011). <a rel="nofollow" class="external text" href="https://mpra.ub.uni-muenchen.de/28142/1/MPRA_paper_28142.pdf">The Narrative and the Algorithm: Genres of Credit Reporting from the Nineteenth Century to Today</a> <span class="cs1-format">(PDF)</span> (Submitted manuscript). <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.2139%2FSSRN.1736283">10.2139/SSRN.1736283</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:166742927">166742927</a>. <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1736283">1736283</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=The+Narrative+and+the+Algorithm%3A+Genres+of+Credit+Reporting+from+the+Nineteenth+Century+to+Today&amp;rft.date=2011-01-06&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A166742927%23id-name%3DS2CID&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1736283%23id-name%3DSSRN&amp;rft_id=info%3Adoi%2F10.2139%2FSSRN.1736283&amp;rft.aulast=Lipartito&amp;rft.aufirst=Kenneth&amp;rft_id=https%3A%2F%2Fmpra.ub.uni-muenchen.de%2F28142%2F1%2FMPRA_paper_28142.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-GoodmanFlaxman2016-19"><span class="mw-cite-backlink">^ <a href="#cite_ref-GoodmanFlaxman2016_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-GoodmanFlaxman2016_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGoodmanFlaxman2017" class="citation journal cs1">Goodman, Bryce; Flaxman, Seth (2017). "EU regulations on algorithmic decision-making and a "right to explanation"<span class="cs1-kern-right"></span>". <i><a href="/wiki/AI_Magazine" class="mw-redirect" title="AI Magazine">AI Magazine</a></i>. <b>38</b> (3): 50. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1606.08813">1606.08813</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1609%2Faimag.v38i3.2741">10.1609/aimag.v38i3.2741</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:7373959">7373959</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+Magazine&amp;rft.atitle=EU+regulations+on+algorithmic+decision-making+and+a+%22right+to+explanation%22&amp;rft.volume=38&amp;rft.issue=3&amp;rft.pages=50&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1606.08813&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A7373959%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1609%2Faimag.v38i3.2741&amp;rft.aulast=Goodman&amp;rft.aufirst=Bryce&amp;rft.au=Flaxman%2C+Seth&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Weizenbaum1976-20"><span class="mw-cite-backlink">^ <a href="#cite_ref-Weizenbaum1976_20-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Weizenbaum1976_20-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Weizenbaum1976_20-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Weizenbaum1976_20-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-Weizenbaum1976_20-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-Weizenbaum1976_20-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-Weizenbaum1976_20-6"><sup><i><b>g</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWeizenbaum1976" class="citation book cs1">Weizenbaum, Joseph (1976). <span class="cs1-lock-registration" title="Free registration required"><a rel="nofollow" class="external text" href="https://archive.org/details/computerpowerhum0000weiz"><i>Computer Power and Human Reason: From Judgment to Calculation</i></a></span>. San Francisco: W.H. Freeman. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-7167-0464-5" title="Special:BookSources/978-0-7167-0464-5"><bdi>978-0-7167-0464-5</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Computer+Power+and+Human+Reason%3A+From+Judgment+to+Calculation&amp;rft.place=San+Francisco&amp;rft.pub=W.H.+Freeman&amp;rft.date=1976&amp;rft.isbn=978-0-7167-0464-5&amp;rft.aulast=Weizenbaum&amp;rft.aufirst=Joseph&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fcomputerpowerhum0000weiz&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Goffrey-21"><span class="mw-cite-backlink">^ <a href="#cite_ref-Goffrey_21-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Goffrey_21-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGoffrey2008" class="citation book cs1">Goffrey, Andrew (2008). "Algorithm". In Fuller, Matthew (ed.). <span class="cs1-lock-limited" title="Free access subject to limited trial, subscription normally required"><a rel="nofollow" class="external text" href="https://archive.org/details/softwarestudiesl00full_007"><i>Software Studies: A Lexicon</i></a></span>. Cambridge, Mass.: MIT Press. pp.&#160;<a rel="nofollow" class="external text" href="https://archive.org/details/softwarestudiesl00full_007/page/n29">15</a>–20. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4356-4787-9" title="Special:BookSources/978-1-4356-4787-9"><bdi>978-1-4356-4787-9</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Algorithm&amp;rft.btitle=Software+Studies%3A+A+Lexicon&amp;rft.place=Cambridge%2C+Mass.&amp;rft.pages=15-20&amp;rft.pub=MIT+Press&amp;rft.date=2008&amp;rft.isbn=978-1-4356-4787-9&amp;rft.aulast=Goffrey&amp;rft.aufirst=Andrew&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fsoftwarestudiesl00full_007&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-LowryMacpherson-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-LowryMacpherson_22-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLowryMacpherson1988" class="citation journal cs1">Lowry, Stella; Macpherson, Gordon (March 5, 1988). <a rel="nofollow" class="external text" href="http://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2545288&amp;blobtype=pdf">"A Blot on the Profession"</a>. <i>British Medical Journal</i>. <b>296</b> (6623): 657–8. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1136%2Fbmj.296.6623.657">10.1136/bmj.296.6623.657</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2545288">2545288</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/3128356">3128356</a><span class="reference-accessdate">. Retrieved <span class="nowrap">November 17,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=British+Medical+Journal&amp;rft.atitle=A+Blot+on+the+Profession&amp;rft.volume=296&amp;rft.issue=6623&amp;rft.pages=657-8&amp;rft.date=1988-03-05&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2545288%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F3128356&amp;rft_id=info%3Adoi%2F10.1136%2Fbmj.296.6623.657&amp;rft.aulast=Lowry&amp;rft.aufirst=Stella&amp;rft.au=Macpherson%2C+Gordon&amp;rft_id=http%3A%2F%2Feuropepmc.org%2Fbackend%2Fptpmcrender.fcgi%3Faccid%3DPMC2545288%26blobtype%3Dpdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Miller-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-Miller_23-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFMiller2018" class="citation journal cs1">Miller, Alex P. (July 26, 2018). <a rel="nofollow" class="external text" href="https://hbr.org/2018/07/want-less-biased-decisions-use-algorithms">"Want Less-Biased Decisions? Use Algorithms"</a>. <i>Harvard Business Review</i><span class="reference-accessdate">. Retrieved <span class="nowrap">July 31,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Harvard+Business+Review&amp;rft.atitle=Want+Less-Biased+Decisions%3F+Use+Algorithms&amp;rft.date=2018-07-26&amp;rft.aulast=Miller&amp;rft.aufirst=Alex+P.&amp;rft_id=https%3A%2F%2Fhbr.org%2F2018%2F07%2Fwant-less-biased-decisions-use-algorithms&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Introna1-24"><span class="mw-cite-backlink">^ <a href="#cite_ref-Introna1_24-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Introna1_24-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Introna1_24-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFIntrona2011" class="citation journal cs1">Introna, Lucas D. (December 2, 2011). "The Enframing of Code". <i>Theory, Culture &amp; Society</i>. <b>28</b> (6): 113–141. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F0263276411418131">10.1177/0263276411418131</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:145190381">145190381</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Theory%2C+Culture+%26+Society&amp;rft.atitle=The+Enframing+of+Code&amp;rft.volume=28&amp;rft.issue=6&amp;rft.pages=113-141&amp;rft.date=2011-12-02&amp;rft_id=info%3Adoi%2F10.1177%2F0263276411418131&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A145190381%23id-name%3DS2CID&amp;rft.aulast=Introna&amp;rft.aufirst=Lucas+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Bogost-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-Bogost_25-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBogost2015" class="citation web cs1">Bogost, Ian (January 15, 2015). <a rel="nofollow" class="external text" href="https://www.theatlantic.com/technology/archive/2015/01/the-cathedral-of-computation/384300/">"The Cathedral of Computation"</a>. <i>The Atlantic</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Atlantic&amp;rft.atitle=The+Cathedral+of+Computation&amp;rft.date=2015-01-15&amp;rft.aulast=Bogost&amp;rft.aufirst=Ian&amp;rft_id=https%3A%2F%2Fwww.theatlantic.com%2Ftechnology%2Farchive%2F2015%2F01%2Fthe-cathedral-of-computation%2F384300%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-IntronaWood-26"><span class="mw-cite-backlink">^ <a href="#cite_ref-IntronaWood_26-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-IntronaWood_26-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-IntronaWood_26-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-IntronaWood_26-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-IntronaWood_26-4"><sup><i><b>e</b></i></sup></a> <a href="#cite_ref-IntronaWood_26-5"><sup><i><b>f</b></i></sup></a> <a href="#cite_ref-IntronaWood_26-6"><sup><i><b>g</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFIntronaWood2004" class="citation journal cs1">Introna, Lucas; Wood, David (2004). <a rel="nofollow" class="external text" href="http://nbn-resolving.de/urn:nbn:de:0168-ssoar-200675">"Picturing algorithmic surveillance: the politics of facial recognition systems"</a>. <i>Surveillance &amp; Society</i>. <b>2</b>: 177–198<span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Surveillance+%26+Society&amp;rft.atitle=Picturing+algorithmic+surveillance%3A+the+politics+of+facial+recognition+systems&amp;rft.volume=2&amp;rft.pages=177-198&amp;rft.date=2004&amp;rft.aulast=Introna&amp;rft.aufirst=Lucas&amp;rft.au=Wood%2C+David&amp;rft_id=http%3A%2F%2Fnbn-resolving.de%2Furn%3Anbn%3Ade%3A0168-ssoar-200675&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Introna2-27"><span class="mw-cite-backlink">^ <a href="#cite_ref-Introna2_27-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Introna2_27-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Introna2_27-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Introna2_27-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFIntrona2006" class="citation journal cs1">Introna, Lucas D. (December 21, 2006). "Maintaining the reversibility of foldings: Making the ethics (politics) of information technology visible". <i>Ethics and Information Technology</i>. <b>9</b> (1): 11–25. <a href="/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.1313">10.1.1.154.1313</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10676-006-9133-z">10.1007/s10676-006-9133-z</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:17355392">17355392</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Ethics+and+Information+Technology&amp;rft.atitle=Maintaining+the+reversibility+of+foldings%3A+Making+the+ethics+%28politics%29+of+information+technology+visible&amp;rft.volume=9&amp;rft.issue=1&amp;rft.pages=11-25&amp;rft.date=2006-12-21&amp;rft_id=https%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.154.1313%23id-name%3DCiteSeerX&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A17355392%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2Fs10676-006-9133-z&amp;rft.aulast=Introna&amp;rft.aufirst=Lucas+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-ShirkyAuthority-28"><span class="mw-cite-backlink">^ <a href="#cite_ref-ShirkyAuthority_28-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ShirkyAuthority_28-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFShirky" class="citation web cs1">Shirky, Clay. <a rel="nofollow" class="external text" href="http://www.shirky.com/weblog/2009/11/a-speculative-post-on-the-idea-of-algorithmic-authority/">"A Speculative Post on the Idea of Algorithmic Authority Clay Shirky"</a>. <i>www.shirky.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 20,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.shirky.com&amp;rft.atitle=A+Speculative+Post+on+the+Idea+of+Algorithmic+Authority+Clay+Shirky&amp;rft.aulast=Shirky&amp;rft.aufirst=Clay&amp;rft_id=http%3A%2F%2Fwww.shirky.com%2Fweblog%2F2009%2F11%2Fa-speculative-post-on-the-idea-of-algorithmic-authority%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Ziewitz1-29"><span class="mw-cite-backlink">^ <a href="#cite_ref-Ziewitz1_29-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Ziewitz1_29-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFZiewitz2016" class="citation journal cs1">Ziewitz, Malte (January 1, 2016). <a rel="nofollow" class="external text" href="http://revistas.ucm.es/index.php/ESMP/article/view/58040">"Governing Algorithms: Myth, Mess, and Methods"</a>. <i>Science, Technology, &amp; Human Values</i>. <b>41</b> (1): 3–16. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F0162243915608948">10.1177/0162243915608948</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0162-2439">0162-2439</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:148023125">148023125</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science%2C+Technology%2C+%26+Human+Values&amp;rft.atitle=Governing+Algorithms%3A+Myth%2C+Mess%2C+and+Methods&amp;rft.volume=41&amp;rft.issue=1&amp;rft.pages=3-16&amp;rft.date=2016-01-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A148023125%23id-name%3DS2CID&amp;rft.issn=0162-2439&amp;rft_id=info%3Adoi%2F10.1177%2F0162243915608948&amp;rft.aulast=Ziewitz&amp;rft.aufirst=Malte&amp;rft_id=http%3A%2F%2Frevistas.ucm.es%2Findex.php%2FESMP%2Farticle%2Fview%2F58040&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Lash-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-Lash_30-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLash2016" class="citation journal cs1">Lash, Scott (June 30, 2016). "Power after Hegemony". <i>Theory, Culture &amp; Society</i>. <b>24</b> (3): 55–78. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F0263276407075956">10.1177/0263276407075956</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:145639801">145639801</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Theory%2C+Culture+%26+Society&amp;rft.atitle=Power+after+Hegemony&amp;rft.volume=24&amp;rft.issue=3&amp;rft.pages=55-78&amp;rft.date=2016-06-30&amp;rft_id=info%3Adoi%2F10.1177%2F0263276407075956&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A145639801%23id-name%3DS2CID&amp;rft.aulast=Lash&amp;rft.aufirst=Scott&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Garcia-31"><span class="mw-cite-backlink">^ <a href="#cite_ref-Garcia_31-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Garcia_31-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGarcia2016" class="citation journal cs1">Garcia, Megan (January 1, 2016). "Racist in the Machine". <i>World Policy Journal</i>. <b>33</b> (4): 111–117. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1215%2F07402775-3813015">10.1215/07402775-3813015</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:151595343">151595343</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=World+Policy+Journal&amp;rft.atitle=Racist+in+the+Machine&amp;rft.volume=33&amp;rft.issue=4&amp;rft.pages=111-117&amp;rft.date=2016-01-01&amp;rft_id=info%3Adoi%2F10.1215%2F07402775-3813015&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A151595343%23id-name%3DS2CID&amp;rft.aulast=Garcia&amp;rft.aufirst=Megan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://facctconference.org/2021/press-release.html">"ACM FAccT 2021 Registration"</a>. <i>fatconference.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 14,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=fatconference.org&amp;rft.atitle=ACM+FAccT+2021+Registration&amp;rft_id=https%3A%2F%2Ffacctconference.org%2F2021%2Fpress-release.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Ochigame-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-Ochigame_33-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFOchigame2019" class="citation web cs1">Ochigame, Rodrigo (December 20, 2019). <a rel="nofollow" class="external text" href="https://theintercept.com/2019/12/20/mit-ethical-ai-artificial-intelligence/">"The Invention of "Ethical AI": How Big Tech Manipulates Academia to Avoid Regulation"</a>. <i>The Intercept</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Intercept&amp;rft.atitle=The+Invention+of+%22Ethical+AI%22%3A+How+Big+Tech+Manipulates+Academia+to+Avoid+Regulation&amp;rft.date=2019-12-20&amp;rft.aulast=Ochigame&amp;rft.aufirst=Rodrigo&amp;rft_id=https%3A%2F%2Ftheintercept.com%2F2019%2F12%2F20%2Fmit-ethical-ai-artificial-intelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-SergotEtAl-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-SergotEtAl_34-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSergotSadriKowalskiKriwaczek1986" class="citation journal cs1">Sergot, MJ; Sadri, F; Kowalski, RA; Kriwaczek, F; Hammond, P; Cory, HT (May 1986). <a rel="nofollow" class="external text" href="https://web.stanford.edu/class/cs227/Readings/BritishNationalityAct.pdf">"The British Nationality Act as a Logic Program"</a> <span class="cs1-format">(PDF)</span>. <i>Communications of the ACM</i>. <b>29</b> (5): 370–386. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F5689.5920">10.1145/5689.5920</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:5665107">5665107</a><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=The+British+Nationality+Act+as+a+Logic+Program&amp;rft.volume=29&amp;rft.issue=5&amp;rft.pages=370-386&amp;rft.date=1986-05&amp;rft_id=info%3Adoi%2F10.1145%2F5689.5920&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A5665107%23id-name%3DS2CID&amp;rft.aulast=Sergot&amp;rft.aufirst=MJ&amp;rft.au=Sadri%2C+F&amp;rft.au=Kowalski%2C+RA&amp;rft.au=Kriwaczek%2C+F&amp;rft.au=Hammond%2C+P&amp;rft.au=Cory%2C+HT&amp;rft_id=https%3A%2F%2Fweb.stanford.edu%2Fclass%2Fcs227%2FReadings%2FBritishNationalityAct.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.brookings.edu/articles/to-stop-algorithmic-bias-we-first-have-to-define-it/">"To stop algorithmic bias, we first have to define it"</a>. <i>Brookings</i><span class="reference-accessdate">. Retrieved <span class="nowrap">June 27,</span> 2023</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Brookings&amp;rft.atitle=To+stop+algorithmic+bias%2C+we+first+have+to+define+it&amp;rft_id=https%3A%2F%2Fwww.brookings.edu%2Farticles%2Fto-stop-algorithmic-bias-we-first-have-to-define-it%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFEvansMathews2019" class="citation news cs1">Evans, Melanie; Mathews, Anna Wilde (October 24, 2019). <a rel="nofollow" class="external text" href="https://www.wsj.com/articles/researchers-find-racial-bias-in-hospital-algorithm-11571941096">"Researchers Find Racial Bias in Hospital Algorithm"</a>. <i>Wall Street Journal</i>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0099-9660">0099-9660</a><span class="reference-accessdate">. Retrieved <span class="nowrap">June 27,</span> 2023</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wall+Street+Journal&amp;rft.atitle=Researchers+Find+Racial+Bias+in+Hospital+Algorithm&amp;rft.date=2019-10-24&amp;rft.issn=0099-9660&amp;rft.aulast=Evans&amp;rft.aufirst=Melanie&amp;rft.au=Mathews%2C+Anna+Wilde&amp;rft_id=https%3A%2F%2Fwww.wsj.com%2Farticles%2Fresearchers-find-racial-bias-in-hospital-algorithm-11571941096&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.brookings.edu/articles/to-stop-algorithmic-bias-we-first-have-to-define-it/">"To stop algorithmic bias, we first have to define it"</a>. <i>Brookings</i><span class="reference-accessdate">. Retrieved <span class="nowrap">June 27,</span> 2023</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Brookings&amp;rft.atitle=To+stop+algorithmic+bias%2C+we+first+have+to+define+it&amp;rft_id=https%3A%2F%2Fwww.brookings.edu%2Farticles%2Fto-stop-algorithmic-bias-we-first-have-to-define-it%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Gillespie-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-Gillespie_38-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGillespie" class="citation web cs1">Gillespie, Tarleton. <a rel="nofollow" class="external text" href="http://culturedigitally.org/2014/06/algorithm-draft-digitalkeyword/">"Algorithm &#91;draft&#93; &#91;#digitalkeywords&#93; – Culture Digitally"</a>. <i>culturedigitally.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 20,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=culturedigitally.org&amp;rft.atitle=Algorithm+%5Bdraft%5D+%5B%23digitalkeywords%5D+%E2%80%93+Culture+Digitally&amp;rft.aulast=Gillespie&amp;rft.aufirst=Tarleton&amp;rft_id=http%3A%2F%2Fculturedigitally.org%2F2014%2F06%2Falgorithm-draft-digitalkeyword%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Roth-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-Roth_39-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFRoth1990" class="citation journal cs1">Roth, A. E. 1524–1528. (December 14, 1990). <a rel="nofollow" class="external text" href="https://stanford.edu/~alroth/science.html">"New physicians: A natural experiment in market organization"</a>. <i>Science</i>. <b>250</b> (4987): 1524–1528. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/1990Sci...250.1524R">1990Sci...250.1524R</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1126%2Fscience.2274783">10.1126/science.2274783</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/2274783">2274783</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:23259274">23259274</a><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=New+physicians%3A+A+natural+experiment+in+market+organization&amp;rft.volume=250&amp;rft.issue=4987&amp;rft.pages=1524-1528&amp;rft.date=1990-12-14&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.2274783&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A23259274%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F2274783&amp;rft_id=info%3Abibcode%2F1990Sci...250.1524R&amp;rft.aulast=Roth&amp;rft.aufirst=A.+E.+1524%E2%80%931528.&amp;rft_id=https%3A%2F%2Fstanford.edu%2F~alroth%2Fscience.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Kuang-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-Kuang_40-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKuang2017" class="citation news cs1">Kuang, Cliff (November 21, 2017). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html">"Can A.I. Be Taught to Explain Itself?"</a>. <i><a href="/wiki/The_New_York_Times_Magazine" title="The New York Times Magazine">The New York Times Magazine</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times+Magazine&amp;rft.atitle=Can+A.I.+Be+Taught+to+Explain+Itself%3F&amp;rft.date=2017-11-21&amp;rft.aulast=Kuang&amp;rft.aufirst=Cliff&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2017%2F11%2F21%2Fmagazine%2Fcan-ai-be-taught-to-explain-itself.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-JouvenalPredPol-41"><span class="mw-cite-backlink">^ <a href="#cite_ref-JouvenalPredPol_41-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-JouvenalPredPol_41-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFJouvenal2016" class="citation news cs1">Jouvenal, Justin (November 17, 2016). <a rel="nofollow" class="external text" href="https://www.washingtonpost.com/local/public-safety/police-are-using-software-to-predict-crime-is-it-a-holy-grail-or-biased-against-minorities/2016/11/17/525a6649-0472-440a-aae1-b283aa8e5de8_story.html">"Police are using software to predict crime. Is it a 'holy grail' or biased against minorities?"</a>. <i>Washington Post</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 25,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Washington+Post&amp;rft.atitle=Police+are+using+software+to+predict+crime.+Is+it+a+%27holy+grail%27+or+biased+against+minorities%3F&amp;rft.date=2016-11-17&amp;rft.aulast=Jouvenal&amp;rft.aufirst=Justin&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Flocal%2Fpublic-safety%2Fpolice-are-using-software-to-predict-crime-is-it-a-holy-grail-or-biased-against-minorities%2F2016%2F11%2F17%2F525a6649-0472-440a-aae1-b283aa8e5de8_story.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Chamma-42"><span class="mw-cite-backlink">^ <a href="#cite_ref-Chamma_42-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Chamma_42-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFChamma2016" class="citation web cs1">Chamma, Maurice (February 3, 2016). <a rel="nofollow" class="external text" href="https://www.themarshallproject.org/2016/02/03/policing-the-future?ref=hp-2-111#.UyhBLnmlj">"Policing the Future"</a>. <i>The Marshall Project</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 25,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Marshall+Project&amp;rft.atitle=Policing+the+Future&amp;rft.date=2016-02-03&amp;rft.aulast=Chamma&amp;rft.aufirst=Maurice&amp;rft_id=https%3A%2F%2Fwww.themarshallproject.org%2F2016%2F02%2F03%2Fpolicing-the-future%3Fref%3Dhp-2-111%23.UyhBLnmlj&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-LumIsaac-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-LumIsaac_43-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLumIsaac2016" class="citation journal cs1">Lum, Kristian; Isaac, William (October 2016). <a rel="nofollow" class="external text" href="https://doi.org/10.1111%2Fj.1740-9713.2016.00960.x">"To predict and serve?"</a>. <i>Significance</i>. <b>13</b> (5): 14–19. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1111%2Fj.1740-9713.2016.00960.x">10.1111/j.1740-9713.2016.00960.x</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Significance&amp;rft.atitle=To+predict+and+serve%3F&amp;rft.volume=13&amp;rft.issue=5&amp;rft.pages=14-19&amp;rft.date=2016-10&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1740-9713.2016.00960.x&amp;rft.aulast=Lum&amp;rft.aufirst=Kristian&amp;rft.au=Isaac%2C+William&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1111%252Fj.1740-9713.2016.00960.x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-SmithPredPol-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-SmithPredPol_44-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSmith2016" class="citation web cs1">Smith, Jack (October 9, 2016). <a rel="nofollow" class="external text" href="https://mic.com/articles/156286/crime-prediction-tool-pred-pol-only-amplifies-racially-biased-policing-study-shows">"Predictive policing only amplifies racial bias, study shows"</a>. <i>Mic</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 25,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Mic&amp;rft.atitle=Predictive+policing+only+amplifies+racial+bias%2C+study+shows&amp;rft.date=2016-10-09&amp;rft.aulast=Smith&amp;rft.aufirst=Jack&amp;rft_id=https%3A%2F%2Fmic.com%2Farticles%2F156286%2Fcrime-prediction-tool-pred-pol-only-amplifies-racially-biased-policing-study-shows&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-LumIsaacFAQ-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-LumIsaacFAQ_45-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLumIsaac2016" class="citation web cs1">Lum, Kristian; Isaac, William (October 1, 2016). <a rel="nofollow" class="external text" href="https://hrdag.org/2016/11/04/faqs-predpol/">"FAQs on Predictive Policing and Bias"</a>. <i>HRDAG</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 25,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=HRDAG&amp;rft.atitle=FAQs+on+Predictive+Policing+and+Bias&amp;rft.date=2016-10-01&amp;rft.aulast=Lum&amp;rft.aufirst=Kristian&amp;rft.au=Isaac%2C+William&amp;rft_id=https%3A%2F%2Fhrdag.org%2F2016%2F11%2F04%2Ffaqs-predpol%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSunNasraouiShafto2018" class="citation book cs1">Sun, Wenlong; Nasraoui, Olfa; Shafto, Patrick (2018). "Iterated Algorithmic Bias in the Interactive Machine Learning Process of Information Filtering". <i>Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management</i>. Seville, Spain: SCITEPRESS - Science and Technology Publications. pp.&#160;110–118. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.5220%2F0006938301100118">10.5220/0006938301100118</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9789897583308" title="Special:BookSources/9789897583308"><bdi>9789897583308</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Iterated+Algorithmic+Bias+in+the+Interactive+Machine+Learning+Process+of+Information+Filtering&amp;rft.btitle=Proceedings+of+the+10th+International+Joint+Conference+on+Knowledge+Discovery%2C+Knowledge+Engineering+and+Knowledge+Management&amp;rft.place=Seville%2C+Spain&amp;rft.pages=110-118&amp;rft.pub=SCITEPRESS+-+Science+and+Technology+Publications&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.5220%2F0006938301100118&amp;rft.isbn=9789897583308&amp;rft.aulast=Sun&amp;rft.aufirst=Wenlong&amp;rft.au=Nasraoui%2C+Olfa&amp;rft.au=Shafto%2C+Patrick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSinhaGleichRamani2018" class="citation journal cs1">Sinha, Ayan; Gleich, David F.; Ramani, Karthik (August 9, 2018). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6085300">"Gauss's law for networks directly reveals community boundaries"</a>. <i>Scientific Reports</i>. <b>8</b> (1): 11909. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018NatSR...811909S">2018NatSR...811909S</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fs41598-018-30401-0">10.1038/s41598-018-30401-0</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2045-2322">2045-2322</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6085300">6085300</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/30093660">30093660</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scientific+Reports&amp;rft.atitle=Gauss%27s+law+for+networks+directly+reveals+community+boundaries&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=11909&amp;rft.date=2018-08-09&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6085300%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2018NatSR...811909S&amp;rft_id=info%3Apmid%2F30093660&amp;rft_id=info%3Adoi%2F10.1038%2Fs41598-018-30401-0&amp;rft.issn=2045-2322&amp;rft.aulast=Sinha&amp;rft.aufirst=Ayan&amp;rft.au=Gleich%2C+David+F.&amp;rft.au=Ramani%2C+Karthik&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6085300&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFHao2018" class="citation web cs1">Hao, Karen (February 2018). <a rel="nofollow" class="external text" href="https://qz.com/1194566/google-is-finally-admitting-it-has-a-filter-bubble-problem/">"Google is finally admitting it has a filter-bubble problem"</a>. <i>Quartz</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 26,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Quartz&amp;rft.atitle=Google+is+finally+admitting+it+has+a+filter-bubble+problem&amp;rft.date=2018-02&amp;rft.aulast=Hao&amp;rft.aufirst=Karen&amp;rft_id=https%3A%2F%2Fqz.com%2F1194566%2Fgoogle-is-finally-admitting-it-has-a-filter-bubble-problem%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://fortune.com/2017/04/25/facebook-related-articles-filter-bubbles/">"Facebook Is Testing This New Feature to Fight 'Filter Bubbles'<span class="cs1-kern-right"></span>"</a>. <i>Fortune</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 26,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Fortune&amp;rft.atitle=Facebook+Is+Testing+This+New+Feature+to+Fight+%27Filter+Bubbles%27&amp;rft_id=http%3A%2F%2Ffortune.com%2F2017%2F04%2F25%2Ffacebook-related-articles-filter-bubbles%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Sandvig1-50"><span class="mw-cite-backlink">^ <a href="#cite_ref-Sandvig1_50-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Sandvig1_50-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSandvigHamiltonKarahaliosLangbort2014" class="citation journal cs1">Sandvig, Christian; Hamilton, Kevin; <a href="/wiki/Karrie_Karahalios" title="Karrie Karahalios">Karahalios, Karrie</a>; Langbort, Cedric (May 22, 2014). <a rel="nofollow" class="external text" href="http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf">"Auditing Algorithms: Research Methods for Detecting Discrimination on Internet Platforms"</a> <span class="cs1-format">(PDF)</span>. <i>64th Annual Meeting of the International Communication Association</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=64th+Annual+Meeting+of+the+International+Communication+Association&amp;rft.atitle=Auditing+Algorithms%3A+Research+Methods+for+Detecting+Discrimination+on+Internet+Platforms&amp;rft.date=2014-05-22&amp;rft.aulast=Sandvig&amp;rft.aufirst=Christian&amp;rft.au=Hamilton%2C+Kevin&amp;rft.au=Karahalios%2C+Karrie&amp;rft.au=Langbort%2C+Cedric&amp;rft_id=http%3A%2F%2Fwww-personal.umich.edu%2F~csandvig%2Fresearch%2FAuditing%2520Algorithms%2520--%2520Sandvig%2520--%2520ICA%25202014%2520Data%2520and%2520Discrimination%2520Preconference.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-BrinPage98-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-BrinPage98_51-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBrinPage" class="citation web cs1">Brin, Sergey; Page, Lawrence. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20190702020902/http://www7.scu.edu.au/1921/com1921.htm">"The Anatomy of a Search Engine"</a>. <i>www7.scu.edu.au</i>. Archived from <a rel="nofollow" class="external text" href="http://www7.scu.edu.au/1921/com1921.htm">the original</a> on July 2, 2019<span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www7.scu.edu.au&amp;rft.atitle=The+Anatomy+of+a+Search+Engine&amp;rft.aulast=Brin&amp;rft.aufirst=Sergey&amp;rft.au=Page%2C+Lawrence&amp;rft_id=http%3A%2F%2Fwww7.scu.edu.au%2F1921%2Fcom1921.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Epstein-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-Epstein_52-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFEpsteinRobertson2015" class="citation journal cs1">Epstein, Robert; Robertson, Ronald E. (August 18, 2015). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547273">"The search engine manipulation effect (SEME) and its possible impact on the outcomes of elections"</a>. <i>Proceedings of the National Academy of Sciences</i>. <b>112</b> (33): E4512–E4521. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2015PNAS..112E4512E">2015PNAS..112E4512E</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1073%2Fpnas.1419828112">10.1073/pnas.1419828112</a></span>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4547273">4547273</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/26243876">26243876</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences&amp;rft.atitle=The+search+engine+manipulation+effect+%28SEME%29+and+its+possible+impact+on+the+outcomes+of+elections&amp;rft.volume=112&amp;rft.issue=33&amp;rft.pages=E4512-E4521&amp;rft.date=2015-08-18&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4547273%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F26243876&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.1419828112&amp;rft_id=info%3Abibcode%2F2015PNAS..112E4512E&amp;rft.aulast=Epstein&amp;rft.aufirst=Robert&amp;rft.au=Robertson%2C+Ronald+E.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4547273&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Bond-etal-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-Bond-etal_53-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBondFarissJonesKramer2012" class="citation journal cs1">Bond, Robert M.; Fariss, Christopher J.; Jones, Jason J.; Kramer, Adam D. I.; Marlow, Cameron; Settle, Jaime E.; Fowler, James H. (September 13, 2012). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3834737">"A 61-million-person experiment in social influence and political mobilization"</a>. <i>Nature</i>. <b>489</b> (7415): 295–8. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2012Natur.489..295B">2012Natur.489..295B</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fnature11421">10.1038/nature11421</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0028-0836">0028-0836</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3834737">3834737</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/22972300">22972300</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=A+61-million-person+experiment+in+social+influence+and+political+mobilization&amp;rft.volume=489&amp;rft.issue=7415&amp;rft.pages=295-8&amp;rft.date=2012-09-13&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3834737%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2012Natur.489..295B&amp;rft_id=info%3Apmid%2F22972300&amp;rft_id=info%3Adoi%2F10.1038%2Fnature11421&amp;rft.issn=0028-0836&amp;rft.aulast=Bond&amp;rft.aufirst=Robert+M.&amp;rft.au=Fariss%2C+Christopher+J.&amp;rft.au=Jones%2C+Jason+J.&amp;rft.au=Kramer%2C+Adam+D.+I.&amp;rft.au=Marlow%2C+Cameron&amp;rft.au=Settle%2C+Jaime+E.&amp;rft.au=Fowler%2C+James+H.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3834737&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Zittrain-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-Zittrain_54-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFZittrain2014" class="citation journal cs1">Zittrain, Jonathan (2014). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20210304064053/http://cdn.harvardlawreview.org/wp-content/uploads/2014/06/vol127_Symposium_Zittrain.pdf">"Engineering an Election"</a> <span class="cs1-format">(PDF)</span>. <i>Harvard Law Review Forum</i>. <b>127</b>: 335–341. Archived from <a rel="nofollow" class="external text" href="http://cdn.harvardlawreview.org/wp-content/uploads/2014/06/vol127_Symposium_Zittrain.pdf">the original</a> <span class="cs1-format">(PDF)</span> on March 4, 2021<span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Harvard+Law+Review+Forum&amp;rft.atitle=Engineering+an+Election&amp;rft.volume=127&amp;rft.pages=335-341&amp;rft.date=2014&amp;rft.aulast=Zittrain&amp;rft.aufirst=Jonathan&amp;rft_id=http%3A%2F%2Fcdn.harvardlawreview.org%2Fwp-content%2Fuploads%2F2014%2F06%2Fvol127_Symposium_Zittrain.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Day-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-Day_55-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDay2016" class="citation web cs1">Day, Matt (August 31, 2016). <a rel="nofollow" class="external text" href="https://www.seattletimes.com/business/microsoft/how-linkedins-search-engine-may-reflect-a-bias/">"How LinkedIn's search engine may reflect a gender bias"</a>. <i><a href="/wiki/The_Seattle_Times" title="The Seattle Times">The Seattle Times</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 25,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Seattle+Times&amp;rft.atitle=How+LinkedIn%27s+search+engine+may+reflect+a+gender+bias&amp;rft.date=2016-08-31&amp;rft.aulast=Day&amp;rft.aufirst=Matt&amp;rft_id=https%3A%2F%2Fwww.seattletimes.com%2Fbusiness%2Fmicrosoft%2Fhow-linkedins-search-engine-may-reflect-a-bias%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-CrawfordSchultz-56"><span class="mw-cite-backlink">^ <a href="#cite_ref-CrawfordSchultz_56-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-CrawfordSchultz_56-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFCrawfordSchultz2014" class="citation journal cs1">Crawford, Kate; Schultz, Jason (2014). <a rel="nofollow" class="external text" href="http://lawdigitalcommons.bc.edu/bclr/vol55/iss1/4/">"Big Data and Due Process: Toward a Framework to Redress Predictive Privacy Harms"</a>. <i>Boston College Law Review</i>. <b>55</b> (1): 93–128<span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Boston+College+Law+Review&amp;rft.atitle=Big+Data+and+Due+Process%3A+Toward+a+Framework+to+Redress+Predictive+Privacy+Harms&amp;rft.volume=55&amp;rft.issue=1&amp;rft.pages=93-128&amp;rft.date=2014&amp;rft.aulast=Crawford&amp;rft.aufirst=Kate&amp;rft.au=Schultz%2C+Jason&amp;rft_id=http%3A%2F%2Flawdigitalcommons.bc.edu%2Fbclr%2Fvol55%2Fiss1%2F4%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Duhigg-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-Duhigg_57-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDuhigg2012" class="citation news cs1">Duhigg, Charles (February 16, 2012). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html">"How Companies Learn Your Secrets"</a>. <i>The New York Times Magazine</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times+Magazine&amp;rft.atitle=How+Companies+Learn+Your+Secrets&amp;rft.date=2012-02-16&amp;rft.aulast=Duhigg&amp;rft.aufirst=Charles&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2012%2F02%2F19%2Fmagazine%2Fshopping-habits.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Noble-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-Noble_58-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFNoble2012" class="citation journal cs1"><a href="/wiki/Safiya_Noble" title="Safiya Noble">Noble, Safiya</a> (2012). <a rel="nofollow" class="external text" href="https://safiyaunoble.files.wordpress.com/2012/03/54_search_engines.pdf">"Missed Connections: What Search Engines Say about Women"</a> <span class="cs1-format">(PDF)</span>. <i><a href="/wiki/Bitch_(magazine)" title="Bitch (magazine)">Bitch</a></i>. <b>12</b> (4): 37–41.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bitch&amp;rft.atitle=Missed+Connections%3A+What+Search+Engines+Say+about+Women&amp;rft.volume=12&amp;rft.issue=4&amp;rft.pages=37-41&amp;rft.date=2012&amp;rft.aulast=Noble&amp;rft.aufirst=Safiya&amp;rft_id=https%3A%2F%2Fsafiyaunoble.files.wordpress.com%2F2012%2F03%2F54_search_engines.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Guynn2-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-Guynn2_59-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGuynn2017" class="citation news cs1">Guynn, Jessica (March 16, 2017). <a rel="nofollow" class="external text" href="https://www.usatoday.com/story/tech/news/2017/03/16/google-flags-offensive-content-search-results/99235548/">"Google starts flagging offensive content in search results"</a>. <i>USA TODAY</i>. USA Today<span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=USA+TODAY&amp;rft.atitle=Google+starts+flagging+offensive+content+in+search+results&amp;rft.date=2017-03-16&amp;rft.aulast=Guynn&amp;rft.aufirst=Jessica&amp;rft_id=https%3A%2F%2Fwww.usatoday.com%2Fstory%2Ftech%2Fnews%2F2017%2F03%2F16%2Fgoogle-flags-offensive-content-search-results%2F99235548%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-SimoniteMIT-60"><span class="mw-cite-backlink"><b><a href="#cite_ref-SimoniteMIT_60-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSimonite" class="citation web cs1">Simonite, Tom. <a rel="nofollow" class="external text" href="https://www.technologyreview.com/s/539021/probing-the-dark-side-of-googles-ad-targeting-system/">"Study Suggests Google's Ad-Targeting System May Discriminate"</a>. <i>MIT Technology Review</i>. Massachusetts Institute of Technology<span class="reference-accessdate">. Retrieved <span class="nowrap">November 17,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=Study+Suggests+Google%27s+Ad-Targeting+System+May+Discriminate&amp;rft.aulast=Simonite&amp;rft.aufirst=Tom&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2Fs%2F539021%2Fprobing-the-dark-side-of-googles-ad-targeting-system%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPratesAvelarLamb2018" class="citation arxiv cs1">Prates, Marcelo O. R.; Avelar, Pedro H. C.; Lamb, Luis (2018). "Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1809.02208">1809.02208</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CY">cs.CY</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Assessing+Gender+Bias+in+Machine+Translation+--+A+Case+Study+with+Google+Translate&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1809.02208&amp;rft.aulast=Prates&amp;rft.aufirst=Marcelo+O.+R.&amp;rft.au=Avelar%2C+Pedro+H.+C.&amp;rft.au=Lamb%2C+Luis&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-62"><span class="mw-cite-backlink"><b><a href="#cite_ref-62">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPratesAvelarLamb2019" class="citation journal cs1">Prates, Marcelo O. R.; Avelar, Pedro H.; Lamb, Luís C. (2019). "Assessing gender bias in machine translation: A case study with Google Translate". <i>Neural Computing and Applications</i>. <b>32</b> (10): 6363–6381. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1809.02208">1809.02208</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs00521-019-04144-6">10.1007/s00521-019-04144-6</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:52179151">52179151</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computing+and+Applications&amp;rft.atitle=Assessing+gender+bias+in+machine+translation%3A+A+case+study+with+Google+Translate&amp;rft.volume=32&amp;rft.issue=10&amp;rft.pages=6363-6381&amp;rft.date=2019&amp;rft_id=info%3Aarxiv%2F1809.02208&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A52179151%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2Fs00521-019-04144-6&amp;rft.aulast=Prates&amp;rft.aufirst=Marcelo+O.+R.&amp;rft.au=Avelar%2C+Pedro+H.&amp;rft.au=Lamb%2C+Lu%C3%ADs+C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-63"><span class="mw-cite-backlink"><b><a href="#cite_ref-63">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFClaburn2018" class="citation news cs1">Claburn, Thomas (September 10, 2018). <a rel="nofollow" class="external text" href="https://www.theregister.com/2018/09/10/boffins_bash_google_translate_for_sexist_language/">"Boffins bash Google Translate for sexism"</a>. <i>The Register</i><span class="reference-accessdate">. Retrieved <span class="nowrap">April 28,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Register&amp;rft.atitle=Boffins+bash+Google+Translate+for+sexism&amp;rft.date=2018-09-10&amp;rft.aulast=Claburn&amp;rft.aufirst=Thomas&amp;rft_id=https%3A%2F%2Fwww.theregister.com%2F2018%2F09%2F10%2Fboffins_bash_google_translate_for_sexist_language%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-64"><span class="mw-cite-backlink"><b><a href="#cite_ref-64">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDastin2018" class="citation news cs1">Dastin, Jeffrey (October 9, 2018). <a rel="nofollow" class="external text" href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G">"Amazon scraps secret AI recruiting tool that showed bias against women"</a>. <i>Reuters</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Reuters&amp;rft.atitle=Amazon+scraps+secret+AI+recruiting+tool+that+showed+bias+against+women&amp;rft.date=2018-10-09&amp;rft.aulast=Dastin&amp;rft.aufirst=Jeffrey&amp;rft_id=https%3A%2F%2Fwww.reuters.com%2Farticle%2Fus-amazon-com-jobs-automation-insight%2Famazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-65"><span class="mw-cite-backlink"><b><a href="#cite_ref-65">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFVincent2018" class="citation web cs1">Vincent, James (October 10, 2018). <a rel="nofollow" class="external text" href="https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report">"Amazon reportedly scraps internal AI recruiting tool that was biased against women"</a>. <i>The Verge</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Verge&amp;rft.atitle=Amazon+reportedly+scraps+internal+AI+recruiting+tool+that+was+biased+against+women&amp;rft.date=2018-10-10&amp;rft.aulast=Vincent&amp;rft.aufirst=James&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2F2018%2F10%2F10%2F17958784%2Fai-recruiting-tool-bias-amazon-report&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-66"><span class="mw-cite-backlink"><b><a href="#cite_ref-66">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://songdata.ca/2019/10/01/reflecting-on-spotifys-recommender-system/">"Reflecting on Spotify's Recommender System – SongData"</a>. October 2019<span class="reference-accessdate">. Retrieved <span class="nowrap">August 7,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Reflecting+on+Spotify%27s+Recommender+System+%E2%80%93+SongData&amp;rft.date=2019-10&amp;rft_id=https%3A%2F%2Fsongdata.ca%2F2019%2F10%2F01%2Freflecting-on-spotifys-recommender-system%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Buolamwini-Gebru-67"><span class="mw-cite-backlink"><b><a href="#cite_ref-Buolamwini-Gebru_67-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBuolamwiniGebru2018" class="citation journal cs1">Buolamwini, Joy; Gebru, Timnit (January 21, 2018). <a rel="nofollow" class="external text" href="http://proceedings.mlr.press/v81/buolamwini18a.html">"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"</a>. <i>Proceedings of Machine Learning Research</i>. <b>81</b> (2018): 77–91<span class="reference-accessdate">. Retrieved <span class="nowrap">September 27,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+Machine+Learning+Research&amp;rft.atitle=Gender+Shades%3A+Intersectional+Accuracy+Disparities+in+Commercial+Gender+Classification&amp;rft.volume=81&amp;rft.issue=2018&amp;rft.pages=77-91&amp;rft.date=2018-01-21&amp;rft.aulast=Buolamwini&amp;rft.aufirst=Joy&amp;rft.au=Gebru%2C+Timnit&amp;rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv81%2Fbuolamwini18a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Noble2-68"><span class="mw-cite-backlink"><b><a href="#cite_ref-Noble2_68-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFNoble2018" class="citation book cs1">Noble, Safiya Umoja (February 20, 2018). <i>Algorithms of Oppression: How Search Engines Reinforce Racism</i>. New York: NYU Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1479837243" title="Special:BookSources/978-1479837243"><bdi>978-1479837243</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Algorithms+of+Oppression%3A+How+Search+Engines+Reinforce+Racism&amp;rft.place=New+York&amp;rft.pub=NYU+Press&amp;rft.date=2018-02-20&amp;rft.isbn=978-1479837243&amp;rft.aulast=Noble&amp;rft.aufirst=Safiya+Umoja&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Nakamura1-69"><span class="mw-cite-backlink">^ <a href="#cite_ref-Nakamura1_69-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Nakamura1_69-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFNakamura2009" class="citation book cs1">Nakamura, Lisa (2009). Magnet, Shoshana; Gates, Kelly (eds.). <i>The New Media of Surveillance</i>. London: Routledge. pp.&#160;149–162. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-415-56812-8" title="Special:BookSources/978-0-415-56812-8"><bdi>978-0-415-56812-8</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+New+Media+of+Surveillance&amp;rft.place=London&amp;rft.pages=149-162&amp;rft.pub=Routledge&amp;rft.date=2009&amp;rft.isbn=978-0-415-56812-8&amp;rft.aulast=Nakamura&amp;rft.aufirst=Lisa&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-70"><span class="mw-cite-backlink"><b><a href="#cite_ref-70">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFMarco_MarabelliSue_NewellValerie_Handunge2021" class="citation journal cs1">Marco Marabelli; Sue Newell; Valerie Handunge (2021). <a rel="nofollow" class="external text" href="https://www.sciencedirect.com/science/article/abs/pii/S0963868721000305">"The lifecycle of algorithmic decision-making systems: Organizational choices and ethical challenges"</a>. <i>Journal of Strategic Information Systems</i>. <b>30</b> (3): 1–15. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.jsis.2021.101683">10.1016/j.jsis.2021.101683</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Strategic+Information+Systems&amp;rft.atitle=The+lifecycle+of+algorithmic+decision-making+systems%3A+Organizational+choices+and+ethical+challenges&amp;rft.volume=30&amp;rft.issue=3&amp;rft.pages=1-15&amp;rft.date=2021&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jsis.2021.101683&amp;rft.au=Marco+Marabelli&amp;rft.au=Sue+Newell&amp;rft.au=Valerie+Handunge&amp;rft_id=https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fabs%2Fpii%2FS0963868721000305&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-71"><span class="mw-cite-backlink"><b><a href="#cite_ref-71">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAlexanderGyamerah1997" class="citation journal cs1">Alexander, Rudolph; Gyamerah, Jacquelyn (September 1997). "Differential Punishing of African Americans and Whites Who Possess Drugs: A Just Policy or a Continuation of the Past?". <i>Journal of Black Studies</i>. <b>28</b> (1): 97–111. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F002193479702800106">10.1177/002193479702800106</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0021-9347">0021-9347</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:152043501">152043501</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Black+Studies&amp;rft.atitle=Differential+Punishing+of+African+Americans+and+Whites+Who+Possess+Drugs%3A+A+Just+Policy+or+a+Continuation+of+the+Past%3F&amp;rft.volume=28&amp;rft.issue=1&amp;rft.pages=97-111&amp;rft.date=1997-09&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A152043501%23id-name%3DS2CID&amp;rft.issn=0021-9347&amp;rft_id=info%3Adoi%2F10.1177%2F002193479702800106&amp;rft.aulast=Alexander&amp;rft.aufirst=Rudolph&amp;rft.au=Gyamerah%2C+Jacquelyn&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-72"><span class="mw-cite-backlink"><b><a href="#cite_ref-72">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPetersilia1985" class="citation journal cs1">Petersilia, Joan (January 1985). "Racial Disparities in the Criminal Justice System: A Summary". <i>Crime &amp; Delinquency</i>. <b>31</b> (1): 15–34. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F0011128785031001002">10.1177/0011128785031001002</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0011-1287">0011-1287</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:146588630">146588630</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Crime+%26+Delinquency&amp;rft.atitle=Racial+Disparities+in+the+Criminal+Justice+System%3A+A+Summary&amp;rft.volume=31&amp;rft.issue=1&amp;rft.pages=15-34&amp;rft.date=1985-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A146588630%23id-name%3DS2CID&amp;rft.issn=0011-1287&amp;rft_id=info%3Adoi%2F10.1177%2F0011128785031001002&amp;rft.aulast=Petersilia&amp;rft.aufirst=Joan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Guynn-73"><span class="mw-cite-backlink">^ <a href="#cite_ref-Guynn_73-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Guynn_73-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGuynn2015" class="citation news cs1">Guynn, Jessica (July 1, 2015). <a rel="nofollow" class="external text" href="https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">"Google Photos labeled black people 'gorillas'<span class="cs1-kern-right"></span>"</a>. <i>USA TODAY</i>. USA Today<span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=USA+TODAY&amp;rft.atitle=Google+Photos+labeled+black+people+%27gorillas%27&amp;rft.date=2015-07-01&amp;rft.aulast=Guynn&amp;rft.aufirst=Jessica&amp;rft_id=https%3A%2F%2Fwww.usatoday.com%2Fstory%2Ftech%2F2015%2F07%2F01%2Fgoogle-apologizes-after-photos-identify-black-people-as-gorillas%2F29567465%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Rose-74"><span class="mw-cite-backlink"><b><a href="#cite_ref-Rose_74-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFRose2010" class="citation magazine cs1">Rose, Adam (January 22, 2010). <a rel="nofollow" class="external text" href="http://content.time.com/time/business/article/0,8599,1954643,00.html">"Are Face-Detection Cameras Racist?"</a>. <i>Time</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Time&amp;rft.atitle=Are+Face-Detection+Cameras+Racist%3F&amp;rft.date=2010-01-22&amp;rft.aulast=Rose&amp;rft.aufirst=Adam&amp;rft_id=http%3A%2F%2Fcontent.time.com%2Ftime%2Fbusiness%2Farticle%2F0%2C8599%2C1954643%2C00.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-75"><span class="mw-cite-backlink"><b><a href="#cite_ref-75">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation news cs1"><a rel="nofollow" class="external text" href="https://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/">"Alexa does not understand your accent"</a>. <i>Washington Post</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Washington+Post&amp;rft.atitle=Alexa+does+not+understand+your+accent&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Fgraphics%2F2018%2Fbusiness%2Falexa-does-not-understand-your-accent%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Sweeney-76"><span class="mw-cite-backlink"><b><a href="#cite_ref-Sweeney_76-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSweeney2013" class="citation arxiv cs1">Sweeney, Latanya (January 28, 2013). "Discrimination in Online Ad Delivery". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1301.6822">1301.6822</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.IR">cs.IR</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Discrimination+in+Online+Ad+Delivery&amp;rft.date=2013-01-28&amp;rft_id=info%3Aarxiv%2F1301.6822&amp;rft.aulast=Sweeney&amp;rft.aufirst=Latanya&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-77"><span class="mw-cite-backlink"><b><a href="#cite_ref-77">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBraun2015" class="citation journal cs1">Braun, Lundy (2015). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4631137">"Race, ethnicity and lung function: A brief history"</a>. <i>Canadian Journal of Respiratory Therapy</i>. <b>51</b> (4): 99–101. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1205-9838">1205-9838</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4631137">4631137</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/26566381">26566381</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Canadian+Journal+of+Respiratory+Therapy&amp;rft.atitle=Race%2C+ethnicity+and+lung+function%3A+A+brief+history&amp;rft.volume=51&amp;rft.issue=4&amp;rft.pages=99-101&amp;rft.date=2015&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4631137%23id-name%3DPMC&amp;rft.issn=1205-9838&amp;rft_id=info%3Apmid%2F26566381&amp;rft.aulast=Braun&amp;rft.aufirst=Lundy&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4631137&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-78"><span class="mw-cite-backlink"><b><a href="#cite_ref-78">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFRobinsonRensonNaimi2020" class="citation journal cs1">Robinson, Whitney R; Renson, Audrey; Naimi, Ashley I (April 1, 2020). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7868043">"Teaching yourself about structural racism will improve your machine learning"</a>. <i>Biostatistics</i>. <b>21</b> (2): 339–344. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1093%2Fbiostatistics%2Fkxz040">10.1093/biostatistics/kxz040</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1465-4644">1465-4644</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7868043">7868043</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/31742353">31742353</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biostatistics&amp;rft.atitle=Teaching+yourself+about+structural+racism+will+improve+your+machine+learning&amp;rft.volume=21&amp;rft.issue=2&amp;rft.pages=339-344&amp;rft.date=2020-04-01&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7868043%23id-name%3DPMC&amp;rft.issn=1465-4644&amp;rft_id=info%3Apmid%2F31742353&amp;rft_id=info%3Adoi%2F10.1093%2Fbiostatistics%2Fkxz040&amp;rft.aulast=Robinson&amp;rft.aufirst=Whitney+R&amp;rft.au=Renson%2C+Audrey&amp;rft.au=Naimi%2C+Ashley+I&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7868043&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-79"><span class="mw-cite-backlink"><b><a href="#cite_ref-79">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFJohnson2019" class="citation news cs1">Johnson, Carolyn Y. (October 24, 2019). <a rel="nofollow" class="external text" href="https://www.washingtonpost.com/health/2019/10/24/racial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients/">"Racial bias in a medical algorithm favors white patients over sicker black patients"</a>. <i>Washington Post</i><span class="reference-accessdate">. Retrieved <span class="nowrap">October 28,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Washington+Post&amp;rft.atitle=Racial+bias+in+a+medical+algorithm+favors+white+patients+over+sicker+black+patients&amp;rft.date=2019-10-24&amp;rft.aulast=Johnson&amp;rft.aufirst=Carolyn+Y.&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Fhealth%2F2019%2F10%2F24%2Fracial-bias-medical-algorithm-favors-white-patients-over-sicker-black-patients%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-80"><span class="mw-cite-backlink"><b><a href="#cite_ref-80">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBartlettMorseStantonWallace2019" class="citation journal cs1">Bartlett, Robert; Morse, Adair; Stanton, Richard; Wallace, Nancy (June 2019). <a rel="nofollow" class="external text" href="http://www.nber.org/papers/w25943">"Consumer-Lending Discrimination in the FinTech Era"</a>. <i>NBER Working Paper No. 25943</i>. Working Paper Series. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3386%2Fw25943">10.3386/w25943</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:242410791">242410791</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NBER+Working+Paper+No.+25943&amp;rft.atitle=Consumer-Lending+Discrimination+in+the+FinTech+Era&amp;rft.date=2019-06&amp;rft_id=info%3Adoi%2F10.3386%2Fw25943&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A242410791%23id-name%3DS2CID&amp;rft.aulast=Bartlett&amp;rft.aufirst=Robert&amp;rft.au=Morse%2C+Adair&amp;rft.au=Stanton%2C+Richard&amp;rft.au=Wallace%2C+Nancy&amp;rft_id=http%3A%2F%2Fwww.nber.org%2Fpapers%2Fw25943&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-81"><span class="mw-cite-backlink"><b><a href="#cite_ref-81">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFJeff_Larson2016" class="citation web cs1">Jeff Larson, Julia Angwin (May 23, 2016). <a rel="nofollow" class="external text" href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">"How We Analyzed the COMPAS Recidivism Algorithm"</a>. <i>ProPublica</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20190429190950/https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">Archived</a> from the original on April 29, 2019<span class="reference-accessdate">. Retrieved <span class="nowrap">June 19,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ProPublica&amp;rft.atitle=How+We+Analyzed+the+COMPAS+Recidivism+Algorithm&amp;rft.date=2016-05-23&amp;rft.aulast=Jeff+Larson&amp;rft.aufirst=Julia+Angwin&amp;rft_id=https%3A%2F%2Fwww.propublica.org%2Farticle%2Fhow-we-analyzed-the-compas-recidivism-algorithm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-82"><span class="mw-cite-backlink"><b><a href="#cite_ref-82">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.channelnewsasia.com/news/commentary/artificial-intelligence-big-data-bias-hiring-loans-key-challenge-11097374">"Commentary: Bad news. Artificial intelligence is biased"</a>. <i>CNA</i>. January 12, 2019. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20190112104421/https://www.channelnewsasia.com/news/commentary/artificial-intelligence-big-data-bias-hiring-loans-key-challenge-11097374">Archived</a> from the original on January 12, 2019<span class="reference-accessdate">. Retrieved <span class="nowrap">June 19,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CNA&amp;rft.atitle=Commentary%3A+Bad+news.+Artificial+intelligence+is+biased&amp;rft.date=2019-01-12&amp;rft_id=https%3A%2F%2Fwww.channelnewsasia.com%2Fnews%2Fcommentary%2Fartificial-intelligence-big-data-bias-hiring-loans-key-challenge-11097374&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-ProPublica-83"><span class="mw-cite-backlink">^ <a href="#cite_ref-ProPublica_83-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ProPublica_83-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAngwinLarsonMattuKirchner2016" class="citation web cs1">Angwin, Julia; Larson, Jeff; Mattu, Surya; Kirchner, Lauren (May 23, 2016). <a rel="nofollow" class="external text" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">"Machine Bias — ProPublica"</a>. <i>ProPublica</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ProPublica&amp;rft.atitle=Machine+Bias+%E2%80%94+ProPublica&amp;rft.date=2016-05-23&amp;rft.aulast=Angwin&amp;rft.aufirst=Julia&amp;rft.au=Larson%2C+Jeff&amp;rft.au=Mattu%2C+Surya&amp;rft.au=Kirchner%2C+Lauren&amp;rft_id=https%3A%2F%2Fwww.propublica.org%2Farticle%2Fmachine-bias-risk-assessments-in-criminal-sentencing&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Harcourt-84"><span class="mw-cite-backlink"><b><a href="#cite_ref-Harcourt_84-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFHarcourt2010" class="citation journal cs1">Harcourt, Bernard (September 16, 2010). <a rel="nofollow" class="external text" href="https://scholarship.law.columbia.edu/cgi/viewcontent.cgi?article=3568&amp;context=faculty_scholarship">"Risk as a Proxy for Race"</a>. <i>Federal Sentencing Reporter</i>. <b>27</b> (4): 237. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1525%2Ffsr.2015.27.4.237">10.1525/fsr.2015.27.4.237</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:53611813">53611813</a>. <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1677654">1677654</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Federal+Sentencing+Reporter&amp;rft.atitle=Risk+as+a+Proxy+for+Race&amp;rft.volume=27&amp;rft.issue=4&amp;rft.pages=237&amp;rft.date=2010-09-16&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A53611813%23id-name%3DS2CID&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1677654%23id-name%3DSSRN&amp;rft_id=info%3Adoi%2F10.1525%2Ffsr.2015.27.4.237&amp;rft.aulast=Harcourt&amp;rft.aufirst=Bernard&amp;rft_id=https%3A%2F%2Fscholarship.law.columbia.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D3568%26context%3Dfaculty_scholarship&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-85"><span class="mw-cite-backlink"><b><a href="#cite_ref-85">^</a></b></span> <span class="reference-text">Skeem J, Lowenkamp C, Risk, Race, &amp; Recidivism: Predictive Bias and Disparate Impact, (June 14, 2016). <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://ssrn.com/abstract=2687339">2687339</a></span>
</li>
<li id="cite_note-86"><span class="mw-cite-backlink"><b><a href="#cite_ref-86">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFThomasNunez2022" class="citation journal cs1">Thomas, C.; Nunez, A. (2022). <a rel="nofollow" class="external text" href="https://scholarship.law.umn.edu/cgi/viewcontent.cgi?article=1680&amp;context=lawineq">"Automating Judicial Discretion: How Algorithmic Risk Assessments in Pretrial Adjudications Violate Equal Protection Rights on the Basis of Race"</a>. <i><a href="/wiki/Law_%26_Inequality" class="mw-redirect" title="Law &amp; Inequality">Law &amp; Inequality</a></i>. <b>40</b> (2): 371–407. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.24926%2F25730037.649">10.24926/25730037.649</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Law+%26+Inequality&amp;rft.atitle=Automating+Judicial+Discretion%3A+How+Algorithmic+Risk+Assessments+in+Pretrial+Adjudications+Violate+Equal+Protection+Rights+on+the+Basis+of+Race&amp;rft.volume=40&amp;rft.issue=2&amp;rft.pages=371-407&amp;rft.date=2022&amp;rft_id=info%3Adoi%2F10.24926%2F25730037.649&amp;rft.aulast=Thomas&amp;rft.aufirst=C.&amp;rft.au=Nunez%2C+A.&amp;rft_id=https%3A%2F%2Fscholarship.law.umn.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1680%26context%3Dlawineq&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-AngwinGrassegger-87"><span class="mw-cite-backlink">^ <a href="#cite_ref-AngwinGrassegger_87-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AngwinGrassegger_87-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAngwinGrassegger2017" class="citation web cs1">Angwin, Julia; Grassegger, Hannes (June 28, 2017). <a rel="nofollow" class="external text" href="https://www.propublica.org/article/facebook-hate-speech-censorship-internal-documents-algorithms">"Facebook's Secret Censorship Rules Protect White Men From Hate Speech But Not Black Children — ProPublica"</a>. <i>ProPublica</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 20,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ProPublica&amp;rft.atitle=Facebook%27s+Secret+Censorship+Rules+Protect+White+Men+From+Hate+Speech+But+Not+Black+Children+%E2%80%94+ProPublica&amp;rft.date=2017-06-28&amp;rft.aulast=Angwin&amp;rft.aufirst=Julia&amp;rft.au=Grassegger%2C+Hannes&amp;rft_id=https%3A%2F%2Fwww.propublica.org%2Farticle%2Ffacebook-hate-speech-censorship-internal-documents-algorithms&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-AngwinVarnerTobin-88"><span class="mw-cite-backlink"><b><a href="#cite_ref-AngwinVarnerTobin_88-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAngwinVarnerTobin2017" class="citation news cs1">Angwin, Julia; Varner, Madeleine; Tobin, Ariana (September 14, 2017). <a rel="nofollow" class="external text" href="https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters">"Facebook Enabled Advertisers to Reach 'Jew Haters' — ProPublica"</a>. <i>ProPublica</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 20,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ProPublica&amp;rft.atitle=Facebook+Enabled+Advertisers+to+Reach+%27Jew+Haters%27+%E2%80%94+ProPublica&amp;rft.date=2017-09-14&amp;rft.aulast=Angwin&amp;rft.aufirst=Julia&amp;rft.au=Varner%2C+Madeleine&amp;rft.au=Tobin%2C+Ariana&amp;rft_id=https%3A%2F%2Fwww.propublica.org%2Farticle%2Ffacebook-enabled-advertisers-to-reach-jew-haters&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-89"><span class="mw-cite-backlink"><b><a href="#cite_ref-89">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSapCardGabrielChoi2019" class="citation conference cs1">Sap, Maarten; Card, Dallas; Gabriel, Saadia; Choi, Yejin; Smith, Noah A. (July 28 – August 2, 2019). <a rel="nofollow" class="external text" href="https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf">"The Risk of Racial Bias in Hate Speech Detection"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of the 57th Annual Meeting of the Association for Computational Linguist</i>. Florence, Italy: Association for Computational Linguistics. pp.&#160;1668–1678. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20190814194616/https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf">Archived</a> <span class="cs1-format">(PDF)</span> from the original on August 14, 2019.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=The+Risk+of+Racial+Bias+in+Hate+Speech+Detection&amp;rft.btitle=Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguist&amp;rft.place=Florence%2C+Italy&amp;rft.pages=1668-1678&amp;rft.pub=Association+for+Computational+Linguistics&amp;rft.date=2019-07-28%2F2019-08-02&amp;rft.aulast=Sap&amp;rft.aufirst=Maarten&amp;rft.au=Card%2C+Dallas&amp;rft.au=Gabriel%2C+Saadia&amp;rft.au=Choi%2C+Yejin&amp;rft.au=Smith%2C+Noah+A.&amp;rft_id=https%3A%2F%2Fhomes.cs.washington.edu%2F~msap%2Fpdfs%2Fsap2019risk.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-90"><span class="mw-cite-backlink"><b><a href="#cite_ref-90">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGhaffary2019" class="citation web cs1">Ghaffary, Shirin (August 15, 2019). <a rel="nofollow" class="external text" href="https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter">"The algorithms that detect hate speech online are biased against black people"</a>. <i>Vox</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 19,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Vox&amp;rft.atitle=The+algorithms+that+detect+hate+speech+online+are+biased+against+black+people&amp;rft.date=2019-08-15&amp;rft.aulast=Ghaffary&amp;rft.aufirst=Shirin&amp;rft_id=https%3A%2F%2Fwww.vox.com%2Frecode%2F2019%2F8%2F15%2F20806384%2Fsocial-media-hate-speech-bias-black-african-american-facebook-twitter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Furl2002-91"><span class="mw-cite-backlink"><b><a href="#cite_ref-Furl2002_91-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFFurl2002" class="citation journal cs1">Furl, N (December 2002). <a rel="nofollow" class="external text" href="https://doi.org/10.1207%2Fs15516709cog2606_4">"Face recognition algorithms and the other-race effect: computational mechanisms for a developmental contact hypothesis"</a>. <i>Cognitive Science</i>. <b>26</b> (6): 797–815. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1207%2Fs15516709cog2606_4">10.1207/s15516709cog2606_4</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Science&amp;rft.atitle=Face+recognition+algorithms+and+the+other-race+effect%3A+computational+mechanisms+for+a+developmental+contact+hypothesis&amp;rft.volume=26&amp;rft.issue=6&amp;rft.pages=797-815&amp;rft.date=2002-12&amp;rft_id=info%3Adoi%2F10.1207%2Fs15516709cog2606_4&amp;rft.aulast=Furl&amp;rft.aufirst=N&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1207%252Fs15516709cog2606_4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Raji-Gebru-Mitchell-2020-92"><span class="mw-cite-backlink"><b><a href="#cite_ref-Raji-Gebru-Mitchell-2020_92-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFRajiGebruMitchellBuolamwini2020" class="citation book cs1">Raji, Inioluwa Deborah; Gebru, Timnit; Mitchell, Margaret; Buolamwini, Joy; Lee, Joonseok; Denton, Emily (February 7, 2020). <a rel="nofollow" class="external text" href="https://dl.acm.org/doi/10.1145/3375627.3375820">"Saving Face"</a>. <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>. Association for Computing Machinery. pp.&#160;145–151. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2001.00964">2001.00964</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3375627.3375820">10.1145/3375627.3375820</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450371100" title="Special:BookSources/9781450371100"><bdi>9781450371100</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:209862419">209862419</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Saving+Face&amp;rft.btitle=Proceedings+of+the+AAAI%2FACM+Conference+on+AI%2C+Ethics%2C+and+Society&amp;rft.pages=145-151&amp;rft.pub=Association+for+Computing+Machinery&amp;rft.date=2020-02-07&amp;rft_id=info%3Aarxiv%2F2001.00964&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A209862419%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F3375627.3375820&amp;rft.isbn=9781450371100&amp;rft.aulast=Raji&amp;rft.aufirst=Inioluwa+Deborah&amp;rft.au=Gebru%2C+Timnit&amp;rft.au=Mitchell%2C+Margaret&amp;rft.au=Buolamwini%2C+Joy&amp;rft.au=Lee%2C+Joonseok&amp;rft.au=Denton%2C+Emily&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3375627.3375820&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-The_New_York_Times_2018_l903-93"><span class="mw-cite-backlink"><b><a href="#cite_ref-The_New_York_Times_2018_l903_93-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html">"Facial Recognition Is Accurate, if You're a White Guy"</a>. <i>The New York Times</i>. February 9, 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">August 24,</span> 2023</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Facial+Recognition+Is+Accurate%2C+if+You%27re+a+White+Guy&amp;rft.date=2018-02-09&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2018%2F02%2F09%2Ftechnology%2Ffacial-recognition-race-artificial-intelligence.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-94"><span class="mw-cite-backlink"><b><a href="#cite_ref-94">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBuolamwiniGebru2018" class="citation journal cs1">Buolamwini, Joy; Gebru, Timnit (2018). <a rel="nofollow" class="external text" href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">"Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification"</a> <span class="cs1-format">(PDF)</span>. <i>Proceedings of Machine Learning Research</i>. <b>81</b>: 1 &#8211; via MLR Press.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+Machine+Learning+Research&amp;rft.atitle=Gender+Shades%3A+Intersectional+Accuracy+Disparities+in+Commercial+Gender+Classification&amp;rft.volume=81&amp;rft.pages=1&amp;rft.date=2018&amp;rft.aulast=Buolamwini&amp;rft.aufirst=Joy&amp;rft.au=Gebru%2C+Timnit&amp;rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv81%2Fbuolamwini18a%2Fbuolamwini18a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Ananny-95"><span class="mw-cite-backlink"><b><a href="#cite_ref-Ananny_95-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFAnanny2011" class="citation web cs1">Ananny, Mike (April 14, 2011). <a rel="nofollow" class="external text" href="https://www.theatlantic.com/technology/archive/2011/04/the-curious-connection-between-apps-for-gay-men-and-sex-offenders/237340/">"The Curious Connection Between Apps for Gay Men and Sex Offenders"</a>. <i>The Atlantic</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Atlantic&amp;rft.atitle=The+Curious+Connection+Between+Apps+for+Gay+Men+and+Sex+Offenders&amp;rft.date=2011-04-14&amp;rft.aulast=Ananny&amp;rft.aufirst=Mike&amp;rft_id=https%3A%2F%2Fwww.theatlantic.com%2Ftechnology%2Farchive%2F2011%2F04%2Fthe-curious-connection-between-apps-for-gay-men-and-sex-offenders%2F237340%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Kafka2-96"><span class="mw-cite-backlink"><b><a href="#cite_ref-Kafka2_96-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKafka" class="citation web cs1">Kafka, Peter. <a rel="nofollow" class="external text" href="http://allthingsd.com/20090412/did-amazon-really-fail-this-weekend-the-twittersphere-says-yes/">"Did Amazon Really Fail This Weekend? The Twittersphere Says 'Yes,' Online Retailer Says 'Glitch.'<span class="cs1-kern-right"></span>"</a>. <i>AllThingsD</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 22,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=AllThingsD&amp;rft.atitle=Did+Amazon+Really+Fail+This+Weekend%3F+The+Twittersphere+Says+%27Yes%2C%27+Online+Retailer+Says+%27Glitch.%27&amp;rft.aulast=Kafka&amp;rft.aufirst=Peter&amp;rft_id=http%3A%2F%2Fallthingsd.com%2F20090412%2Fdid-amazon-really-fail-this-weekend-the-twittersphere-says-yes%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Kafka-97"><span class="mw-cite-backlink"><b><a href="#cite_ref-Kafka_97-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKafka" class="citation web cs1">Kafka, Peter. <a rel="nofollow" class="external text" href="http://allthingsd.com/20090413/amazon-apologizes-for-ham-fisted-cataloging-error/">"Amazon Apologizes for 'Ham-fisted Cataloging Error'<span class="cs1-kern-right"></span>"</a>. <i>AllThingsD</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 22,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=AllThingsD&amp;rft.atitle=Amazon+Apologizes+for+%27Ham-fisted+Cataloging+Error%27&amp;rft.aulast=Kafka&amp;rft.aufirst=Peter&amp;rft_id=http%3A%2F%2Fallthingsd.com%2F20090413%2Famazon-apologizes-for-ham-fisted-cataloging-error%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-98"><span class="mw-cite-backlink"><b><a href="#cite_ref-98">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFMatsakis2019" class="citation news cs1">Matsakis, Louise (February 22, 2019). <a rel="nofollow" class="external text" href="https://www.wired.com/story/facebook-female-friends-photo-search-bug/">"A 'Sexist' Search Bug Says More About Us Than Facebook"</a>. <i>Wired</i>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1059-1028">1059-1028</a><span class="reference-accessdate">. Retrieved <span class="nowrap">February 26,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=A+%27Sexist%27+Search+Bug+Says+More+About+Us+Than+Facebook&amp;rft.date=2019-02-22&amp;rft.issn=1059-1028&amp;rft.aulast=Matsakis&amp;rft.aufirst=Louise&amp;rft_id=https%3A%2F%2Fwww.wired.com%2Fstory%2Ffacebook-female-friends-photo-search-bug%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-99"><span class="mw-cite-backlink"><b><a href="#cite_ref-99">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.vox.com/future-perfect/2019/4/19/18412674/ai-bias-facial-recognition-black-gay-transgender">"Some AI just shouldn't exist"</a>. April 19, 2019.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Some+AI+just+shouldn%27t+exist&amp;rft.date=2019-04-19&amp;rft_id=https%3A%2F%2Fwww.vox.com%2Ffuture-perfect%2F2019%2F4%2F19%2F18412674%2Fai-bias-facial-recognition-black-gay-transgender&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-100"><span class="mw-cite-backlink"><b><a href="#cite_ref-100">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSamuel2019" class="citation web cs1">Samuel, Sigal (April 19, 2019). <a rel="nofollow" class="external text" href="https://www.vox.com/future-perfect/2019/4/19/18412674/ai-bias-facial-recognition-black-gay-transgender">"Some AI just shouldn't exist"</a>. <i>Vox</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 12,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Vox&amp;rft.atitle=Some+AI+just+shouldn%27t+exist&amp;rft.date=2019-04-19&amp;rft.aulast=Samuel&amp;rft.aufirst=Sigal&amp;rft_id=https%3A%2F%2Fwww.vox.com%2Ffuture-perfect%2F2019%2F4%2F19%2F18412674%2Fai-bias-facial-recognition-black-gay-transgender&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-101"><span class="mw-cite-backlink"><b><a href="#cite_ref-101">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWangKosinski2017" class="citation journal cs1">Wang, Yilun; Kosinski, Michal (February 15, 2017). <a rel="nofollow" class="external text" href="https://osf.io/zn79k/">"Deep neural networks are more accurate than humans at detecting sexual orientation from facial images"</a>. <i>OSF</i>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.17605%2FOSF.IO%2FZN79K">10.17605/OSF.IO/ZN79K</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=OSF&amp;rft.atitle=Deep+neural+networks+are+more+accurate+than+humans+at+detecting+sexual+orientation+from+facial+images.&amp;rft.date=2017-02-15&amp;rft_id=info%3Adoi%2F10.17605%2FOSF.IO%2FZN79K&amp;rft.aulast=Wang&amp;rft.aufirst=Yilun&amp;rft.au=Kosinski%2C+Michal&amp;rft_id=https%3A%2F%2Fosf.io%2Fzn79k%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-102"><span class="mw-cite-backlink"><b><a href="#cite_ref-102">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLevin2017" class="citation news cs1">Levin, Sam (September 9, 2017). <a rel="nofollow" class="external text" href="https://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford">"LGBT groups denounce 'dangerous' AI that uses your face to guess sexuality"</a>. <i>The Guardian</i>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0261-3077">0261-3077</a><span class="reference-accessdate">. Retrieved <span class="nowrap">December 12,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=LGBT+groups+denounce+%27dangerous%27+AI+that+uses+your+face+to+guess+sexuality&amp;rft.date=2017-09-09&amp;rft.issn=0261-3077&amp;rft.aulast=Levin&amp;rft.aufirst=Sam&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fworld%2F2017%2Fsep%2F08%2Fai-gay-gaydar-algorithm-facial-recognition-criticism-stanford&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-103"><span class="mw-cite-backlink"><b><a href="#cite_ref-103">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPal2011" class="citation journal cs1 cs1-prop-long-vol">Pal, G.C. (September 16, 2011). <a rel="nofollow" class="external text" href="https://journals.sagepub.com/doi/abs/10.1177/097133361102300202?journalCode=pdsa">"Disability, Intersectionality and Deprivation: An Excluded Agenda"</a>. <i>Psychology and Developing Societies</i>. 23(2), 159–176. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F097133361102300202">10.1177/097133361102300202</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:147322669">147322669</a> &#8211; via Sagepub.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychology+and+Developing+Societies&amp;rft.atitle=Disability%2C+Intersectionality+and+Deprivation%3A+An+Excluded+Agenda&amp;rft.volume=23%282%29%2C+159%E2%80%93176.&amp;rft.date=2011-09-16&amp;rft_id=info%3Adoi%2F10.1177%2F097133361102300202&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A147322669%23id-name%3DS2CID&amp;rft.aulast=Pal&amp;rft.aufirst=G.C.&amp;rft_id=https%3A%2F%2Fjournals.sagepub.com%2Fdoi%2Fabs%2F10.1177%2F097133361102300202%3FjournalCode%3Dpdsa&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-104"><span class="mw-cite-backlink"><b><a href="#cite_ref-104">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBrinkmanRea-SandinLundFitzpatrick2022" class="citation journal cs1">Brinkman, Aurora H.; Rea-Sandin, Gianna; Lund, Emily M.; Fitzpatrick, Olivia M.; Gusman, Michaela S.; Boness, Cassandra L.; Scholars for Elevating Equity and Diversity (SEED) (October 20, 2022). <a rel="nofollow" class="external text" href="http://doi.apa.org/getdoi.cfm?doi=10.1037/ort0000653">"Shifting the discourse on disability: Moving to an inclusive, intersectional focus"</a>. <i>American Journal of Orthopsychiatry</i>. <b>93</b> (1): 50–62. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1037%2Fort0000653">10.1037/ort0000653</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1939-0025">1939-0025</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;9951269. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/36265035">36265035</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Journal+of+Orthopsychiatry&amp;rft.atitle=Shifting+the+discourse+on+disability%3A+Moving+to+an+inclusive%2C+intersectional+focus.&amp;rft.volume=93&amp;rft.issue=1&amp;rft.pages=50-62&amp;rft.date=2022-10-20&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9951269%23id-name%3DPMC&amp;rft.issn=1939-0025&amp;rft_id=info%3Apmid%2F36265035&amp;rft_id=info%3Adoi%2F10.1037%2Fort0000653&amp;rft.aulast=Brinkman&amp;rft.aufirst=Aurora+H.&amp;rft.au=Rea-Sandin%2C+Gianna&amp;rft.au=Lund%2C+Emily+M.&amp;rft.au=Fitzpatrick%2C+Olivia+M.&amp;rft.au=Gusman%2C+Michaela+S.&amp;rft.au=Boness%2C+Cassandra+L.&amp;rft.au=Scholars+for+Elevating+Equity+and+Diversity+%28SEED%29&amp;rft_id=http%3A%2F%2Fdoi.apa.org%2Fgetdoi.cfm%3Fdoi%3D10.1037%2Fort0000653&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-105"><span class="mw-cite-backlink"><b><a href="#cite_ref-105">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWhittaker2019" class="citation web cs1">Whittaker, Meredith (November 2019). <a rel="nofollow" class="external text" href="https://ainowinstitute.org/disabilitybiasai-2019.pdf">"Disability, Bias, and AI"</a> <span class="cs1-format">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Disability%2C+Bias%2C+and+AI&amp;rft.date=2019-11&amp;rft.aulast=Whittaker&amp;rft.aufirst=Meredith&amp;rft_id=https%3A%2F%2Fainowinstitute.org%2Fdisabilitybiasai-2019.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-106"><span class="mw-cite-backlink"><b><a href="#cite_ref-106">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://disabilityisdiversity.com/mission">"Mission — Disability is Diversity — Dear Entertainment Industry, THERE'S NO DIVERSITY, EQUITY &amp; INCLUSION WITHOUT DISABILITY"</a>. <i>Disability is Diversity</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 2,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Disability+is+Diversity&amp;rft.atitle=Mission+%E2%80%94+Disability+is+Diversity+%E2%80%94+Dear+Entertainment+Industry%2C+THERE%27S+NO+DIVERSITY%2C+EQUITY+%26+INCLUSION+WITHOUT+DISABILITY&amp;rft_id=https%3A%2F%2Fdisabilityisdiversity.com%2Fmission&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-107"><span class="mw-cite-backlink"><b><a href="#cite_ref-107">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.microsoft.com/design/inclusive/">"Microsoft Design"</a>. <i>www.microsoft.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 2,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.microsoft.com&amp;rft.atitle=Microsoft+Design&amp;rft_id=https%3A%2F%2Fwww.microsoft.com%2Fdesign%2Finclusive%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-108"><span class="mw-cite-backlink"><b><a href="#cite_ref-108">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPulrang" class="citation web cs1">Pulrang, Andrew. <a rel="nofollow" class="external text" href="https://www.forbes.com/sites/andrewpulrang/2020/01/03/4-ways-to-understand-the-diversity-of-the-disability-community/">"4 Ways To Understand The Diversity Of The Disability Community"</a>. <i>Forbes</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 2,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Forbes&amp;rft.atitle=4+Ways+To+Understand+The+Diversity+Of+The+Disability+Community&amp;rft.aulast=Pulrang&amp;rft.aufirst=Andrew&amp;rft_id=https%3A%2F%2Fwww.forbes.com%2Fsites%2Fandrewpulrang%2F2020%2F01%2F03%2F4-ways-to-understand-the-diversity-of-the-disability-community%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-109"><span class="mw-cite-backlink"><b><a href="#cite_ref-109">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWatermeyerSwartz2022" class="citation journal cs1">Watermeyer, Brian; Swartz, Leslie (October 12, 2022). <a rel="nofollow" class="external text" href="https://doi.org/10.1080/09687599.2022.2130177">"Disability and the problem of lazy intersectionality"</a>. <i>Disability &amp; Society</i>. <b>38</b> (2): 362–366. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F09687599.2022.2130177">10.1080/09687599.2022.2130177</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0968-7599">0968-7599</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:252959399">252959399</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Disability+%26+Society&amp;rft.atitle=Disability+and+the+problem+of+lazy+intersectionality&amp;rft.volume=38&amp;rft.issue=2&amp;rft.pages=362-366&amp;rft.date=2022-10-12&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A252959399%23id-name%3DS2CID&amp;rft.issn=0968-7599&amp;rft_id=info%3Adoi%2F10.1080%2F09687599.2022.2130177&amp;rft.aulast=Watermeyer&amp;rft.aufirst=Brian&amp;rft.au=Swartz%2C+Leslie&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1080%2F09687599.2022.2130177&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-110"><span class="mw-cite-backlink"><b><a href="#cite_ref-110">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://disabilitydata.ace.fordham.edu/reports/disability-data-initiative-2021-report/">"Disability Data Report 2021"</a>. <i>Disability Data Initiative</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 2,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Disability+Data+Initiative&amp;rft.atitle=Disability+Data+Report+2021&amp;rft_id=https%3A%2F%2Fdisabilitydata.ace.fordham.edu%2Freports%2Fdisability-data-initiative-2021-report%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-111"><span class="mw-cite-backlink"><b><a href="#cite_ref-111">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWhite2020" class="citation journal cs1">White, Jason J. G. (March 2, 2020). <a rel="nofollow" class="external text" href="https://doi.org/10.1145/3386296.3386299">"Fairness of AI for people with disabilities: problem analysis and interdisciplinary collaboration"</a>. <i>ACM SIGACCESS Accessibility and Computing</i> (125): 3:1. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3386296.3386299">10.1145/3386296.3386299</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1558-2337">1558-2337</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:211723415">211723415</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+SIGACCESS+Accessibility+and+Computing&amp;rft.atitle=Fairness+of+AI+for+people+with+disabilities%3A+problem+analysis+and+interdisciplinary+collaboration&amp;rft.issue=125&amp;rft.pages=3%3A1&amp;rft.date=2020-03-02&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A211723415%23id-name%3DS2CID&amp;rft.issn=1558-2337&amp;rft_id=info%3Adoi%2F10.1145%2F3386296.3386299&amp;rft.aulast=White&amp;rft.aufirst=Jason+J.+G.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1145%2F3386296.3386299&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-112"><span class="mw-cite-backlink"><b><a href="#cite_ref-112">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.psu.edu/news/information-sciences-and-technology/story/ai-language-models-show-bias-against-people-disabilities/">"AI language models show bias against people with disabilities, study finds | Penn State University"</a>. <i>www.psu.edu</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 2,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.psu.edu&amp;rft.atitle=AI+language+models+show+bias+against+people+with+disabilities%2C+study+finds+%7C+Penn+State+University&amp;rft_id=https%3A%2F%2Fwww.psu.edu%2Fnews%2Finformation-sciences-and-technology%2Fstory%2Fai-language-models-show-bias-against-people-disabilities%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-113"><span class="mw-cite-backlink"><b><a href="#cite_ref-113">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGivens2020" class="citation web cs1">Givens, Alexandra Reeve (February 6, 2020). <a rel="nofollow" class="external text" href="https://slate.com/technology/2020/02/algorithmic-bias-people-with-disabilities.html">"How Algorithmic Bias Hurts People With Disabilities"</a>. <i>Slate Magazine</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 2,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Slate+Magazine&amp;rft.atitle=How+Algorithmic+Bias+Hurts+People+With+Disabilities&amp;rft.date=2020-02-06&amp;rft.aulast=Givens&amp;rft.aufirst=Alexandra+Reeve&amp;rft_id=https%3A%2F%2Fslate.com%2Ftechnology%2F2020%2F02%2Falgorithmic-bias-people-with-disabilities.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-114"><span class="mw-cite-backlink"><b><a href="#cite_ref-114">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFMorris2020" class="citation journal cs1">Morris, Meredith Ringel (May 22, 2020). <a rel="nofollow" class="external text" href="https://doi.org/10.1145/3356727">"AI and accessibility"</a>. <i>Communications of the ACM</i>. <b>63</b> (6): 35–37. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1908.08939">1908.08939</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3356727">10.1145/3356727</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0001-0782">0001-0782</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:201645229">201645229</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=AI+and+accessibility&amp;rft.volume=63&amp;rft.issue=6&amp;rft.pages=35-37&amp;rft.date=2020-05-22&amp;rft_id=info%3Aarxiv%2F1908.08939&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A201645229%23id-name%3DS2CID&amp;rft.issn=0001-0782&amp;rft_id=info%3Adoi%2F10.1145%2F3356727&amp;rft.aulast=Morris&amp;rft.aufirst=Meredith+Ringel&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1145%2F3356727&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-115"><span class="mw-cite-backlink"><b><a href="#cite_ref-115">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFNoble,_Safiya_Umoja2018" class="citation book cs1">Noble, Safiya Umoja (February 20, 2018). <i>Algorithms of Oppression: How Search Engines Reinforce Racism</i>. New York. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781479837243" title="Special:BookSources/9781479837243"><bdi>9781479837243</bdi></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/987591529">987591529</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Algorithms+of+Oppression%3A+How+Search+Engines+Reinforce+Racism&amp;rft.place=New+York&amp;rft.date=2018-02-20&amp;rft_id=info%3Aoclcnum%2F987591529&amp;rft.isbn=9781479837243&amp;rft.au=Noble%2C+Safiya+Umoja&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span><span class="cs1-maint citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_book" title="Template:Cite book">cite book</a>}}</code>:  CS1 maint: location missing publisher (<a href="/wiki/Category:CS1_maint:_location_missing_publisher" title="Category:CS1 maint: location missing publisher">link</a>)</span></span>
</li>
<li id="cite_note-Friedler-116"><span class="mw-cite-backlink"><b><a href="#cite_ref-Friedler_116-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFFriedlerScheideggerVenkatasubramanian2016" class="citation arxiv cs1">Friedler, Sorelle A.; Scheidegger, Carlos; Venkatasubramanian, Suresh (2016). "On the (im)possibility of fairness". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1609.07236">1609.07236</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CY">cs.CY</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=On+the+%28im%29possibility+of+fairness&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1609.07236&amp;rft.aulast=Friedler&amp;rft.aufirst=Sorelle+A.&amp;rft.au=Scheidegger%2C+Carlos&amp;rft.au=Venkatasubramanian%2C+Suresh&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Hu-117"><span class="mw-cite-backlink"><b><a href="#cite_ref-Hu_117-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFHuChen2018" class="citation arxiv cs1">Hu, Lily; <a href="/wiki/Yiling_Chen" title="Yiling Chen">Chen, Yiling</a> (2018). "Welfare and Distributional Impacts of Fair Classification". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1807.01134">1807.01134</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Welfare+and+Distributional+Impacts+of+Fair+Classification&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1807.01134&amp;rft.aulast=Hu&amp;rft.aufirst=Lily&amp;rft.au=Chen%2C+Yiling&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Dwork-118"><span class="mw-cite-backlink"><b><a href="#cite_ref-Dwork_118-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFDworkHardtPitassiReingold2011" class="citation arxiv cs1">Dwork, Cynthia; Hardt, Moritz; Pitassi, Toniann; Reingold, Omer; Zemel, Rich (November 28, 2011). "Fairness Through Awareness". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1104.3913">1104.3913</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CC">cs.CC</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Fairness+Through+Awareness&amp;rft.date=2011-11-28&amp;rft_id=info%3Aarxiv%2F1104.3913&amp;rft.aulast=Dwork&amp;rft.aufirst=Cynthia&amp;rft.au=Hardt%2C+Moritz&amp;rft.au=Pitassi%2C+Toniann&amp;rft.au=Reingold%2C+Omer&amp;rft.au=Zemel%2C+Rich&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Sandvig2-119"><span class="mw-cite-backlink">^ <a href="#cite_ref-Sandvig2_119-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Sandvig2_119-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Sandvig2_119-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSandvigHamiltonKarahaliosLangbort2014" class="citation journal cs1">Sandvig, Christian; Hamilton, Kevin; Karahalios, Karrie; Langbort, Cedric (2014). Gangadharan, Seeta Pena; Eubanks, Virginia; Barocas, Solon (eds.). <a rel="nofollow" class="external text" href="http://www-personal.umich.edu/~csandvig/research/An%20Algorithm%20Audit.pdf">"An Algorithm Audit"</a> <span class="cs1-format">(PDF)</span>. <i>Data and Discrimination: Collected Essays</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Data+and+Discrimination%3A+Collected+Essays&amp;rft.atitle=An+Algorithm+Audit&amp;rft.date=2014&amp;rft.aulast=Sandvig&amp;rft.aufirst=Christian&amp;rft.au=Hamilton%2C+Kevin&amp;rft.au=Karahalios%2C+Karrie&amp;rft.au=Langbort%2C+Cedric&amp;rft_id=http%3A%2F%2Fwww-personal.umich.edu%2F~csandvig%2Fresearch%2FAn%2520Algorithm%2520Audit.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-LaFrance-120"><span class="mw-cite-backlink"><b><a href="#cite_ref-LaFrance_120-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFLaFrance2015" class="citation web cs1">LaFrance, Adrienne (September 18, 2015). <a rel="nofollow" class="external text" href="https://www.theatlantic.com/technology/archive/2015/09/not-even-the-people-who-write-algorithms-really-know-how-they-work/406099/">"The Algorithms That Power the Web Are Only Getting More Mysterious"</a>. <i>The Atlantic</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Atlantic&amp;rft.atitle=The+Algorithms+That+Power+the+Web+Are+Only+Getting+More+Mysterious&amp;rft.date=2015-09-18&amp;rft.aulast=LaFrance&amp;rft.aufirst=Adrienne&amp;rft_id=https%3A%2F%2Fwww.theatlantic.com%2Ftechnology%2Farchive%2F2015%2F09%2Fnot-even-the-people-who-write-algorithms-really-know-how-they-work%2F406099%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-121"><span class="mw-cite-backlink"><b><a href="#cite_ref-121">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBruno_Latour1999" class="citation book cs1">Bruno Latour (1999). <i>Pandora's Hope: Essays On the Reality of Science Studies</i>. <a href="/wiki/Cambridge,_Massachusetts" title="Cambridge, Massachusetts">Cambridge, Massachusetts</a>: <a href="/wiki/Harvard_University_Press" title="Harvard University Press">Harvard University Press</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Pandora%27s+Hope%3A+Essays+On+the+Reality+of+Science+Studies&amp;rft.place=Cambridge%2C+Massachusetts&amp;rft.pub=Harvard+University+Press&amp;rft.date=1999&amp;rft.au=Bruno+Latour&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-KubitschkoKaun-122"><span class="mw-cite-backlink"><b><a href="#cite_ref-KubitschkoKaun_122-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKubitschkoKaun2016" class="citation book cs1">Kubitschko, Sebastian; Kaun, Anne (2016). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=ZdzMDQAAQBAJ"><i>Innovative Methods in Media and Communication Research</i></a>. Springer. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-319-40700-5" title="Special:BookSources/978-3-319-40700-5"><bdi>978-3-319-40700-5</bdi></a><span class="reference-accessdate">. Retrieved <span class="nowrap">November 19,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Innovative+Methods+in+Media+and+Communication+Research&amp;rft.pub=Springer&amp;rft.date=2016&amp;rft.isbn=978-3-319-40700-5&amp;rft.aulast=Kubitschko&amp;rft.aufirst=Sebastian&amp;rft.au=Kaun%2C+Anne&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DZdzMDQAAQBAJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-McGee-123"><span class="mw-cite-backlink"><b><a href="#cite_ref-McGee_123-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFMcGee2013" class="citation web cs1">McGee, Matt (August 16, 2013). <a rel="nofollow" class="external text" href="https://marketingland.com/edgerank-is-dead-facebooks-news-feed-algorithm-now-has-close-to-100k-weight-factors-55908">"EdgeRank Is Dead: Facebook's News Feed Algorithm Now Has Close To 100K Weight Factors"</a>. <i>Marketing Land</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Marketing+Land&amp;rft.atitle=EdgeRank+Is+Dead%3A+Facebook%27s+News+Feed+Algorithm+Now+Has+Close+To+100K+Weight+Factors&amp;rft.date=2013-08-16&amp;rft.aulast=McGee&amp;rft.aufirst=Matt&amp;rft_id=https%3A%2F%2Fmarketingland.com%2Fedgerank-is-dead-facebooks-news-feed-algorithm-now-has-close-to-100k-weight-factors-55908&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Granka-124"><span class="mw-cite-backlink">^ <a href="#cite_ref-Granka_124-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Granka_124-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Granka_124-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFGranka2010" class="citation journal cs1">Granka, Laura A. (September 27, 2010). <a rel="nofollow" class="external text" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36914.pdf">"The Politics of Search: A Decade Retrospective"</a> <span class="cs1-format">(PDF)</span>. <i>The Information Society</i>. <b>26</b> (5): 364–374. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F01972243.2010.511560">10.1080/01972243.2010.511560</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:16306443">16306443</a><span class="reference-accessdate">. Retrieved <span class="nowrap">November 18,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Information+Society&amp;rft.atitle=The+Politics+of+Search%3A+A+Decade+Retrospective&amp;rft.volume=26&amp;rft.issue=5&amp;rft.pages=364-374&amp;rft.date=2010-09-27&amp;rft_id=info%3Adoi%2F10.1080%2F01972243.2010.511560&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A16306443%23id-name%3DS2CID&amp;rft.aulast=Granka&amp;rft.aufirst=Laura+A.&amp;rft_id=https%3A%2F%2Fstatic.googleusercontent.com%2Fmedia%2Fresearch.google.com%2Fen%2F%2Fpubs%2Farchive%2F36914.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-125"><span class="mw-cite-backlink"><b><a href="#cite_ref-125">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSzymielewicz2020" class="citation web cs1">Szymielewicz, Katarzyna (January 20, 2020). <a rel="nofollow" class="external text" href="https://medium.com/@szymielewicz/black-boxed-politics-cebc0d5a54ad">"Black-Boxed Politics"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Black-Boxed+Politics&amp;rft.date=2020-01-20&amp;rft.aulast=Szymielewicz&amp;rft.aufirst=Katarzyna&amp;rft_id=https%3A%2F%2Fmedium.com%2F%40szymielewicz%2Fblack-boxed-politics-cebc0d5a54ad&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-126"><span class="mw-cite-backlink"><b><a href="#cite_ref-126">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFVealeBinns2017" class="citation journal cs1">Veale, Michael; Binns, Reuben (2017). <a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F2053951717743530">"Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data"</a>. <i>Big Data &amp; Society</i>. <b>4</b> (2): 205395171774353. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F2053951717743530">10.1177/2053951717743530</a></span>. <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3060763">3060763</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Big+Data+%26+Society&amp;rft.atitle=Fairer+machine+learning+in+the+real+world%3A+Mitigating+discrimination+without+collecting+sensitive+data&amp;rft.volume=4&amp;rft.issue=2&amp;rft.pages=205395171774353&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D3060763%23id-name%3DSSRN&amp;rft_id=info%3Adoi%2F10.1177%2F2053951717743530&amp;rft.aulast=Veale&amp;rft.aufirst=Michael&amp;rft.au=Binns%2C+Reuben&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1177%252F2053951717743530&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-127"><span class="mw-cite-backlink"><b><a href="#cite_ref-127">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFElliottMorrisonFremontMcCaffrey2009" class="citation journal cs1">Elliott, Marc N.; Morrison, Peter A.; Fremont, Allen; McCaffrey, Daniel F.; Pantoja, Philip; Lurie, Nicole (June 2009). "Using the Census Bureau's surname list to improve estimates of race/ethnicity and associated disparities". <i>Health Services and Outcomes Research Methodology</i>. <b>9</b> (2): 69–83. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs10742-009-0047-1">10.1007/s10742-009-0047-1</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1387-3741">1387-3741</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:43293144">43293144</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Health+Services+and+Outcomes+Research+Methodology&amp;rft.atitle=Using+the+Census+Bureau%27s+surname+list+to+improve+estimates+of+race%2Fethnicity+and+associated+disparities&amp;rft.volume=9&amp;rft.issue=2&amp;rft.pages=69-83&amp;rft.date=2009-06&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A43293144%23id-name%3DS2CID&amp;rft.issn=1387-3741&amp;rft_id=info%3Adoi%2F10.1007%2Fs10742-009-0047-1&amp;rft.aulast=Elliott&amp;rft.aufirst=Marc+N.&amp;rft.au=Morrison%2C+Peter+A.&amp;rft.au=Fremont%2C+Allen&amp;rft.au=McCaffrey%2C+Daniel+F.&amp;rft.au=Pantoja%2C+Philip&amp;rft.au=Lurie%2C+Nicole&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-128"><span class="mw-cite-backlink"><b><a href="#cite_ref-128">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFChenKallusMaoSvacha2019" class="citation book cs1">Chen, Jiahao; Kallus, Nathan; Mao, Xiaojie; Svacha, Geoffry; Udell, Madeleine (2019). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?doid=3287560.3287594">"Fairness Under Unawareness"</a>. <i>Proceedings of the Conference on Fairness, Accountability, and Transparency</i>. Atlanta, GA, USA: ACM Press. pp.&#160;339–348. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1811.11154">1811.11154</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3287560.3287594">10.1145/3287560.3287594</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450361255" title="Special:BookSources/9781450361255"><bdi>9781450361255</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:58006233">58006233</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Fairness+Under+Unawareness&amp;rft.btitle=Proceedings+of+the+Conference+on+Fairness%2C+Accountability%2C+and+Transparency&amp;rft.place=Atlanta%2C+GA%2C+USA&amp;rft.pages=339-348&amp;rft.pub=ACM+Press&amp;rft.date=2019&amp;rft_id=info%3Aarxiv%2F1811.11154&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A58006233%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F3287560.3287594&amp;rft.isbn=9781450361255&amp;rft.aulast=Chen&amp;rft.aufirst=Jiahao&amp;rft.au=Kallus%2C+Nathan&amp;rft.au=Mao%2C+Xiaojie&amp;rft.au=Svacha%2C+Geoffry&amp;rft.au=Udell%2C+Madeleine&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fdoid%3D3287560.3287594&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-129"><span class="mw-cite-backlink"><b><a href="#cite_ref-129">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKilbertusGasconKusnerVeale2018" class="citation journal cs1">Kilbertus, Niki; Gascon, Adria; Kusner, Matt; Veale, Michael; Gummadi, Krishna; Weller, Adrian (2018). <a rel="nofollow" class="external text" href="http://proceedings.mlr.press/v80/kilbertus18a.html">"Blind Justice: Fairness with Encrypted Sensitive Attributes"</a>. <i>International Conference on Machine Learning</i>: 2630–2639. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1806.03281">1806.03281</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018arXiv180603281K">2018arXiv180603281K</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Machine+Learning&amp;rft.atitle=Blind+Justice%3A+Fairness+with+Encrypted+Sensitive+Attributes&amp;rft.pages=2630-2639&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1806.03281&amp;rft_id=info%3Abibcode%2F2018arXiv180603281K&amp;rft.aulast=Kilbertus&amp;rft.aufirst=Niki&amp;rft.au=Gascon%2C+Adria&amp;rft.au=Kusner%2C+Matt&amp;rft.au=Veale%2C+Michael&amp;rft.au=Gummadi%2C+Krishna&amp;rft.au=Weller%2C+Adrian&amp;rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv80%2Fkilbertus18a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-130"><span class="mw-cite-backlink"><b><a href="#cite_ref-130">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBinnsVealeKleekShadbolt2017" class="citation book cs1">Binns, Reuben; Veale, Michael; Kleek, Max Van; Shadbolt, Nigel (September 13, 2017). "Like Trainer, Like Bot? Inheritance of Bias in Algorithmic Content Moderation". <i>Social Informatics</i>. Lecture Notes in Computer Science. Vol.&#160;10540. pp.&#160;405–415. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1707.01477">1707.01477</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-319-67256-4_32">10.1007/978-3-319-67256-4_32</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-319-67255-7" title="Special:BookSources/978-3-319-67255-7"><bdi>978-3-319-67255-7</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:2814848">2814848</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Like+Trainer%2C+Like+Bot%3F+Inheritance+of+Bias+in+Algorithmic+Content+Moderation&amp;rft.btitle=Social+Informatics&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=405-415&amp;rft.date=2017-09-13&amp;rft_id=info%3Aarxiv%2F1707.01477&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A2814848%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-67256-4_32&amp;rft.isbn=978-3-319-67255-7&amp;rft.aulast=Binns&amp;rft.aufirst=Reuben&amp;rft.au=Veale%2C+Michael&amp;rft.au=Kleek%2C+Max+Van&amp;rft.au=Shadbolt%2C+Nigel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Claburn-131"><span class="mw-cite-backlink"><b><a href="#cite_ref-Claburn_131-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFClaburn2016" class="citation web cs1">Claburn, Thomas (July 18, 2016). <a rel="nofollow" class="external text" href="https://www.informationweek.com/government/big-data-analytics/eu-data-protection-law-may-end-the-unknowable-algorithm/d/d-id/1326294?">"EU Data Protection Law May End The Unknowable Algorithm – InformationWeek"</a>. <i>InformationWeek</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 25,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=InformationWeek&amp;rft.atitle=EU+Data+Protection+Law+May+End+The+Unknowable+Algorithm+%E2%80%93+InformationWeek&amp;rft.date=2016-07-18&amp;rft.aulast=Claburn&amp;rft.aufirst=Thomas&amp;rft_id=https%3A%2F%2Fwww.informationweek.com%2Fgovernment%2Fbig-data-analytics%2Feu-data-protection-law-may-end-the-unknowable-algorithm%2Fd%2Fd-id%2F1326294%3F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-:0-132"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_132-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_132-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFJobinIencaVayena2019" class="citation journal cs1">Jobin, Anna; Ienca, Marcello; <a href="/wiki/Effy_Vayena" title="Effy Vayena">Vayena, Effy</a> (September 2, 2019). "The global landscape of AI ethics guidelines". <i>Nature Machine Intelligence</i>. <b>1</b> (9): 389–399. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1906.11668">1906.11668</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fs42256-019-0088-2">10.1038/s42256-019-0088-2</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:201827642">201827642</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Machine+Intelligence&amp;rft.atitle=The+global+landscape+of+AI+ethics+guidelines&amp;rft.volume=1&amp;rft.issue=9&amp;rft.pages=389-399&amp;rft.date=2019-09-02&amp;rft_id=info%3Aarxiv%2F1906.11668&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A201827642%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1038%2Fs42256-019-0088-2&amp;rft.aulast=Jobin&amp;rft.aufirst=Anna&amp;rft.au=Ienca%2C+Marcello&amp;rft.au=Vayena%2C+Effy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-133"><span class="mw-cite-backlink"><b><a href="#cite_ref-133">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="https://research.google.com/bigpicture/attacking-discrimination-in-ml/">https://research.google.com/bigpicture/attacking-discrimination-in-ml/</a> Attacking discrimination with smarter machine learning</span>
</li>
<li id="cite_note-134"><span class="mw-cite-backlink"><b><a href="#cite_ref-134">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFHardtPriceSrebro2016" class="citation arxiv cs1">Hardt, Moritz; Price, Eric; Srebro, Nathan (2016). "Equality of Opportunity in Supervised Learning". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1610.02413">1610.02413</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Equality+of+Opportunity+in+Supervised+Learning&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1610.02413&amp;rft.aulast=Hardt&amp;rft.aufirst=Moritz&amp;rft.au=Price%2C+Eric&amp;rft.au=Srebro%2C+Nathan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-135"><span class="mw-cite-backlink"><b><a href="#cite_ref-135">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="https://venturebeat.com/2018/05/25/microsoft-is-developing-a-tool-to-help-engineers-catch-bias-in-algorithms/">https://venturebeat.com/2018/05/25/microsoft-is-developing-a-tool-to-help-engineers-catch-bias-in-algorithms/</a> Microsoft is developing a tool to help engineers catch bias in algorithms</span>
</li>
<li id="cite_note-136"><span class="mw-cite-backlink"><b><a href="#cite_ref-136">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://qz.com/1268520/facebook-says-it-has-a-tool-to-detect-bias-in-its-artificial-intelligence/">"Facebook says it has a tool to detect bias in its artificial intelligence"</a>. <i><a href="/wiki/Quartz_(publication)" title="Quartz (publication)">Quartz</a></i>. May 3, 2018. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230305194710/https://qz.com/1268520/facebook-says-it-has-a-tool-to-detect-bias-in-its-artificial-intelligence">Archived</a> from the original on March 5, 2023.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Quartz&amp;rft.atitle=Facebook+says+it+has+a+tool+to+detect+bias+in+its+artificial+intelligence&amp;rft.date=2018-05-03&amp;rft_id=https%3A%2F%2Fqz.com%2F1268520%2Ffacebook-says-it-has-a-tool-to-detect-bias-in-its-artificial-intelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-137"><span class="mw-cite-backlink"><b><a href="#cite_ref-137">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://github.com/pymetrics/audit-ai">open source</a> Pymetrics audit-ai</span>
</li>
<li id="cite_note-138"><span class="mw-cite-backlink"><b><a href="#cite_ref-138">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="https://venturebeat-com.cdn.ampproject.org/c/s/venturebeat.com/2018/05/31/pymetrics-open-sources-audit-ai-an-algorithm-bias-detection-tool/amp/">https://venturebeat-com.cdn.ampproject.org/c/s/venturebeat.com/2018/05/31/pymetrics-open-sources-audit-ai-an-algorithm-bias-detection-tool/amp/</a> Pymetrics open-sources Audit AI, an algorithm bias detection tool</span>
</li>
<li id="cite_note-139"><span class="mw-cite-backlink"><b><a href="#cite_ref-139">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="https://github.com/dssg/aequitas">https://github.com/dssg/aequitas</a> open source Aequitas: Bias and Fairness Audit Toolkit</span>
</li>
<li id="cite_note-140"><span class="mw-cite-backlink"><b><a href="#cite_ref-140">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="https://dsapp.uchicago.edu/aequitas/">https://dsapp.uchicago.edu/aequitas/</a> open-sources Audit AI, Aequitas at University of Chicago</span>
</li>
<li id="cite_note-141"><span class="mw-cite-backlink"><b><a href="#cite_ref-141">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="https://www.ibm.com/blogs/research/2018/02/mitigating-bias-ai-models/">https://www.ibm.com/blogs/research/2018/02/mitigating-bias-ai-models/</a> Mitigating Bias in AI Models</span>
</li>
<li id="cite_note-142"><span class="mw-cite-backlink"><b><a href="#cite_ref-142">^</a></b></span> <span class="reference-text">S. Sen, D. Dasgupta and K. D. Gupta, "An Empirical Study on Algorithmic Bias", 2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC), Madrid, Spain, 2020, pp. 1189-1194, <link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FCOMPSAC48688.2020.00-95">10.1109/COMPSAC48688.2020.00-95</a>.</span>
</li>
<li id="cite_note-143"><span class="mw-cite-backlink"><b><a href="#cite_ref-143">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFZouSchiebinger2018" class="citation journal cs1">Zou, James; Schiebinger, Londa (July 2018). <a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fd41586-018-05707-8">"AI can be sexist and racist — it's time to make it fair"</a>. <i>Nature</i>. <b>559</b> (7714): 324–326. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018Natur.559..324Z">2018Natur.559..324Z</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fd41586-018-05707-8">10.1038/d41586-018-05707-8</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/30018439">30018439</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=AI+can+be+sexist+and+racist+%E2%80%94+it%27s+time+to+make+it+fair&amp;rft.volume=559&amp;rft.issue=7714&amp;rft.pages=324-326&amp;rft.date=2018-07&amp;rft_id=info%3Apmid%2F30018439&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-018-05707-8&amp;rft_id=info%3Abibcode%2F2018Natur.559..324Z&amp;rft.aulast=Zou&amp;rft.aufirst=James&amp;rft.au=Schiebinger%2C+Londa&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1038%252Fd41586-018-05707-8&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-144"><span class="mw-cite-backlink"><b><a href="#cite_ref-144">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFJiaWelfareCristianini2018" class="citation conference cs1">Jia, Sen; Welfare, Thomas; Cristianini, Nello (2018). <i>Right for the right reason: Training agnostic networks</i>. International Symposium on Intelligent Data Analysis. Springer.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Right+for+the+right+reason%3A+Training+agnostic+networks.&amp;rft.pub=Springer&amp;rft.date=2018&amp;rft.aulast=Jia&amp;rft.aufirst=Sen&amp;rft.au=Welfare%2C+Thomas&amp;rft.au=Cristianini%2C+Nello&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Sutton-145"><span class="mw-cite-backlink"><b><a href="#cite_ref-Sutton_145-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSuttonWelfareCristianini2018" class="citation conference cs1">Sutton, Adam; Welfare, Thomas; Cristianini, Nello (2018). <i>Biased embeddings from wild data: Measuring, understanding and removing</i>. International Symposium on Intelligent Data Analysis. Springer.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Biased+embeddings+from+wild+data%3A+Measuring%2C+understanding+and+removing.&amp;rft.pub=Springer&amp;rft.date=2018&amp;rft.aulast=Sutton&amp;rft.aufirst=Adam&amp;rft.au=Welfare%2C+Thomas&amp;rft.au=Cristianini%2C+Nello&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-146"><span class="mw-cite-backlink"><b><a href="#cite_ref-146">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKoene2017" class="citation journal cs1">Koene, Ansgar (June 2017). <a rel="nofollow" class="external text" href="http://eprints.nottingham.ac.uk/44207/8/IEEE_Tech_Sociery_Magazine_AlgoBias_2017_AKoene.pdf">"Algorithmic Bias: Addressing Growing Concerns &#91;Leading Edge&#93;"</a> <span class="cs1-format">(PDF)</span>. <i>IEEE Technology and Society Magazine</i>. <b>36</b> (2): 31–32. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2Fmts.2017.2697080">10.1109/mts.2017.2697080</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0278-0097">0278-0097</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Technology+and+Society+Magazine&amp;rft.atitle=Algorithmic+Bias%3A+Addressing+Growing+Concerns+%5BLeading+Edge%5D&amp;rft.volume=36&amp;rft.issue=2&amp;rft.pages=31-32&amp;rft.date=2017-06&amp;rft_id=info%3Adoi%2F10.1109%2Fmts.2017.2697080&amp;rft.issn=0278-0097&amp;rft.aulast=Koene&amp;rft.aufirst=Ansgar&amp;rft_id=http%3A%2F%2Feprints.nottingham.ac.uk%2F44207%2F8%2FIEEE_Tech_Sociery_Magazine_AlgoBias_2017_AKoene.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-147"><span class="mw-cite-backlink"><b><a href="#cite_ref-147">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://standards.ieee.org/project/7003.html">"P7003 - Algorithmic Bias Considerations"</a>. <i>standards.ieee.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">December 3,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=standards.ieee.org&amp;rft.atitle=P7003+-+Algorithmic+Bias+Considerations&amp;rft_id=https%3A%2F%2Fstandards.ieee.org%2Fproject%2F7003.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-148"><span class="mw-cite-backlink"><b><a href="#cite_ref-148">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFThe_Internet_Society2017" class="citation web cs1">The Internet Society (April 18, 2017). <a rel="nofollow" class="external text" href="https://www.internetsociety.org/resources/doc/2017/artificial-intelligence-and-machine-learning-policy-paper/">"Artificial Intelligence and Machine Learning: Policy Paper"</a>. <i>Internet Society</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Internet+Society&amp;rft.atitle=Artificial+Intelligence+and+Machine+Learning%3A+Policy+Paper&amp;rft.date=2017-04-18&amp;rft.au=The+Internet+Society&amp;rft_id=https%3A%2F%2Fwww.internetsociety.org%2Fresources%2Fdoc%2F2017%2Fartificial-intelligence-and-machine-learning-policy-paper%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-:2-149"><span class="mw-cite-backlink">^ <a href="#cite_ref-:2_149-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_149-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.weforum.org/whitepapers/how-to-prevent-discriminatory-outcomes-in-machine-learning">"White Paper: How to Prevent Discriminatory Outcomes in Machine Learning"</a>. <i>World Economic Forum</i>. March 12, 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=World+Economic+Forum&amp;rft.atitle=White+Paper%3A+How+to+Prevent+Discriminatory+Outcomes+in+Machine+Learning&amp;rft.date=2018-03-12&amp;rft_id=https%3A%2F%2Fwww.weforum.org%2Fwhitepapers%2Fhow-to-prevent-discriminatory-outcomes-in-machine-learning&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-150"><span class="mw-cite-backlink"><b><a href="#cite_ref-150">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.darpa.mil/program/explainable-artificial-intelligence">"Explainable Artificial Intelligence"</a>. <i>www.darpa.mil</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.darpa.mil&amp;rft.atitle=Explainable+Artificial+Intelligence&amp;rft_id=https%3A%2F%2Fwww.darpa.mil%2Fprogram%2Fexplainable-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-151"><span class="mw-cite-backlink"><b><a href="#cite_ref-151">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPricewaterhouseCoopers" class="citation web cs1">PricewaterhouseCoopers. <a rel="nofollow" class="external text" href="https://www.pwc.co.uk/services/risk-assurance/insights/accelerating-innovation-through-responsible-ai/responsible-ai-framework.html">"The responsible AI framework"</a>. <i>PwC</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=PwC&amp;rft.atitle=The+responsible+AI+framework&amp;rft.au=PricewaterhouseCoopers&amp;rft_id=https%3A%2F%2Fwww.pwc.co.uk%2Fservices%2Frisk-assurance%2Finsights%2Faccelerating-innovation-through-responsible-ai%2Fresponsible-ai-framework.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-152"><span class="mw-cite-backlink"><b><a href="#cite_ref-152">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFHeald2006" class="citation book cs1">Heald, David (September 7, 2006). <i>Transparency: The Key to Better Governance?</i>. British Academy. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.5871%2Fbacad%2F9780197263839.003.0002">10.5871/bacad/9780197263839.003.0002</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-19-726383-9" title="Special:BookSources/978-0-19-726383-9"><bdi>978-0-19-726383-9</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Transparency%3A+The+Key+to+Better+Governance%3F&amp;rft.pub=British+Academy&amp;rft.date=2006-09-07&amp;rft_id=info%3Adoi%2F10.5871%2Fbacad%2F9780197263839.003.0002&amp;rft.isbn=978-0-19-726383-9&amp;rft.aulast=Heald&amp;rft.aufirst=David&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-153"><span class="mw-cite-backlink"><b><a href="#cite_ref-153">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKemperKolkman2019" class="citation journal cs1">Kemper, Jakko; Kolkman, Daan (December 6, 2019). <a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F1369118X.2018.1477967">"Transparent to whom? No algorithmic accountability without a critical audience"</a>. <i>Information, Communication &amp; Society</i>. <b>22</b> (14): 2081–2096. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F1369118X.2018.1477967">10.1080/1369118X.2018.1477967</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1369-118X">1369-118X</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information%2C+Communication+%26+Society&amp;rft.atitle=Transparent+to+whom%3F+No+algorithmic+accountability+without+a+critical+audience&amp;rft.volume=22&amp;rft.issue=14&amp;rft.pages=2081-2096&amp;rft.date=2019-12-06&amp;rft_id=info%3Adoi%2F10.1080%2F1369118X.2018.1477967&amp;rft.issn=1369-118X&amp;rft.aulast=Kemper&amp;rft.aufirst=Jakko&amp;rft.au=Kolkman%2C+Daan&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1080%252F1369118X.2018.1477967&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-154"><span class="mw-cite-backlink"><b><a href="#cite_ref-154">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.hrw.org/news/2018/07/03/toronto-declaration-protecting-rights-equality-and-non-discrimination-machine">"The Toronto Declaration: Protecting the rights to equality and non-discrimination in machine learning systems"</a>. <i>Human Rights Watch</i>. July 3, 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Human+Rights+Watch&amp;rft.atitle=The+Toronto+Declaration%3A+Protecting+the+rights+to+equality+and+non-discrimination+in+machine+learning+systems&amp;rft.date=2018-07-03&amp;rft_id=https%3A%2F%2Fwww.hrw.org%2Fnews%2F2018%2F07%2F03%2Ftoronto-declaration-protecting-rights-equality-and-non-discrimination-machine&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-155"><span class="mw-cite-backlink"><b><a href="#cite_ref-155">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation book cs1"><a rel="nofollow" class="external text" href="https://www.accessnow.org/cms/assets/uploads/2018/08/The-Toronto-Declaration_ENG_08-2018.pdf"><i>The Toronto Declaration: Protecting the Right to Equality and Non-Discrimination in Machine Learning Systems</i></a> <span class="cs1-format">(PDF)</span>. Human Rights Watch. 2018. p.&#160;15.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Toronto+Declaration%3A+Protecting+the+Right+to+Equality+and+Non-Discrimination+in+Machine+Learning+Systems&amp;rft.pages=15&amp;rft.pub=Human+Rights+Watch&amp;rft.date=2018&amp;rft_id=https%3A%2F%2Fwww.accessnow.org%2Fcms%2Fassets%2Fuploads%2F2018%2F08%2FThe-Toronto-Declaration_ENG_08-2018.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-156"><span class="mw-cite-backlink"><b><a href="#cite_ref-156">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFFloridiCowlsBeltramettiChatila2018" class="citation journal cs1">Floridi, Luciano; Cowls, Josh; Beltrametti, Monica; Chatila, Raja; Chazerand, Patrice; Dignum, Virginia; Luetge, Christoph; Madelin, Robert; Pagallo, Ugo; Rossi, Francesca; Schafer, Burkhard (December 1, 2018). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6404626">"AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations"</a>. <i>Minds and Machines</i>. <b>28</b> (4): 703. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs11023-018-9482-5">10.1007/s11023-018-9482-5</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1572-8641">1572-8641</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6404626">6404626</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/30930541">30930541</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Minds+and+Machines&amp;rft.atitle=AI4People%E2%80%94An+Ethical+Framework+for+a+Good+AI+Society%3A+Opportunities%2C+Risks%2C+Principles%2C+and+Recommendations&amp;rft.volume=28&amp;rft.issue=4&amp;rft.pages=703&amp;rft.date=2018-12-01&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6404626%23id-name%3DPMC&amp;rft.issn=1572-8641&amp;rft_id=info%3Apmid%2F30930541&amp;rft_id=info%3Adoi%2F10.1007%2Fs11023-018-9482-5&amp;rft.aulast=Floridi&amp;rft.aufirst=Luciano&amp;rft.au=Cowls%2C+Josh&amp;rft.au=Beltrametti%2C+Monica&amp;rft.au=Chatila%2C+Raja&amp;rft.au=Chazerand%2C+Patrice&amp;rft.au=Dignum%2C+Virginia&amp;rft.au=Luetge%2C+Christoph&amp;rft.au=Madelin%2C+Robert&amp;rft.au=Pagallo%2C+Ugo&amp;rft.au=Rossi%2C+Francesca&amp;rft.au=Schafer%2C+Burkhard&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6404626&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-157"><span class="mw-cite-backlink"><b><a href="#cite_ref-157">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFCrawford2016" class="citation news cs1">Crawford, Kate (June 25, 2016). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html">"Opinion | Artificial Intelligence's White Guy Problem"</a>. <i><a href="/wiki/The_New_York_Times" title="The New York Times">The New York Times</a></i>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0362-4331">0362-4331</a><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Opinion+%7C+Artificial+Intelligence%27s+White+Guy+Problem&amp;rft.date=2016-06-25&amp;rft.issn=0362-4331&amp;rft.aulast=Crawford&amp;rft.aufirst=Kate&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2016%2F06%2F26%2Fopinion%2Fsunday%2Fartificial-intelligences-white-guy-problem.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-158"><span class="mw-cite-backlink"><b><a href="#cite_ref-158">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation news cs1"><a rel="nofollow" class="external text" href="https://www.wired.com/story/artificial-intelligence-researchers-gender-imbalance/">"AI Is the Future—But Where Are the Women?"</a>. <i>Wired</i>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1059-1028">1059-1028</a><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=AI+Is+the+Future%E2%80%94But+Where+Are+the+Women%3F&amp;rft.issn=1059-1028&amp;rft_id=https%3A%2F%2Fwww.wired.com%2Fstory%2Fartificial-intelligence-researchers-gender-imbalance%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-159"><span class="mw-cite-backlink"><b><a href="#cite_ref-159">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSnow" class="citation web cs1">Snow, Jackie. <a rel="nofollow" class="external text" href="https://www.technologyreview.com/s/610192/were-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our/">"<span class="cs1-kern-left"></span>"We're in a diversity crisis": cofounder of Black in AI on what's poisoning algorithms in our lives"</a>. <i>MIT Technology Review</i><span class="reference-accessdate">. Retrieved <span class="nowrap">February 11,</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=%22We%27re+in+a+diversity+crisis%22%3A+cofounder+of+Black+in+AI+on+what%27s+poisoning+algorithms+in+our+lives&amp;rft.aulast=Snow&amp;rft.aufirst=Jackie&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2Fs%2F610192%2Fwere-in-a-diversity-crisis-black-in-ais-founder-on-whats-poisoning-the-algorithms-in-our%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-160"><span class="mw-cite-backlink"><b><a href="#cite_ref-160">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFHao2021" class="citation news cs1">Hao, Karen (June 14, 2021). <a rel="nofollow" class="external text" href="https://www.technologyreview.com/2021/06/14/1026148/ai-big-tech-timnit-gebru-paper-ethics/">"Inside the fight to reclaim AI from Big Tech's control"</a>. <i>MIT Technology Review</i><span class="reference-accessdate">. Retrieved <span class="nowrap">June 21,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=Inside+the+fight+to+reclaim+AI+from+Big+Tech%27s+control&amp;rft.date=2021-06-14&amp;rft.aulast=Hao&amp;rft.aufirst=Karen&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2F2021%2F06%2F14%2F1026148%2Fai-big-tech-timnit-gebru-paper-ethics%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-161"><span class="mw-cite-backlink"><b><a href="#cite_ref-161">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFCiston2019" class="citation journal cs1">Ciston, Sarah (December 29, 2019). <a rel="nofollow" class="external text" href="https://doi.org/10.7559%2Fcitarj.v11i2.665">"Intersectional AI Is Essential"</a>. <i>Journal of Science and Technology of the Arts</i>. <b>11</b> (2): 3–8. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.7559%2Fcitarj.v11i2.665">10.7559/citarj.v11i2.665</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2183-0088">2183-0088</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Science+and+Technology+of+the+Arts&amp;rft.atitle=Intersectional+AI+Is+Essential&amp;rft.volume=11&amp;rft.issue=2&amp;rft.pages=3-8&amp;rft.date=2019-12-29&amp;rft_id=info%3Adoi%2F10.7559%2Fcitarj.v11i2.665&amp;rft.issn=2183-0088&amp;rft.aulast=Ciston&amp;rft.aufirst=Sarah&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.7559%252Fcitarj.v11i2.665&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-DataFeminism-162"><span class="mw-cite-backlink"><b><a href="#cite_ref-DataFeminism_162-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFD&#39;IgnazioKlein2020" class="citation book cs1">D'Ignazio, Catherine; Klein, Lauren F. (2020). <i>Data Feminism</i>. MIT Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0262044004" title="Special:BookSources/978-0262044004"><bdi>978-0262044004</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Data+Feminism&amp;rft.pub=MIT+Press&amp;rft.date=2020&amp;rft.isbn=978-0262044004&amp;rft.aulast=D%27Ignazio&amp;rft.aufirst=Catherine&amp;rft.au=Klein%2C+Lauren+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-163"><span class="mw-cite-backlink"><b><a href="#cite_ref-163">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFCaveDihal2020" class="citation journal cs1">Cave, Stephen; Dihal, Kanta (August 6, 2020). <a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs13347-020-00415-6">"The Whiteness of AI"</a>. <i>Philosophy &amp; Technology</i>. <b>33</b> (4): 685–703. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs13347-020-00415-6">10.1007/s13347-020-00415-6</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2210-5441">2210-5441</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophy+%26+Technology&amp;rft.atitle=The+Whiteness+of+AI&amp;rft.volume=33&amp;rft.issue=4&amp;rft.pages=685-703&amp;rft.date=2020-08-06&amp;rft_id=info%3Adoi%2F10.1007%2Fs13347-020-00415-6&amp;rft.issn=2210-5441&amp;rft.aulast=Cave&amp;rft.aufirst=Stephen&amp;rft.au=Dihal%2C+Kanta&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%252Fs13347-020-00415-6&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-164"><span class="mw-cite-backlink"><b><a href="#cite_ref-164">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBondiXuAcosta-NavasKillian2021" class="citation book cs1">Bondi, Elizabeth; Xu, Lily; Acosta-Navas, Diana; Killian, Jackson A. (2021). "Envisioning Communities: A Participatory Approach Towards AI for Social Good". <a rel="nofollow" class="external text" href="https://ezpa.library.ualberta.ca/ezpAuthen.cgi?url=https://dl.acm.org/doi/abs/10.1145/3461702.3462612"><i>Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</i></a>. pp.&#160;425–436. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2105.01774">2105.01774</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3461702.3462612">10.1145/3461702.3462612</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450384735" title="Special:BookSources/9781450384735"><bdi>9781450384735</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:233740121">233740121</a><span class="reference-accessdate">. Retrieved <span class="nowrap">April 6,</span> 2023</span> &#8211; via ezpa.library.ualberta.ca.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Envisioning+Communities%3A+A+Participatory+Approach+Towards+AI+for+Social+Good&amp;rft.btitle=Proceedings+of+the+2021+AAAI%2FACM+Conference+on+AI%2C+Ethics%2C+and+Society&amp;rft.pages=425-436&amp;rft.date=2021&amp;rft_id=info%3Aarxiv%2F2105.01774&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233740121%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F3461702.3462612&amp;rft.isbn=9781450384735&amp;rft.aulast=Bondi&amp;rft.aufirst=Elizabeth&amp;rft.au=Xu%2C+Lily&amp;rft.au=Acosta-Navas%2C+Diana&amp;rft.au=Killian%2C+Jackson+A.&amp;rft_id=https%3A%2F%2Fezpa.library.ualberta.ca%2FezpAuthen.cgi%3Furl%3Dhttps%3A%2F%2Fdl.acm.org%2Fdoi%2Fabs%2F10.1145%2F3461702.3462612&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-165"><span class="mw-cite-backlink"><b><a href="#cite_ref-165">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFUniversity2019" class="citation web cs1">University, Stanford (March 18, 2019). <a rel="nofollow" class="external text" href="https://news.stanford.edu/2019/03/18/stanford_university_launches_human-centered_ai/">"Stanford University launches the Institute for Human-Centered Artificial Intelligence"</a>. <i>Stanford News</i><span class="reference-accessdate">. Retrieved <span class="nowrap">April 6,</span> 2023</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stanford+News&amp;rft.atitle=Stanford+University+launches+the+Institute+for+Human-Centered+Artificial+Intelligence&amp;rft.date=2019-03-18&amp;rft.aulast=University&amp;rft.aufirst=Stanford&amp;rft_id=https%3A%2F%2Fnews.stanford.edu%2F2019%2F03%2F18%2Fstanford_university_launches_human-centered_ai%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-166"><span class="mw-cite-backlink"><b><a href="#cite_ref-166">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBondiXuAcosta-NavasKillian2021" class="citation book cs1">Bondi, Elizabeth; Xu, Lily; Acosta-Navas, Diana; Killian, Jackson A. (July 21, 2021). "Envisioning Communities: A Participatory Approach Towards AI for Social Good". <i>Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</i>. pp.&#160;425–436. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2105.01774">2105.01774</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3461702.3462612">10.1145/3461702.3462612</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450384735" title="Special:BookSources/9781450384735"><bdi>9781450384735</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:233740121">233740121</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Envisioning+Communities%3A+A+Participatory+Approach+Towards+AI+for+Social+Good&amp;rft.btitle=Proceedings+of+the+2021+AAAI%2FACM+Conference+on+AI%2C+Ethics%2C+and+Society&amp;rft.pages=425-436&amp;rft.date=2021-07-21&amp;rft_id=info%3Aarxiv%2F2105.01774&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233740121%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F3461702.3462612&amp;rft.isbn=9781450384735&amp;rft.aulast=Bondi&amp;rft.aufirst=Elizabeth&amp;rft.au=Xu%2C+Lily&amp;rft.au=Acosta-Navas%2C+Diana&amp;rft.au=Killian%2C+Jackson+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-167"><span class="mw-cite-backlink"><b><a href="#cite_ref-167">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBygrave2001" class="citation journal cs1">Bygrave, Lee A (2001). "Automated Profiling". <i>Computer Law &amp; Security Review</i>. <b>17</b> (1): 17–24. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fs0267-3649%2801%2900104-2">10.1016/s0267-3649(01)00104-2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computer+Law+%26+Security+Review&amp;rft.atitle=Automated+Profiling&amp;rft.volume=17&amp;rft.issue=1&amp;rft.pages=17-24&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1016%2Fs0267-3649%2801%2900104-2&amp;rft.aulast=Bygrave&amp;rft.aufirst=Lee+A&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-:1-168"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_168-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_168-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFVealeEdwards2018" class="citation journal cs1">Veale, Michael; Edwards, Lilian (2018). <a rel="nofollow" class="external text" href="http://discovery.ucl.ac.uk/10046182/1/Veale%201-s2.0-S026736491730376X-main%281%29.pdf">"Clarity, Surprises, and Further Questions in the Article 29 Working Party Draft Guidance on Automated Decision-Making and Profiling"</a> <span class="cs1-format">(PDF)</span>. <i>Computer Law &amp; Security Review</i>. <b>34</b> (2): 398–404. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.clsr.2017.12.002">10.1016/j.clsr.2017.12.002</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:4797884">4797884</a>. <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3071679">3071679</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computer+Law+%26+Security+Review&amp;rft.atitle=Clarity%2C+Surprises%2C+and+Further+Questions+in+the+Article+29+Working+Party+Draft+Guidance+on+Automated+Decision-Making+and+Profiling&amp;rft.volume=34&amp;rft.issue=2&amp;rft.pages=398-404&amp;rft.date=2018&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A4797884%23id-name%3DS2CID&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D3071679%23id-name%3DSSRN&amp;rft_id=info%3Adoi%2F10.1016%2Fj.clsr.2017.12.002&amp;rft.aulast=Veale&amp;rft.aufirst=Michael&amp;rft.au=Edwards%2C+Lilian&amp;rft_id=http%3A%2F%2Fdiscovery.ucl.ac.uk%2F10046182%2F1%2FVeale%25201-s2.0-S026736491730376X-main%25281%2529.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-169"><span class="mw-cite-backlink"><b><a href="#cite_ref-169">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFWachterMittelstadtFloridi2017" class="citation journal cs1">Wachter, Sandra; Mittelstadt, Brent; Floridi, Luciano (May 1, 2017). <a rel="nofollow" class="external text" href="https://doi.org/10.1093%2Fidpl%2Fipx005">"Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation"</a>. <i>International Data Privacy Law</i>. <b>7</b> (2): 76–99. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1093%2Fidpl%2Fipx005">10.1093/idpl/ipx005</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2044-3994">2044-3994</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Data+Privacy+Law&amp;rft.atitle=Why+a+Right+to+Explanation+of+Automated+Decision-Making+Does+Not+Exist+in+the+General+Data+Protection+Regulation&amp;rft.volume=7&amp;rft.issue=2&amp;rft.pages=76-99&amp;rft.date=2017-05-01&amp;rft_id=info%3Adoi%2F10.1093%2Fidpl%2Fipx005&amp;rft.issn=2044-3994&amp;rft.aulast=Wachter&amp;rft.aufirst=Sandra&amp;rft.au=Mittelstadt%2C+Brent&amp;rft.au=Floridi%2C+Luciano&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1093%252Fidpl%252Fipx005&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Edwards-170"><span class="mw-cite-backlink"><b><a href="#cite_ref-Edwards_170-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFEdwardsVeale2017" class="citation journal cs1">Edwards, Lilian; Veale, Michael (May 23, 2017). "Slave to the Algorithm? Why a Right to an Explanation Is Probably Not the Remedy You Are Looking For". <i>Duke Law &amp; Technology Review</i>. <b>16</b>: 18–84. <a href="/wiki/SSRN_(identifier)" class="mw-redirect" title="SSRN (identifier)">SSRN</a>&#160;<a rel="nofollow" class="external text" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2972855">2972855</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Duke+Law+%26+Technology+Review&amp;rft.atitle=Slave+to+the+Algorithm%3F+Why+a+Right+to+an+Explanation+Is+Probably+Not+the+Remedy+You+Are+Looking+For&amp;rft.volume=16&amp;rft.pages=18-84&amp;rft.date=2017-05-23&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D2972855%23id-name%3DSSRN&amp;rft.aulast=Edwards&amp;rft.aufirst=Lilian&amp;rft.au=Veale%2C+Michael&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Singer-171"><span class="mw-cite-backlink">^ <a href="#cite_ref-Singer_171-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Singer_171-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFSinger2013" class="citation news cs1">Singer, Natasha (February 2, 2013). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2013/02/03/technology/consumer-data-protection-laws-an-ocean-apart.html">"Consumer Data Protection Laws, an Ocean Apart"</a>. <i>The New York Times</i><span class="reference-accessdate">. Retrieved <span class="nowrap">November 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Consumer+Data+Protection+Laws%2C+an+Ocean+Apart&amp;rft.date=2013-02-02&amp;rft.aulast=Singer&amp;rft.aufirst=Natasha&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2013%2F02%2F03%2Ftechnology%2Fconsumer-data-protection-laws-an-ocean-apart.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-ObamaAdmin-172"><span class="mw-cite-backlink"><b><a href="#cite_ref-ObamaAdmin_172-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFObama2016" class="citation web cs1">Obama, Barack (October 12, 2016). <a rel="nofollow" class="external text" href="https://obamawhitehouse.archives.gov/blog/2016/10/12/administrations-report-future-artificial-intelligence">"The Administration's Report on the Future of Artificial Intelligence"</a>. <i>whitehouse.gov</i>. National Archives<span class="reference-accessdate">. Retrieved <span class="nowrap">November 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=whitehouse.gov&amp;rft.atitle=The+Administration%27s+Report+on+the+Future+of+Artificial+Intelligence&amp;rft.date=2016-10-12&amp;rft.aulast=Obama&amp;rft.aufirst=Barack&amp;rft_id=https%3A%2F%2Fobamawhitehouse.archives.gov%2Fblog%2F2016%2F10%2F12%2Fadministrations-report-future-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-NSTC-173"><span class="mw-cite-backlink"><b><a href="#cite_ref-NSTC_173-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFand_Technology_Council2016" class="citation book cs1">and Technology Council, National Science (2016). <a rel="nofollow" class="external text" href="https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/national_ai_rd_strategic_plan.pdf"><i>National Artificial Intelligence Research and Development Strategic Plan</i></a> <span class="cs1-format">(PDF)</span>. US Government<span class="reference-accessdate">. Retrieved <span class="nowrap">November 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=National+Artificial+Intelligence+Research+and+Development+Strategic+Plan&amp;rft.pub=US+Government&amp;rft.date=2016&amp;rft.aulast=and+Technology+Council&amp;rft.aufirst=National+Science&amp;rft_id=https%3A%2F%2Fobamawhitehouse.archives.gov%2Fsites%2Fdefault%2Ffiles%2Fwhitehouse_files%2Fmicrosites%2Fostp%2FNSTC%2Fnational_ai_rd_strategic_plan.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Kirchner-174"><span class="mw-cite-backlink"><b><a href="#cite_ref-Kirchner_174-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFKirchner2017" class="citation web cs1">Kirchner, Lauren (December 18, 2017). <a rel="nofollow" class="external text" href="https://www.propublica.org/article/new-york-city-moves-to-create-accountability-for-algorithms">"New York City Moves to Create Accountability for Algorithms — ProPublica"</a>. <i>ProPublica</i><span class="reference-accessdate">. Retrieved <span class="nowrap">July 28,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ProPublica&amp;rft.atitle=New+York+City+Moves+to+Create+Accountability+for+Algorithms+%E2%80%94+ProPublica&amp;rft.date=2017-12-18&amp;rft.aulast=Kirchner&amp;rft.aufirst=Lauren&amp;rft_id=https%3A%2F%2Fwww.propublica.org%2Farticle%2Fnew-york-city-moves-to-create-accountability-for-algorithms&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-NYC-175"><span class="mw-cite-backlink"><b><a href="#cite_ref-NYC_175-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="http://legistar.council.nyc.gov/LegislationDetail.aspx?ID=3137815&amp;GUID=437A6A6D-62E1-47E2-9C42-461253F9C6D0">"The New York City Council - File #: Int 1696-2017"</a>. <i>legistar.council.nyc.gov</i>. New York City Council<span class="reference-accessdate">. Retrieved <span class="nowrap">July 28,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=legistar.council.nyc.gov&amp;rft.atitle=The+New+York+City+Council+-+File+%23%3A+Int+1696-2017&amp;rft_id=http%3A%2F%2Flegistar.council.nyc.gov%2FLegislationDetail.aspx%3FID%3D3137815%26GUID%3D437A6A6D-62E1-47E2-9C42-461253F9C6D0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-Powles-176"><span class="mw-cite-backlink"><b><a href="#cite_ref-Powles_176-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFPowles" class="citation magazine cs1">Powles, Julia. <a rel="nofollow" class="external text" href="https://www.newyorker.com/tech/elements/new-york-citys-bold-flawed-attempt-to-make-algorithms-accountable">"New York City's Bold, Flawed Attempt to Make Algorithms Accountable"</a>. <i>The New Yorker</i><span class="reference-accessdate">. Retrieved <span class="nowrap">July 28,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Yorker&amp;rft.atitle=New+York+City%27s+Bold%2C+Flawed+Attempt+to+Make+Algorithms+Accountable&amp;rft.aulast=Powles&amp;rft.aufirst=Julia&amp;rft_id=https%3A%2F%2Fwww.newyorker.com%2Ftech%2Felements%2Fnew-york-citys-bold-flawed-attempt-to-make-algorithms-accountable&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-177"><span class="mw-cite-backlink"><b><a href="#cite_ref-177">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.insurancejournal.com/news/international/2018/07/31/496489.htm">"India Weighs Comprehensive Data Privacy Bill, Similar to EU's GDPR"</a>. <i>Insurance Journal</i>. July 31, 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">February 26,</span> 2019</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Insurance+Journal&amp;rft.atitle=India+Weighs+Comprehensive+Data+Privacy+Bill%2C+Similar+to+EU%27s+GDPR&amp;rft.date=2018-07-31&amp;rft_id=https%3A%2F%2Fwww.insurancejournal.com%2Fnews%2Finternational%2F2018%2F07%2F31%2F496489.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
<li id="cite_note-178"><span class="mw-cite-backlink"><b><a href="#cite_ref-178">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://meity.gov.in/writereaddata/files/Personal_Data_Protection_Bill,2018.pdf">"The Personal Data Protection Bill, 2018"</a> <span class="cs1-format">(PDF)</span>. Ministry of Electronics &amp; Information Technology, Government of India. 2018<span class="reference-accessdate">. Retrieved <span class="nowrap">April 29,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Personal+Data+Protection+Bill%2C+2018&amp;rft.pub=Ministry+of+Electronics+%26+Information+Technology%2C+Government+of+India&amp;rft.date=2018&amp;rft_id=https%3A%2F%2Fmeity.gov.in%2Fwritereaddata%2Ffiles%2FPersonal_Data_Protection_Bill%2C2018.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Algorithmic_bias&amp;action=edit&amp;section=41" title="Edit section: Further reading"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFBaer2019" class="citation book cs1">Baer, Tobias (2019). <a rel="nofollow" class="external text" href="https://www.springer.com/us/book/9781484248843"><i>Understand, Manage, and Prevent Algorithmic Bias: A Guide for Business Users and Data Scientists</i></a>. New York: Apress. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781484248843" title="Special:BookSources/9781484248843"><bdi>9781484248843</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Understand%2C+Manage%2C+and+Prevent+Algorithmic+Bias%3A+A+Guide+for+Business+Users+and+Data+Scientists&amp;rft.place=New+York&amp;rft.pub=Apress&amp;rft.date=2019&amp;rft.isbn=9781484248843&amp;rft.aulast=Baer&amp;rft.aufirst=Tobias&amp;rft_id=https%3A%2F%2Fwww.springer.com%2Fus%2Fbook%2F9781484248843&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></li>
<li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1133582631"><cite id="CITEREFNoble2018" class="citation book cs1">Noble, Safiya Umoja (2018). <a href="/wiki/Algorithms_of_Oppression" title="Algorithms of Oppression"><i>Algorithms of Oppression: How Search Engines Reinforce Racism</i></a>. New York: New York University Press. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781479837243" title="Special:BookSources/9781479837243"><bdi>9781479837243</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Algorithms+of+Oppression%3A+How+Search+Engines+Reinforce+Racism&amp;rft.place=New+York&amp;rft.pub=New+York+University+Press&amp;rft.date=2018&amp;rft.isbn=9781479837243&amp;rft.aulast=Noble&amp;rft.aufirst=Safiya+Umoja&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAlgorithmic+bias" class="Z3988"></span></li></ul>
<!-- 
NewPP limit report
Parsed by mw1452
Cached time: 20231031015422
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 1.785 seconds
Real time usage: 1.903 seconds
Preprocessor visited node count: 31556/1000000
Post‐expand include size: 422663/2097152 bytes
Template argument size: 7954/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 541133/5000000 bytes
Lua time usage: 1.096/10.000 seconds
Lua memory usage: 9040934/52428800 bytes
Lua Profile:
    recursiveClone <mwInit.lua:41>                                   200 ms       16.7%
    ?                                                                180 ms       15.0%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::gsub      120 ms       10.0%
    citation0 <Module:Citation/CS1:2536>                              60 ms        5.0%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::callParserFunction       60 ms        5.0%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::getExpandedArgument       60 ms        5.0%
    <mw.lua:694>                                                      60 ms        5.0%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::match       60 ms        5.0%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::getAllExpandedArguments       40 ms        3.3%
    <mwInit.lua:41>                                                   40 ms        3.3%
    [others]                                                         320 ms       26.7%
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 1735.977      1 -total
 55.28%  959.577      1 Template:Reflist
 26.18%  454.551     91 Template:Rp
 25.24%  438.079     91 Template:R/superscript
 17.14%  297.567     58 Template:Cite_journal
 16.96%  294.354     60 Template:Cite_web
 12.80%  222.226    273 Template:R/where
  5.72%   99.326     21 Template:Cite_book
  4.73%   82.051     18 Template:Cite_news
  4.04%   70.136      1 Template:Artificial_intelligence
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:55817338-0!canonical and timestamp 20231031015421 and revision id 1182647182. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> --><noscript><img src="https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Algorithmic_bias&amp;oldid=1182647182">https://en.wikipedia.org/w/index.php?title=Algorithmic_bias&amp;oldid=1182647182</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li><li><a href="/wiki/Category:Information_ethics" title="Category:Information ethics">Information ethics</a></li><li><a href="/wiki/Category:Computing_and_society" title="Category:Computing and society">Computing and society</a></li><li><a href="/wiki/Category:Philosophy_of_artificial_intelligence" title="Category:Philosophy of artificial intelligence">Philosophy of artificial intelligence</a></li><li><a href="/wiki/Category:Discrimination" title="Category:Discrimination">Discrimination</a></li><li><a href="/wiki/Category:Bias" title="Category:Bias">Bias</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1:_long_volume_value" title="Category:CS1: long volume value">CS1: long volume value</a></li><li><a href="/wiki/Category:CS1_maint:_location_missing_publisher" title="Category:CS1 maint: location missing publisher">CS1 maint: location missing publisher</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">Short description is different from Wikidata</a></li><li><a href="/wiki/Category:Good_articles" title="Category:Good articles">Good articles</a></li><li><a href="/wiki/Category:Use_mdy_dates_from_October_2023" title="Category:Use mdy dates from October 2023">Use mdy dates from October 2023</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_September_2021" title="Category:Wikipedia articles needing clarification from September 2021">Wikipedia articles needing clarification from September 2021</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 30 October 2023, at 14:55<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License">Creative Commons Attribution-ShareAlike License 4.0</a><a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Algorithmic_bias&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-settings" id="p-dock-bottom">
	<ul>
		<li>
		
		<button class="cdx-button cdx-button--icon-only vector-limited-width-toggle" id=""><span class="vector-icon mw-ui-icon-fullScreen mw-ui-icon-wikimedia-fullScreen"></span>

<span>Toggle limited content width</span>
</button>
</li>
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw1452","wgBackendResponseTime":2072,"wgPageParseReport":{"limitreport":{"cputime":"1.785","walltime":"1.903","ppvisitednodes":{"value":31556,"limit":1000000},"postexpandincludesize":{"value":422663,"limit":2097152},"templateargumentsize":{"value":7954,"limit":2097152},"expansiondepth":{"value":16,"limit":100},"expensivefunctioncount":{"value":4,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":541133,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00% 1735.977      1 -total"," 55.28%  959.577      1 Template:Reflist"," 26.18%  454.551     91 Template:Rp"," 25.24%  438.079     91 Template:R/superscript"," 17.14%  297.567     58 Template:Cite_journal"," 16.96%  294.354     60 Template:Cite_web"," 12.80%  222.226    273 Template:R/where","  5.72%   99.326     21 Template:Cite_book","  4.73%   82.051     18 Template:Cite_news","  4.04%   70.136      1 Template:Artificial_intelligence"]},"scribunto":{"limitreport-timeusage":{"value":"1.096","limit":"10.000"},"limitreport-memusage":{"value":9040934,"limit":52428800},"limitreport-profile":[["recursiveClone \u003CmwInit.lua:41\u003E","200","16.7"],["?","180","15.0"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::gsub","120","10.0"],["citation0 \u003CModule:Citation/CS1:2536\u003E","60","5.0"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::callParserFunction","60","5.0"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::getExpandedArgument","60","5.0"],["\u003Cmw.lua:694\u003E","60","5.0"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::match","60","5.0"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::getAllExpandedArguments","40","3.3"],["\u003CmwInit.lua:41\u003E","40","3.3"],["[others]","320","26.7"]]},"cachereport":{"origin":"mw1452","timestamp":"20231031015422","ttl":1814400,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Algorithmic bias","url":"https:\/\/en.wikipedia.org\/wiki\/Algorithmic_bias","sameAs":"http:\/\/www.wikidata.org\/entity\/Q45253460","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q45253460","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2017-11-17T06:51:17Z","dateModified":"2023-10-30T14:55:36Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1c\/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg","headline":"systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others"}</script>
</body>
</html>
